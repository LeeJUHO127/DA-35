{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing) 자연어 처리란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어\n",
    "- 사람이 일상적으로 사용하는 언어를 말한다.\n",
    "    - 어떤 목적을 가지고 사람이 만든 것이 아니라 **자연적으로 만들어진 언어**를 말한다.\n",
    "- **인공언어**\n",
    "    - 특정 목적을 위해 사람이 인위적으로 만든 언어로 자연어의 대척점에 있는 언어 개념.\n",
    "    - 예: 프로그래밍 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리 (NLP)\n",
    "- 사람이 사용하는 자연어를 컴퓨터가 이해 할 수 있도록 처리하는 컴퓨터 공학의 한 분야.\n",
    "- 자연어 처리도 오래된 분야인데 딥러닝이 적용되면서 획기적인 발전을 이루었다.\n",
    "- 자연어 처리 응용분야\n",
    "    - 번역 시스템(기계 번역)\n",
    "    - 문서요약\n",
    "    - 감성분석\n",
    "    - 대화형 시스템(챗봇)\n",
    "    - 정보 검색 시스템\n",
    "    - 음성인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어\n",
    "- **말뭉치(corpus, corpora)**: NLP 머신러닝 모델을 학습 시키기 위한  자연어 데이터 셋.\n",
    "- **어간(stem)**\n",
    "\t- 어간은 접미사나 다른 변화 형태가 추가되기 전의 **단어 기본 형태**를 말한다. 즉 활용시 변하지 않는 부분을 말한다.\n",
    "\t- **view** + ing, **view** + er\n",
    "\t- **먹** + 습니다.  **먹** + 었다.  **먹** + 고\n",
    "\t- **예쁘**  + 다, **예쁘**+ 고, **예쁘** + 지만, **예쁘**+ 어서(예뻐서)\n",
    "- **어미**\n",
    "\t- 어미는  어간(주로  용언-동사,형용사) 뒤에 붙어서 단어의 문법적 기능이나 의미를 구체화하는 요소.\n",
    "\t- walked: walk+**ed**: 과거시제, run+**s**: 3인칭 단수 주어의 동사에 붙는 어미.\n",
    "\t- 먹 + **었다**,  먹 + **다**, 갔습니까? : 가 + **ㅆ습니까?**\n",
    "- **조사**\n",
    "\t- 조사는 체언(명사, 대명사) 뒤에 붙어 그 단어의 문법적인 기능이나 관계(예: 주체, 목적, 소유, 방향 등)를 나타내는 요소.\n",
    "\t- 학교 + **에서** (장소를 나타내는 조사), 책+**을** 읽다. (목적어를 타나내는 조사.), 우리+**가** 간다. (주어를 나타내는 조사)\n",
    "- **어절(word segment)**\n",
    "\t- 문장 성분의 최소 단위로 띄어 쓰기의 기준이 된다. \n",
    "\t- 나는 학교에 간다. -> \"**나는**\", \"**학교에**\", \"**간다**\"\t\n",
    "- **형태소(morpheme)**: 의미(뜻)을 가진 가장 작은 언어의 단위, 더 이상 나눌 수 없는 언어의 조각 단위. 형태소는 그 자체로 의미를 가지며 단어를 형성하거나 변형 시키는데 사용한다.\n",
    "\t- 자립 형태소:  명사, 동사, 형용사 같이 독립적으로 사용될 수 있는 형태소\n",
    "\t\t- 나, 너, 택시, 가다, you, have\n",
    "\t- 의존 형태소: 조사, 접미사, 접두사 같이 다른 형태소와 결합해서 사용 되야 하는 형태소.\n",
    "\t\t- \\~의, \\~가, un\\~, \\~able\n",
    "\n",
    "> - **어간과 형태소의 차이**\n",
    "> \t-  **형태소**는 언어의 의미를 가진 최소 단위이며, 자립 형태소와 의존 형태소로 나뉩니다.   \n",
    "> \t  **어간**은 특정 단어에서 그 단어의 핵심 의미를 담은 부분으로, 주로 자립 형태소에 해당하지만, 어미와 같은 의존 형태소가 결합하여 문법적 기능이나 형태를 변화시킬 수 있습니다.\n",
    "> \t-  **형태소**는 의미를 구성하는 기본 단위로서의 역할을 하며, **어간**은 특히 단어를 형성하고 변형 시키는 기반으로서의 역할을 합니다.\n",
    "\n",
    "- **언어 형태 유형 종류**\n",
    "    - **교착어**    \n",
    "        - **정의**: 교착어는 단어에 하나 이상의 접사가 추가되어 새로운 의미나 문법적 기능을 형성하는 언어\n",
    "        \t- 예) \"책\"\n",
    "        \t\t- 책이\n",
    "        \t\t- 책의\n",
    "        \t\t- 책에\n",
    "        \t\t- 책을\n",
    "        - **주요 언어**: 한국어, 몽골어, 튀르키예어, 우랄어족(핀란드어, 헝가리어) 등\n",
    "    \n",
    "    - **굴절어**\n",
    "        - **정의**: 굴절어는 단어 내의 모음이나 자음의 변화로 문법적 기능을 나타내는 언어이다. 이 변화는 단어의 뜻을 변경하지 않으면서도 시제, 인칭, 수, 성 등을 표현할 수 있습니다.\n",
    "        \t- 단어 내의 변화로 문법적 기능을 표현.\n",
    "        - 예) 라틴어에서 \"liber\"는 책을 의미\n",
    "        \t-  **liber** (단수 주격, 남성) - \"책이/책은\"\n",
    "        \t- **libri** (단수 소유격, 남성) - \"책의\"\n",
    "        \t- **libro** (단수 여격, 남성) - \"책에\"\n",
    "        \t- **librum** (단수 목적격, 남성) - \"책을\"\n",
    "        - **주요 언어**: 라틴어, 그리스어, 이탈리아어, 스페인어, 독일어, 러시아어, 힌디어 등\n",
    "    \n",
    "    - **고립어**\n",
    "        - **정의**: 어형 변화나 접사가 없고 위치에 의해서 단어가 문장 속에서 가지는 여러가지 관계가 결정되는 언어. 분석어, 위치어라고도 한다.\n",
    "        - **주요 언어**: 영어, 중국어, 베트남어, 태국어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 모델링 프로세스\n",
    "\n",
    "1. **Corpus(말뭉치) 수집**\n",
    "    - 자연어 학습을 위한 수집한 언어 표본 집합\n",
    "    - 수집방법\n",
    "        - 공개데이터 사용\n",
    "        - 구매\n",
    "        - 크롤링\n",
    "1. **토큰화(Tokenization) 및 전처리**\n",
    "    - 텍스트 토큰화\n",
    "        - 분석의 최소단위로 나누는 작업\n",
    "        - 보통 단어단위나 글자단위로 나눈다.\n",
    "    - **텍스트 전처리**\n",
    "        - 정제(cleaning)\n",
    "            - 문서내의 노이즈(불필요한 문자, 기호들, 빈도수 적은 단어)들을 제거한다.\n",
    "            - 토큰화 전,후 또는 지속적으로 진행한다.\n",
    "        - 정규화(Normalization)\n",
    "            - 같은 의미의 토큰들을 하나로 만들어준다. (말하다. 말하면, 말하기 -> 말)\n",
    "            - 어간추출(stemming), 원형복원(lemmatization, 표제어추출), 형태소분석\n",
    "        - Stop word(불용어-분석에 필요 없는 토큰) 제거\n",
    "1. **Embedding/Feature vectorization**   \n",
    "    - Token을 기계가 이해할 수 있는 Vector로 변환하는 작업\n",
    "    - 빈도수 기반 통계적 방식과 딥러닝(뉴럴 네트워크)를 이용한 데이터학습방식이 있다.\n",
    "    - TF/IDF, Word2Vec, FastText, \n",
    "1. **모델링**\n",
    "    - Embedding된 데이터를 입력으로 받아 자연어 관련 해결하려는 문제를 처리하는 머신러닝(딥러닝) 모델을 구현한다. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- Natural Language ToolKit\n",
    "- https://www.nltk.org/\n",
    "- 자연어 처리를 위한 파이썬 패키지\n",
    "\n",
    "## NLTK 설치\n",
    "- nltk 패키지 설치\n",
    "    - `pip install nltk`\n",
    "- NLTK 추가 패키지 설치\n",
    "```python\n",
    "import nltk\n",
    "nltk.download() # 설치 GUI 프로그램 실행을 실행해 다운로드\n",
    "nltk.download('패키지명')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 주요기능\n",
    "\n",
    "- ### 텍스트 토큰화/정규화/전처리등 처리를 위한 기능 제공\n",
    "    - 토큰화(Tokenization)\n",
    "    - Stop word(불용어) 제공\n",
    "    - 형태소 분석\n",
    "        - 형태소\n",
    "            - 의미가 있는 가장 작은 말의 단위\n",
    "        - 형태소 분석\n",
    "            - 말뭉치에서 의미있는(분석시 필요한) 형태소들만 추출하는 것           \n",
    "        - 어간추출(Stemming)\n",
    "        - 원형복원(Lemmatization)\n",
    "        - 품사부착(POS tagging - Part Of Speech)\n",
    "- ### 텍스트 분석 기능을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"\"\"Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "Readability counts.\n",
    "Special cases aren't special enough to break the rules.\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "Namespaces are one honking great idea -- let's do more of those!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화\n",
    "- 분석을 위해 문서를 작은 단위로 나누는 작업.\n",
    "- **토큰(Token)**\n",
    "    - 나뉜 문자열의 단위를 말한다. \n",
    "    - 정하기에 따라 문장, 단어일 수도 있고 문자일 수도 있다. \n",
    "- **Tokenizer**\n",
    "    - 문장을 token으로 나눠주는 함수\n",
    "    - NLTK 주요 tokenizer\n",
    "        - **sent_tokenize()** : 문장단위로 나눠준다. (`.`를 기준으로 나눈다.)\n",
    "        - **word_tokenize()** : 단어단위로 나눠준다. (공백을 기준으로 나눈다.)\n",
    "        - **regexp_tokenize()** : 토큰의 단위를 정규표현식으로 지정\n",
    "        - 반환타입 : 토큰하나 하나를 원소로 하는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 19\n"
     ]
    }
   ],
   "source": [
    "## 문장 단위 토큰화\n",
    "import nltk\n",
    "sent_list = nltk.sent_tokenize(text_sample)\n",
    "print(type(sent_list), len(sent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful is better than ugly.',\n",
       " 'Explicit is better than implicit.',\n",
       " 'Simple is better than complex.',\n",
       " 'Complex is better than complicated.',\n",
       " 'Flat is better than nested.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = \"Many of life’s failures are people who didn't realize how close they were to success when they gave up.\"\n",
    "txt2 = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Many', 'of', 'life', '’', 's', 'failures', 'are', 'people', 'who', 'did', \"n't\", 'realize', 'how', 'close', 'they', 'were', 'to', 'success', 'when', 'they', 'gave', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 토큰화\n",
    "word_list1 = nltk.word_tokenize(txt1)\n",
    "print(word_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list2 = nltk.word_tokenize(txt2)\n",
    "print(word_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many',\n",
       " 'of',\n",
       " 'life',\n",
       " 's',\n",
       " 'failures',\n",
       " 'are',\n",
       " 'people',\n",
       " 'who',\n",
       " 'didn',\n",
       " 't',\n",
       " 'realize',\n",
       " 'how',\n",
       " 'close',\n",
       " 'they',\n",
       " 'were',\n",
       " 'to',\n",
       " 'success',\n",
       " 'when',\n",
       " 'they',\n",
       " 'gave',\n",
       " 'up']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 패턴을 지정해서 토큰화\n",
    "regex_list1 = nltk.regexp_tokenize(\n",
    "    txt1, #대상 문자열\n",
    "    r\"\\w+\"  # 토큰 패턴 정규표현식. \\w: 일반글자, 숫자, `_` , +: 한글자 이상\n",
    ")\n",
    "regex_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Starting',\n",
       " 'a',\n",
       " 'home',\n",
       " 'based',\n",
       " 'restaurant',\n",
       " 'may',\n",
       " 'be',\n",
       " 'an',\n",
       " 'ideal',\n",
       " 'it',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'have',\n",
       " 'a',\n",
       " 'food',\n",
       " 'chain',\n",
       " 'or',\n",
       " 'restaurant',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_list2 = nltk.regexp_tokenize(txt2, r\"\\w+\")\n",
    "regex_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Penn Treebank 토큰화 - word 단위 토큰화\n",
    "#### 규칙1:  하이픈(-) 으로 구성된 것은 하나의 토큰으로 유지 \"home-base\"\n",
    "#### 규칙2:  didn't  접어를 이용해서 축약한 경우는 분할 해준다. \n",
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "tb_list1 = tokenizer.tokenize(txt1)\n",
    "tb_list2 = tokenizer.tokenize(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many',\n",
       " 'of',\n",
       " 'life’s',\n",
       " 'failures',\n",
       " 'are',\n",
       " 'people',\n",
       " 'who',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'realize',\n",
       " 'how',\n",
       " 'close',\n",
       " 'they',\n",
       " 'were',\n",
       " 'to',\n",
       " 'success',\n",
       " 'when',\n",
       " 'they',\n",
       " 'gave',\n",
       " 'up',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Starting',\n",
       " 'a',\n",
       " 'home-based',\n",
       " 'restaurant',\n",
       " 'may',\n",
       " 'be',\n",
       " 'an',\n",
       " 'ideal.',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'food',\n",
       " 'chain',\n",
       " 'or',\n",
       " 'restaurant',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword (불용어)\n",
    "- 분석에 필요없는 단어들을 말한다.\n",
    "    - 문장내에서는 많이 사용되지만 문장의 전체 맥락이나 내용 파악에는 필요 없는 단어들을 말한다.\n",
    "    - 많이 나오기 때문에 중요한 단어로 인식 할 수 있다. 그래서 제거하는 것이 좋다.\n",
    "    - 대표적인 불용어들은 조사, 접미사, 접속사, 대명사 등이 있다. \n",
    "-  실제 분석대상에 맞는 Stop word 를 만들어서 사용한다. \n",
    "    - 보통 text 파일에 저장하여 관리한다.\n",
    "    - nltk는 언어별로 일반적인 Stop word 사전을 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# NLTK가 제공하는 stopword 언어\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many',\n",
       " 'of',\n",
       " 'life',\n",
       " '’',\n",
       " 's',\n",
       " 'failures',\n",
       " 'are',\n",
       " 'people',\n",
       " 'who',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'realize',\n",
       " 'how',\n",
       " 'close',\n",
       " 'they',\n",
       " 'were',\n",
       " 'to',\n",
       " 'success',\n",
       " 'when',\n",
       " 'they',\n",
       " 'gave',\n",
       " 'up',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many',\n",
       " 'life',\n",
       " '’',\n",
       " 'failures',\n",
       " 'people',\n",
       " \"n't\",\n",
       " 'realize',\n",
       " 'close',\n",
       " 'success',\n",
       " 'gave',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  토큰화한 내용에 stopword 적용\n",
    "[word for word in word_list1 if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T02:03:51.905615Z",
     "start_time": "2021-04-13T02:03:51.892618Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    전체 문장들을 토큰화해 반환하는 함수\n",
    "    문장별로 단어 리스트(의미를 파악하는데 중요한 단어들)를 2차원 배열 형태로 반환\n",
    "    구두점/특수문자, 숫자, 불용어(stop words)들은 모두 제거한다.\n",
    "    [매개변수]\n",
    "        text: string - 변환하려는 전체문장\n",
    "    [반환값]\n",
    "        2차원 리스트. 1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    \"\"\"\n",
    "    #1. 받은 문장을 모두 소문자로 변환.\n",
    "    text = text.lower()\n",
    "    #2. 문장단위 토큰화\n",
    "    sent_tokens = nltk.sent_tokenize(text)\n",
    "    #3. 클린징 작업 - 불용어, 특수문자, 숫자 등등을 제거\n",
    "    ## 불용어사전 loading\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stop_words.extend(['although', 'unless', 'may']) # 필요하다면 불용어 단어를 추가할 수있다.\n",
    "    result_list = [] # 최종 결과를 저장할 리스트\n",
    "    for sent in sent_tokens:  # 한문장씩 처리\n",
    "        result = nltk.regexp_tokenize(sent, r\"[A-Za-z]+\")\n",
    "        # 불용어제거\n",
    "        result = [word for word in result if word not in stop_words]\n",
    "        result_list.append(result)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text_sample)\n",
    "result = tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple', 'better', 'complex']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[4]\n",
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 한국어에서 토큰화가 어려운 이유\n",
    "> - 영어는 띄어쓰기(공백)을 기준으로 토큰화를 진행해도 큰 문제가 없다.\n",
    "> - 한국어는 교착어이기 때문에 띄어쓰기를 기준으로 토큰화를 하면 같은 단어가 다른 토큰으로 인식되어 여러개 추출되는 문제가 발생한다.\n",
    ">     - 예) \"그가\", \"그는\", \"그의\", \"그와\" 모두 \"그\"를 나타내지만 붙은 조사가 달라 다 다른 토큰으로 추출되게 된다.\n",
    "> - 그래서 한국어는 어절 단위 토큰화는 하지 않도록 한다.\n",
    "> - 대신 형태소에 기반한 토큰화를 하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "- 형태소\n",
    "    - 일정한 의미가 있는 가장 작은 말의 단위\n",
    "- 형태소 분석  \n",
    "    - 말뭉치에서 의미있는(분석에 필요한) 형태소들만 추출하는 것\n",
    "    - 보통 단어로 부터 어근, 접두사, 접미사, 품사등 언어적 속성을 파악하여 처리한다. \n",
    "- 형태소 분석을 위한 기법\n",
    "    - 어간추출(Stemming)\n",
    "    - 원형(기본형) 복원 (Lemmatization)\n",
    "    - 품사부착 (POS tagging - Part Of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)\n",
    "- 어간: 활용어에서 변하지 않는 부분\n",
    "    - painted, paint, painting → 어간: paint\n",
    "    - 보다, 보니, 보고 → 어간 보-\n",
    "- 어간 추출 \n",
    "    - 단어에서 어미를 제거하고 어간을 추출하는 작업\n",
    "- 목적\n",
    "    - 같은 의미를 가지는 단어의 여러가지 활용이 있을 경우 다른 단어로 카운트 되는 문제점을 해결한다.\n",
    "        - flower, flowers 가 두개의 단어로 카운트 되는 것을 flower로 통일한다.        \n",
    "- nltk의 주요 어간 추출 알고리즘\n",
    "    - Porter Stemmer\n",
    "    - Lancaster Stemmer\n",
    "    - Snowball Stemmer\n",
    "- 메소드\n",
    "    - `stemmer객체.stem(단어)`\n",
    "- stemming의 문제\n",
    "    - 완벽하지 않다는 것이 문제이다.        \n",
    "        - ex) new와 news는 다른 단어 인데 둘다 new로 처리한다.\n",
    "    - 처리후 눈으로 직접 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"Working\",\n",
    "    \"works\",\n",
    "    \"worked\",\n",
    "    \"Painting\",\n",
    "    \"Painted\",\n",
    "    \"paints\",\n",
    "    \"Happy\",\n",
    "    \"happier\",\n",
    "    \"happiest\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원형(기본형)복원(Lemmatization)\n",
    "- 단어의 기본형을 반환한다.\n",
    "    - ex) am, is, are => be\n",
    "- 단어의 품사를 지정하면 정확한 결과를 얻을 수 있다. \n",
    "- `WordNetLemmatizer객체.lemmatize(단어 [, pos=품사])`\n",
    "\n",
    "> 어간추출과 원형복원은 문법적 또는 의미적으로 변한 단어의 원형을 찾는 역할은 동일하다.<br>\n",
    "> **원형복원**은 품사와 같은 문법적요소와 문장내에서의 의미적인 부분을 감안해 찾기 때문에 어간추출 방식보다 더 정교하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착-POS Tagging(Part-Of-Speech Tagging)\n",
    "- 형태소에 품사를 붙이는 작업.\n",
    "    - 품사의 구분이나 표현은 언어, 학자마다 다르다. \n",
    "- NLTK는 [펜 트리뱅크 태그세트](https://bluebreeze.co.kr/1357)(Penn Treebank Tagset) 이용\n",
    "    - 명사 : N으로 시작 (NN-일반명사, NNP-고유명사)\n",
    "    - 형용사 : J로 시작(JJ, JJR-비교급, JJS-최상급)\n",
    "    - 동사: V로 시작 (VB-동사원형, VBP-3인칭 아닌 현재형 동사)\n",
    "    - 부사: R로 시작 (RB-부사)\n",
    "    - `nltk.help.upenn_tagset('키워드')` : 도움말\n",
    "- `pos_tag(단어_리스트)`    \n",
    "    - 단어와 품사를 튜플로 묶은 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "words = [\"Book\", \"car\", \"have\", \"Korea\", \"is\", 'well', 'can']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착과 원형복원을 이용해 원형복원하기.\n",
    "- 품사부착으로 품사 조회\n",
    "    - pos_tag와 lemmatization이 사용하는 품사 형태 다르기 때문에 변환함수 만듬\n",
    "- lemmatization하기.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos-tag 에서 반환한 품사표기(펜 트리뱅크 태그세트)을 WordNetLemmatizer의 품사표기로 변환\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"\n",
    "    펜 트리뱅크 품사표기를 WordNetLemmatizer에서 사용하는 품사표기로 변환\n",
    "    형용사/동사/명사/부사 표기 변환\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석을 위한 클래스들\n",
    "\n",
    "## Text클래스\n",
    "- 문서 분석에 유용한 여러 메소드 제공\n",
    "- **토큰 리스트**을 입력해 객체생성 후 제공되는 메소드를 이용해 분석한다.\n",
    "- ### 생성\n",
    "    - Text(토큰리스트, [name=이름])\n",
    "- ### 주요 메소드\n",
    "    - count(단어)\n",
    "        - 매개변수로 전달한 단어의 빈도수\n",
    "    - plot(N)\n",
    "        - 빈도수 상위 N개 단어를 선그래프로 시각화\n",
    "    - dispersion_plot(단어리스트)\n",
    "        - 매개변수로 전달한 단어들이 전체 말뭉치의 어느 부분에 나오는지 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqDist\n",
    "- document에서 사용된 토큰(단어)의 사용빈도 데이터를 가지는 클래스\n",
    "    - 토큰(단어)를 key, 개수를 value로 가지는 딕셔너리 형태\n",
    "- 생성\n",
    "    - Text 객체의 vocab() 메소드로 조회한다.\n",
    "    - 생성자(Initializer)에 토큰 List를 직접 넣어 생성가능\n",
    "- 주요 메소드\n",
    "    - B(): 출연한 고유 단어의 개수\n",
    "        - [Apple, Apple] -> 1\n",
    "    - N(): 총 단어수 \n",
    "        - [Apple, Apple] -> 2\n",
    "    - get(단어) 또는 FreqDist['단어'] : 특정 단어의 출연 빈도수\n",
    "    - freq(단어): 총 단어수 대비 특정단어의 출연비율\n",
    "    - most_common() : 빈도수 순서로 정렬하여 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordcloud\n",
    "- 단어의 빈도수를 시각화 \n",
    "- 많이 출현한 단어를 크게 적게 출현한 단어를 작게 표현하여 핵심단어들을 쉽게 파악할 수있게 한다.\n",
    "- wordcloud 패키지 사용\n",
    "     - 설치: `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
