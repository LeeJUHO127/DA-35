{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing) 자연어 처리란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어\n",
    "- 사람이 일상적으로 사용하는 언어를 말한다.\n",
    "    - 어떤 목적을 가지고 사람이 만든 것이 아니라 **자연적으로 만들어진 언어**를 말한다.\n",
    "- **인공언어**\n",
    "    - 특정 목적을 위해 사람이 인위적으로 만든 언어로 자연어의 대척점에 있는 언어 개념.\n",
    "    - 예: 프로그래밍 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리 (NLP)\n",
    "- 사람이 사용하는 자연어를 컴퓨터가 이해 할 수 있도록 처리하는 컴퓨터 공학의 한 분야.\n",
    "- 자연어 처리도 오래된 분야인데 딥러닝이 적용되면서 획기적인 발전을 이루었다.\n",
    "- 자연어 처리 응용분야\n",
    "    - 번역 시스템(기계 번역)\n",
    "    - 문서요약\n",
    "    - 감성분석\n",
    "    - 대화형 시스템(챗봇)\n",
    "    - 정보 검색 시스템\n",
    "    - 음성인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어\n",
    "- **말뭉치(corpus, corpora)**: NLP 머신러닝 모델을 학습 시키기 위한  자연어 데이터 셋.\n",
    "- **어간(stem)**\n",
    "\t- 어간은 접미사나 다른 변화 형태가 추가되기 전의 **단어 기본 형태**를 말한다. 즉 활용시 변하지 않는 부분을 말한다.\n",
    "\t- **view** + ing, **view** + er\n",
    "\t- **먹** + 습니다.  **먹** + 었다.  **먹** + 고\n",
    "\t- **예쁘**  + 다, **예쁘**+ 고, **예쁘** + 지만, **예쁘**+ 어서(예뻐서)\n",
    "- **어미**\n",
    "\t- 어미는  어간(주로  용언-동사,형용사) 뒤에 붙어서 단어의 문법적 기능이나 의미를 구체화하는 요소.\n",
    "\t- walked: walk+**ed**: 과거시제, run+**s**: 3인칭 단수 주어의 동사에 붙는 어미.\n",
    "\t- 먹 + **었다**,  먹 + **다**, 갔습니까? : 가 + **ㅆ습니까?**\n",
    "- **조사**\n",
    "\t- 조사는 체언(명사, 대명사) 뒤에 붙어 그 단어의 문법적인 기능이나 관계(예: 주체, 목적, 소유, 방향 등)를 나타내는 요소.\n",
    "\t- 학교 + **에서** (장소를 나타내는 조사), 책+**을** 읽다. (목적어를 타나내는 조사.), 우리+**가** 간다. (주어를 나타내는 조사)\n",
    "- **어절(word segment)**\n",
    "\t- 문장 성분의 최소 단위로 띄어 쓰기의 기준이 된다. \n",
    "\t- 나는 학교에 간다. -> \"**나는**\", \"**학교에**\", \"**간다**\"\t\n",
    "- **형태소(morpheme)**: 의미(뜻)을 가진 가장 작은 언어의 단위, 더 이상 나눌 수 없는 언어의 조각 단위. 형태소는 그 자체로 의미를 가지며 단어를 형성하거나 변형 시키는데 사용한다.\n",
    "\t- 자립 형태소:  명사, 동사, 형용사 같이 독립적으로 사용될 수 있는 형태소\n",
    "\t\t- 나, 너, 택시, 가다, you, have\n",
    "\t- 의존 형태소: 조사, 접미사, 접두사 같이 다른 형태소와 결합해서 사용 되야 하는 형태소.\n",
    "\t\t- \\~의, \\~가, un\\~, \\~able\n",
    "\n",
    "> - **어간과 형태소의 차이**\n",
    "> \t-  **형태소**는 언어의 의미를 가진 최소 단위이며, 자립 형태소와 의존 형태소로 나뉩니다.   \n",
    "> \t  **어간**은 특정 단어에서 그 단어의 핵심 의미를 담은 부분으로, 주로 자립 형태소에 해당하지만, 어미와 같은 의존 형태소가 결합하여 문법적 기능이나 형태를 변화시킬 수 있습니다.\n",
    "> \t-  **형태소**는 의미를 구성하는 기본 단위로서의 역할을 하며, **어간**은 특히 단어를 형성하고 변형 시키는 기반으로서의 역할을 합니다.\n",
    "\n",
    "- **언어 형태 유형 종류**\n",
    "    - **교착어**    \n",
    "        - **정의**: 교착어는 단어에 하나 이상의 접사가 추가되어 새로운 의미나 문법적 기능을 형성하는 언어\n",
    "        \t- 예) \"책\"\n",
    "        \t\t- 책이\n",
    "        \t\t- 책의\n",
    "        \t\t- 책에\n",
    "        \t\t- 책을\n",
    "        - **주요 언어**: 한국어, 몽골어, 튀르키예어, 우랄어족(핀란드어, 헝가리어) 등\n",
    "    \n",
    "    - **굴절어**\n",
    "        - **정의**: 굴절어는 단어 내의 모음이나 자음의 변화로 문법적 기능을 나타내는 언어이다. 이 변화는 단어의 뜻을 변경하지 않으면서도 시제, 인칭, 수, 성 등을 표현할 수 있습니다.\n",
    "        \t- 단어 내의 변화로 문법적 기능을 표현.\n",
    "        - 예) 라틴어에서 \"liber\"는 책을 의미\n",
    "        \t-  **liber** (단수 주격, 남성) - \"책이/책은\"\n",
    "        \t- **libri** (단수 소유격, 남성) - \"책의\"\n",
    "        \t- **libro** (단수 여격, 남성) - \"책에\"\n",
    "        \t- **librum** (단수 목적격, 남성) - \"책을\"\n",
    "        - **주요 언어**: 라틴어, 그리스어, 이탈리아어, 스페인어, 독일어, 러시아어, 힌디어 등\n",
    "    \n",
    "    - **고립어**\n",
    "        - **정의**: 어형 변화나 접사가 없고 위치에 의해서 단어가 문장 속에서 가지는 여러가지 관계가 결정되는 언어. 분석어, 위치어라고도 한다.\n",
    "        - **주요 언어**: 영어, 중국어, 베트남어, 태국어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 모델링 프로세스\n",
    "\n",
    "1. **Corpus(말뭉치) 수집**\n",
    "    - 자연어 학습을 위한 수집한 언어 표본 집합\n",
    "    - 수집방법\n",
    "        - 공개데이터 사용\n",
    "        - 구매\n",
    "        - 크롤링\n",
    "1. **토큰화(Tokenization) 및 전처리**\n",
    "    - 텍스트 토큰화\n",
    "        - 분석의 최소단위로 나누는 작업\n",
    "        - 보통 단어단위나 글자단위로 나눈다.\n",
    "    - **텍스트 전처리**\n",
    "        - 정제(cleaning)\n",
    "            - 문서내의 노이즈(불필요한 문자, 기호들, 빈도수 적은 단어)들을 제거한다.\n",
    "            - 토큰화 전,후 또는 지속적으로 진행한다.\n",
    "        - 정규화(Normalization)\n",
    "            - 같은 의미의 토큰들을 하나로 만들어준다. (말하다. 말하면, 말하기 -> 말)\n",
    "            - 어간추출(stemming), 원형복원(lemmatization, 표제어추출), 형태소분석\n",
    "        - Stop word(불용어-분석에 필요 없는 토큰) 제거\n",
    "1. **Embedding/Feature vectorization**   \n",
    "    - Token을 기계가 이해할 수 있는 Vector로 변환하는 작업\n",
    "    - 빈도수 기반 통계적 방식과 딥러닝(뉴럴 네트워크)를 이용한 데이터학습방식이 있다.\n",
    "    - TF/IDF, Word2Vec, FastText, \n",
    "1. **모델링**\n",
    "    - Embedding된 데이터를 입력으로 받아 자연어 관련 해결하려는 문제를 처리하는 머신러닝(딥러닝) 모델을 구현한다. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- Natural Language ToolKit\n",
    "- https://www.nltk.org/\n",
    "- 자연어 처리를 위한 파이썬 패키지\n",
    "\n",
    "## NLTK 설치\n",
    "- nltk 패키지 설치\n",
    "    - `pip install nltk`\n",
    "- NLTK 추가 패키지 설치\n",
    "```python\n",
    "import nltk\n",
    "nltk.download() # 설치 GUI 프로그램 실행을 실행해 다운로드\n",
    "nltk.download('패키지명')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 주요기능\n",
    "\n",
    "- ### 텍스트 토큰화/정규화/전처리등 처리를 위한 기능 제공\n",
    "    - 토큰화(Tokenization)\n",
    "    - Stop word(불용어) 제공\n",
    "    - 형태소 분석\n",
    "        - 형태소\n",
    "            - 의미가 있는 가장 작은 말의 단위\n",
    "        - 형태소 분석\n",
    "            - 말뭉치에서 의미있는(분석시 필요한) 형태소들만 추출하는 것           \n",
    "        - 어간추출(Stemming)\n",
    "        - 원형복원(Lemmatization)\n",
    "        - 품사부착(POS tagging - Part Of Speech)\n",
    "- ### 텍스트 분석 기능을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화\n",
    "- 분석을 위해 문서를 작은 단위로 나누는 작업.\n",
    "- **토큰(Token)**\n",
    "    - 나뉜 문자열의 단위를 말한다. \n",
    "    - 정하기에 따라 문장, 단어일 수도 있고 문자일 수도 있다. \n",
    "- **Tokenizer**\n",
    "    - 문장을 token으로 나눠주는 함수\n",
    "    - NLTK 주요 tokenizer\n",
    "        - **sent_tokenize()** : 문장단위로 나눠준다. (`.`를 기준으로 나눈다.)\n",
    "        - **word_tokenize()** : 단어단위로 나눠준다. (공백을 기준으로 나눈다.)\n",
    "        - **regexp_tokenize()** : 토큰의 단위를 정규표현식으로 지정\n",
    "        - 반환타입 : 토큰하나 하나를 원소로 하는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = \"Many of life’s failures are people who didn't realize how close they were to success when they gave up.\"\n",
    "txt2 = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword (불용어)\n",
    "- 분석에 필요없는 단어들을 말한다.\n",
    "    - 문장내에서는 많이 사용되지만 문장의 전체 맥락이나 내용 파악에는 필요 없는 단어들을 말한다.\n",
    "    - 많이 나오기 때문에 중요한 단어로 인식 할 수 있다. 그래서 제거하는 것이 좋다.\n",
    "    - 대표적인 불용어들은 조사, 접미사, 접속사, 대명사 등이 있다. \n",
    "-  실제 분석대상에 맞는 Stop word 를 만들어서 사용한다. \n",
    "    - 보통 text 파일에 저장하여 관리한다.\n",
    "    - nltk는 언어별로 일반적인 Stop word 사전을 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T02:03:51.905615Z",
     "start_time": "2021-04-13T02:03:51.892618Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    전체 문장들을 토큰화해 반환하는 함수\n",
    "    문장별로 단어 리스트(의미를 파악하는데 중요한 단어들)를 2차원 배열 형태로 반환\n",
    "    구두점/특수문자, 숫자, 불용어(stop words)들은 모두 제거한다.\n",
    "    [매개변수]\n",
    "        text: string - 변환하려는 전체문장\n",
    "    [반환값]\n",
    "        2차원 리스트. 1차원: 문장 리스트, 2차원: 문장내 토큰.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 한국어에서 토큰화가 어려운 이유\n",
    "> - 영어는 띄어쓰기(공백)을 기준으로 토큰화를 진행해도 큰 문제가 없다.\n",
    "> - 한국어는 교착어이기 때문에 띄어쓰기를 기준으로 토큰화를 하면 같은 단어가 다른 토큰으로 인식되어 여러개 추출되는 문제가 발생한다.\n",
    ">     - 예) \"그가\", \"그는\", \"그의\", \"그와\" 모두 \"그\"를 나타내지만 붙은 조사가 달라 다 다른 토큰으로 추출되게 된다.\n",
    "> - 그래서 한국어는 어절 단위 토큰화는 하지 않도록 한다.\n",
    "> - 대신 형태소에 기반한 토큰화를 하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "- 형태소\n",
    "    - 일정한 의미가 있는 가장 작은 말의 단위\n",
    "- 형태소 분석  \n",
    "    - 말뭉치에서 의미있는(분석에 필요한) 형태소들만 추출하는 것\n",
    "    - 보통 단어로 부터 어근, 접두사, 접미사, 품사등 언어적 속성을 파악하여 처리한다. \n",
    "- 형태소 분석을 위한 기법\n",
    "    - 어간추출(Stemming)\n",
    "    - 원형(기본형) 복원 (Lemmatization)\n",
    "    - 품사부착 (POS tagging - Part Of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)\n",
    "- 어간: 활용어에서 변하지 않는 부분\n",
    "    - painted, paint, painting → 어간: paint\n",
    "    - 보다, 보니, 보고 → 어간 보-\n",
    "- 어간 추출 \n",
    "    - 단어에서 어미를 제거하고 어간을 추출하는 작업\n",
    "- 목적\n",
    "    - 같은 의미를 가지는 단어의 여러가지 활용이 있을 경우 다른 단어로 카운트 되는 문제점을 해결한다.\n",
    "        - flower, flowers 가 두개의 단어로 카운트 되는 것을 flower로 통일한다.        \n",
    "- nltk의 주요 어간 추출 알고리즘\n",
    "    - Porter Stemmer\n",
    "    - Lancaster Stemmer\n",
    "    - Snowball Stemmer\n",
    "- 메소드\n",
    "    - `stemmer객체.stem(단어)`\n",
    "- stemming의 문제\n",
    "    - 완벽하지 않다는 것이 문제이다.        \n",
    "        - ex) new와 news는 다른 단어 인데 둘다 new로 처리한다.\n",
    "    - 처리후 눈으로 직접 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"Working\",\n",
    "    \"works\",\n",
    "    \"worked\",\n",
    "    \"Painting\",\n",
    "    \"Painted\",\n",
    "    \"paints\",\n",
    "    \"Happy\",\n",
    "    \"happier\",\n",
    "    \"happiest\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원형(기본형)복원(Lemmatization)\n",
    "- 단어의 기본형을 반환한다.\n",
    "    - ex) am, is, are => be\n",
    "- 단어의 품사를 지정하면 정확한 결과를 얻을 수 있다. \n",
    "- `WordNetLemmatizer객체.lemmatize(단어 [, pos=품사])`\n",
    "\n",
    "> 어간추출과 원형복원은 문법적 또는 의미적으로 변한 단어의 원형을 찾는 역할은 동일하다.<br>\n",
    "> **원형복원**은 품사와 같은 문법적요소와 문장내에서의 의미적인 부분을 감안해 찾기 때문에 어간추출 방식보다 더 정교하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착-POS Tagging(Part-Of-Speech Tagging)\n",
    "- 형태소에 품사를 붙이는 작업.\n",
    "    - 품사의 구분이나 표현은 언어, 학자마다 다르다. \n",
    "- NLTK는 [펜 트리뱅크 태그세트](https://bluebreeze.co.kr/1357)(Penn Treebank Tagset) 이용\n",
    "    - 명사 : N으로 시작 (NN-일반명사, NNP-고유명사)\n",
    "    - 형용사 : J로 시작(JJ, JJR-비교급, JJS-최상급)\n",
    "    - 동사: V로 시작 (VB-동사원형, VBP-3인칭 아닌 현재형 동사)\n",
    "    - 부사: R로 시작 (RB-부사)\n",
    "    - `nltk.help.upenn_tagset('키워드')` : 도움말\n",
    "- `pos_tag(단어_리스트)`    \n",
    "    - 단어와 품사를 튜플로 묶은 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "words = [\"Book\", \"car\", \"have\", \"Korea\", \"is\", 'well', 'can']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착과 원형복원을 이용해 원형복원하기.\n",
    "- 품사부착으로 품사 조회\n",
    "    - pos_tag와 lemmatization이 사용하는 품사 형태 다르기 때문에 변환함수 만듬\n",
    "- lemmatization하기.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos-tag 에서 반환한 품사표기(펜 트리뱅크 태그세트)을 WordNetLemmatizer의 품사표기로 변환\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"\n",
    "    펜 트리뱅크 품사표기를 WordNetLemmatizer에서 사용하는 품사표기로 변환\n",
    "    형용사/동사/명사/부사 표기 변환\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석을 위한 클래스들\n",
    "\n",
    "## Text클래스\n",
    "- 문서 분석에 유용한 여러 메소드 제공\n",
    "- **토큰 리스트**을 입력해 객체생성 후 제공되는 메소드를 이용해 분석한다.\n",
    "- ### 생성\n",
    "    - Text(토큰리스트, [name=이름])\n",
    "- ### 주요 메소드\n",
    "    - count(단어)\n",
    "        - 매개변수로 전달한 단어의 빈도수\n",
    "    - plot(N)\n",
    "        - 빈도수 상위 N개 단어를 선그래프로 시각화\n",
    "    - dispersion_plot(단어리스트)\n",
    "        - 매개변수로 전달한 단어들이 전체 말뭉치의 어느 부분에 나오는지 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqDist\n",
    "- document에서 사용된 토큰(단어)의 사용빈도 데이터를 가지는 클래스\n",
    "    - 토큰(단어)를 key, 개수를 value로 가지는 딕셔너리 형태\n",
    "- 생성\n",
    "    - Text 객체의 vocab() 메소드로 조회한다.\n",
    "    - 생성자(Initializer)에 토큰 List를 직접 넣어 생성가능\n",
    "- 주요 메소드\n",
    "    - B(): 출연한 고유 단어의 개수\n",
    "        - [Apple, Apple] -> 1\n",
    "    - N(): 총 단어수 \n",
    "        - [Apple, Apple] -> 2\n",
    "    - get(단어) 또는 FreqDist['단어'] : 특정 단어의 출연 빈도수\n",
    "    - freq(단어): 총 단어수 대비 특정단어의 출연비율\n",
    "    - most_common() : 빈도수 순서로 정렬하여 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordcloud\n",
    "- 단어의 빈도수를 시각화 \n",
    "- 많이 출현한 단어를 크게 적게 출현한 단어를 작게 표현하여 핵심단어들을 쉽게 파악할 수있게 한다.\n",
    "- wordcloud 패키지 사용\n",
    "     - 설치: `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
