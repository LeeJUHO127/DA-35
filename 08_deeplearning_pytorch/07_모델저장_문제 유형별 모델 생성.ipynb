{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 **모델의 파라미터만 저장**하는 방법과 **모델 구조와 파라미터 모두를 저장**하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "# loading된 checkpoint 값 이용해 이전 학습상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module instance 초기화\n",
    "        self.lr = nn.Linear(784, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = MyNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 저장\n",
    "torch.save(sample_model, \"saved_models/sample_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = torch.load(\"saved_models/sample_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0604, -0.0751,  0.0346,  0.1173,  0.0278, -0.0202, -0.0600, -0.0056,\n",
       "          0.1058, -0.0004, -0.0481, -0.1229, -0.1025, -0.0039, -0.0145, -0.0356,\n",
       "          0.0561, -0.1071,  0.1110,  0.0410, -0.1036, -0.0985,  0.0488,  0.0405,\n",
       "          0.0228, -0.0644, -0.0354,  0.1013,  0.1175, -0.0426, -0.0393,  0.0053,\n",
       "         -0.0845, -0.1012,  0.0067,  0.1082,  0.0537, -0.0222, -0.0471,  0.0936,\n",
       "         -0.0441,  0.0436,  0.0543, -0.0111, -0.0381, -0.0128, -0.0565,  0.0742,\n",
       "          0.0429, -0.0781, -0.0088,  0.0818, -0.1061, -0.0348, -0.0035, -0.0739,\n",
       "          0.0526, -0.0355, -0.0639,  0.0298, -0.0043,  0.1197,  0.0988,  0.0104],\n",
       "        [ 0.0324, -0.0218,  0.1191,  0.0195, -0.0769,  0.0806,  0.0238,  0.0496,\n",
       "          0.0556, -0.1210,  0.0379,  0.0094, -0.1067, -0.0264,  0.0368,  0.1144,\n",
       "          0.0932, -0.0140,  0.0369, -0.0371,  0.0051,  0.0121,  0.0783, -0.0279,\n",
       "          0.1240,  0.1120, -0.0090, -0.0803,  0.1102, -0.0858,  0.0063, -0.0901,\n",
       "         -0.1156,  0.1078, -0.0248,  0.0265,  0.0593, -0.0512, -0.0672, -0.0395,\n",
       "         -0.0408, -0.0408,  0.0870,  0.0408,  0.0670, -0.0878,  0.0835,  0.1158,\n",
       "          0.0793,  0.0144, -0.0834, -0.1085, -0.0440, -0.0846,  0.0255, -0.0813,\n",
       "         -0.0034,  0.0583, -0.0514,  0.0237,  0.0772,  0.0921,  0.0129, -0.1099],\n",
       "        [-0.1020,  0.0772,  0.1106,  0.0150,  0.0645,  0.0410,  0.1192,  0.0761,\n",
       "         -0.0887, -0.0846,  0.0446,  0.0684, -0.0788,  0.0619, -0.0847,  0.1233,\n",
       "         -0.0266,  0.1033, -0.1167,  0.0734, -0.0662,  0.1013, -0.0304, -0.0091,\n",
       "         -0.0216, -0.0792,  0.0380,  0.0613, -0.0923, -0.0197, -0.1154, -0.0283,\n",
       "         -0.0406,  0.0205,  0.0041, -0.0926,  0.0999, -0.0885, -0.0400,  0.1050,\n",
       "         -0.0078, -0.0754, -0.0862,  0.0287, -0.0333,  0.0788, -0.0890,  0.0389,\n",
       "         -0.1230, -0.0789, -0.0409, -0.1215, -0.0911,  0.0040,  0.0377, -0.0638,\n",
       "          0.0322, -0.0078, -0.0883,  0.0717, -0.0417,  0.0024,  0.0126,  0.0075],\n",
       "        [ 0.0768,  0.0348, -0.0555, -0.0513, -0.0081,  0.0284,  0.0715, -0.0041,\n",
       "         -0.1011,  0.0138, -0.0105,  0.1204,  0.0675,  0.0827,  0.1212, -0.0213,\n",
       "         -0.0946, -0.0802, -0.0498,  0.0059,  0.1088, -0.0979, -0.0367,  0.0124,\n",
       "         -0.0014,  0.0215,  0.0287,  0.0280,  0.0894,  0.0087, -0.0835,  0.0217,\n",
       "          0.0998,  0.0071,  0.0028, -0.0783,  0.0924, -0.0765, -0.0713,  0.0442,\n",
       "         -0.0344, -0.1022, -0.0342,  0.1062, -0.1115,  0.0665,  0.0142, -0.0844,\n",
       "         -0.0572,  0.0734,  0.1197, -0.0226,  0.0802,  0.0103,  0.0518,  0.0498,\n",
       "          0.0277, -0.0867, -0.1156,  0.0931,  0.0503, -0.0345,  0.0725, -0.0058],\n",
       "        [-0.0091, -0.0702, -0.0967, -0.0738,  0.0977,  0.0613, -0.1137,  0.0629,\n",
       "          0.1198,  0.0161, -0.0921, -0.1213, -0.1077, -0.0087, -0.0577,  0.0298,\n",
       "         -0.0641,  0.0710,  0.0570, -0.1222, -0.1091,  0.0811,  0.0065, -0.0363,\n",
       "          0.0125, -0.1226, -0.0096,  0.0693, -0.0888,  0.0789, -0.0353, -0.0966,\n",
       "         -0.1079, -0.0271, -0.0858, -0.0562,  0.0154,  0.0959,  0.0974,  0.0779,\n",
       "         -0.0711, -0.0830,  0.0641, -0.0954, -0.0203,  0.0171, -0.0629, -0.0722,\n",
       "         -0.1133, -0.0574, -0.0861, -0.0630, -0.0717, -0.0072,  0.0575,  0.0537,\n",
       "          0.0648, -0.0124,  0.0203,  0.0985,  0.0122, -0.0222, -0.0154,  0.0211],\n",
       "        [ 0.0383, -0.0344,  0.0799,  0.0622, -0.0695, -0.1111, -0.0339, -0.0717,\n",
       "          0.0368, -0.0667,  0.0044, -0.1111,  0.0961, -0.0714, -0.0937, -0.0837,\n",
       "         -0.0005, -0.0426,  0.0665, -0.0287,  0.0711,  0.0309, -0.0176, -0.0231,\n",
       "         -0.0032, -0.0251, -0.0175, -0.0063,  0.0982,  0.0147, -0.1214, -0.0884,\n",
       "         -0.0309, -0.0158,  0.0514,  0.0383,  0.0981,  0.0014, -0.0576,  0.0648,\n",
       "          0.0648, -0.0381,  0.0577, -0.1220, -0.0129, -0.0764,  0.0598,  0.1098,\n",
       "         -0.1068, -0.0154, -0.0016,  0.0538,  0.0220, -0.0786,  0.0642,  0.0473,\n",
       "         -0.0799, -0.0295, -0.1072, -0.0358, -0.0566, -0.1086,  0.0462, -0.1098],\n",
       "        [-0.0869,  0.0590,  0.0639, -0.0066, -0.0761, -0.1198,  0.0597,  0.0085,\n",
       "         -0.0875, -0.1130,  0.1021, -0.0306, -0.0275, -0.1239, -0.1205,  0.0152,\n",
       "         -0.1001, -0.0525,  0.0208,  0.0055, -0.1084, -0.1239, -0.0166,  0.1115,\n",
       "         -0.0910,  0.0728, -0.0363,  0.1175, -0.0543,  0.1076,  0.0621,  0.0907,\n",
       "         -0.0451, -0.0633,  0.0926,  0.0573, -0.0826,  0.0696, -0.0325,  0.0721,\n",
       "          0.0132,  0.0929, -0.0159, -0.0335,  0.0931,  0.0519,  0.0140, -0.1235,\n",
       "          0.0145, -0.1198, -0.0833,  0.0483, -0.0053,  0.0298,  0.0792, -0.0711,\n",
       "          0.0840, -0.1205, -0.0888, -0.0223,  0.1243,  0.0750, -0.0565,  0.0863],\n",
       "        [-0.0746, -0.0896, -0.0977, -0.0474,  0.0700, -0.0994,  0.0916, -0.0111,\n",
       "          0.0235, -0.0970,  0.0009,  0.0814,  0.0411,  0.0164,  0.0988, -0.0918,\n",
       "          0.0099,  0.1247,  0.1243, -0.0401, -0.0937, -0.0505, -0.0614, -0.0263,\n",
       "         -0.0080,  0.1223,  0.0018,  0.0087,  0.0467, -0.0128, -0.0770,  0.0883,\n",
       "          0.1120,  0.0857, -0.0426,  0.0962, -0.0438,  0.1228,  0.0796, -0.0098,\n",
       "          0.0650, -0.0879, -0.0169, -0.1109, -0.1226,  0.0996,  0.0724, -0.0562,\n",
       "          0.1058,  0.0129,  0.0260, -0.0868,  0.0895, -0.0565, -0.0656,  0.0868,\n",
       "          0.0710, -0.0978, -0.1232,  0.0526, -0.0492, -0.0750,  0.0915, -0.1094],\n",
       "        [-0.0060,  0.1010, -0.1244, -0.0826,  0.0582,  0.1041,  0.0046, -0.0598,\n",
       "          0.0709, -0.0059,  0.0108,  0.0111,  0.0956, -0.0687, -0.0793,  0.0174,\n",
       "          0.0381,  0.0632, -0.0539, -0.0541,  0.0504, -0.0429, -0.1041, -0.1245,\n",
       "         -0.1210,  0.0861,  0.0414,  0.0709, -0.0026, -0.0437,  0.1088, -0.0771,\n",
       "          0.0240,  0.0385,  0.0694, -0.0439, -0.1241, -0.0945,  0.0098, -0.0149,\n",
       "          0.0908, -0.0441,  0.0158,  0.0306,  0.0068, -0.0245,  0.1092,  0.1048,\n",
       "         -0.0538,  0.1173, -0.0758, -0.0488, -0.0967, -0.0025, -0.0170, -0.0321,\n",
       "          0.0734, -0.0954, -0.1208, -0.1127,  0.0859, -0.0950,  0.1225,  0.0045],\n",
       "        [-0.1045,  0.0357,  0.0484,  0.0454, -0.0695,  0.0878, -0.1082,  0.1216,\n",
       "         -0.0238, -0.0362,  0.0988,  0.0100,  0.0051, -0.0389, -0.1043,  0.0129,\n",
       "          0.0329, -0.0891,  0.0599, -0.0329, -0.0068, -0.0835, -0.0611, -0.0706,\n",
       "          0.1012,  0.0303, -0.0152,  0.1162, -0.0080,  0.1141, -0.0064,  0.0813,\n",
       "         -0.0527, -0.1239,  0.0972,  0.0434, -0.0530,  0.0874,  0.1063, -0.0270,\n",
       "          0.0414, -0.1117, -0.0275,  0.0808,  0.1247,  0.0324, -0.1080,  0.0253,\n",
       "         -0.0375,  0.0152, -0.0685, -0.0715,  0.1087, -0.0495, -0.0383,  0.0397,\n",
       "          0.0240,  0.0685, -0.0230, -0.1073, -0.0282, -0.0199, -0.0166, -0.1234]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.out.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0604, -0.0751,  0.0346,  0.1173,  0.0278, -0.0202, -0.0600, -0.0056,\n",
       "          0.1058, -0.0004, -0.0481, -0.1229, -0.1025, -0.0039, -0.0145, -0.0356,\n",
       "          0.0561, -0.1071,  0.1110,  0.0410, -0.1036, -0.0985,  0.0488,  0.0405,\n",
       "          0.0228, -0.0644, -0.0354,  0.1013,  0.1175, -0.0426, -0.0393,  0.0053,\n",
       "         -0.0845, -0.1012,  0.0067,  0.1082,  0.0537, -0.0222, -0.0471,  0.0936,\n",
       "         -0.0441,  0.0436,  0.0543, -0.0111, -0.0381, -0.0128, -0.0565,  0.0742,\n",
       "          0.0429, -0.0781, -0.0088,  0.0818, -0.1061, -0.0348, -0.0035, -0.0739,\n",
       "          0.0526, -0.0355, -0.0639,  0.0298, -0.0043,  0.1197,  0.0988,  0.0104],\n",
       "        [ 0.0324, -0.0218,  0.1191,  0.0195, -0.0769,  0.0806,  0.0238,  0.0496,\n",
       "          0.0556, -0.1210,  0.0379,  0.0094, -0.1067, -0.0264,  0.0368,  0.1144,\n",
       "          0.0932, -0.0140,  0.0369, -0.0371,  0.0051,  0.0121,  0.0783, -0.0279,\n",
       "          0.1240,  0.1120, -0.0090, -0.0803,  0.1102, -0.0858,  0.0063, -0.0901,\n",
       "         -0.1156,  0.1078, -0.0248,  0.0265,  0.0593, -0.0512, -0.0672, -0.0395,\n",
       "         -0.0408, -0.0408,  0.0870,  0.0408,  0.0670, -0.0878,  0.0835,  0.1158,\n",
       "          0.0793,  0.0144, -0.0834, -0.1085, -0.0440, -0.0846,  0.0255, -0.0813,\n",
       "         -0.0034,  0.0583, -0.0514,  0.0237,  0.0772,  0.0921,  0.0129, -0.1099],\n",
       "        [-0.1020,  0.0772,  0.1106,  0.0150,  0.0645,  0.0410,  0.1192,  0.0761,\n",
       "         -0.0887, -0.0846,  0.0446,  0.0684, -0.0788,  0.0619, -0.0847,  0.1233,\n",
       "         -0.0266,  0.1033, -0.1167,  0.0734, -0.0662,  0.1013, -0.0304, -0.0091,\n",
       "         -0.0216, -0.0792,  0.0380,  0.0613, -0.0923, -0.0197, -0.1154, -0.0283,\n",
       "         -0.0406,  0.0205,  0.0041, -0.0926,  0.0999, -0.0885, -0.0400,  0.1050,\n",
       "         -0.0078, -0.0754, -0.0862,  0.0287, -0.0333,  0.0788, -0.0890,  0.0389,\n",
       "         -0.1230, -0.0789, -0.0409, -0.1215, -0.0911,  0.0040,  0.0377, -0.0638,\n",
       "          0.0322, -0.0078, -0.0883,  0.0717, -0.0417,  0.0024,  0.0126,  0.0075],\n",
       "        [ 0.0768,  0.0348, -0.0555, -0.0513, -0.0081,  0.0284,  0.0715, -0.0041,\n",
       "         -0.1011,  0.0138, -0.0105,  0.1204,  0.0675,  0.0827,  0.1212, -0.0213,\n",
       "         -0.0946, -0.0802, -0.0498,  0.0059,  0.1088, -0.0979, -0.0367,  0.0124,\n",
       "         -0.0014,  0.0215,  0.0287,  0.0280,  0.0894,  0.0087, -0.0835,  0.0217,\n",
       "          0.0998,  0.0071,  0.0028, -0.0783,  0.0924, -0.0765, -0.0713,  0.0442,\n",
       "         -0.0344, -0.1022, -0.0342,  0.1062, -0.1115,  0.0665,  0.0142, -0.0844,\n",
       "         -0.0572,  0.0734,  0.1197, -0.0226,  0.0802,  0.0103,  0.0518,  0.0498,\n",
       "          0.0277, -0.0867, -0.1156,  0.0931,  0.0503, -0.0345,  0.0725, -0.0058],\n",
       "        [-0.0091, -0.0702, -0.0967, -0.0738,  0.0977,  0.0613, -0.1137,  0.0629,\n",
       "          0.1198,  0.0161, -0.0921, -0.1213, -0.1077, -0.0087, -0.0577,  0.0298,\n",
       "         -0.0641,  0.0710,  0.0570, -0.1222, -0.1091,  0.0811,  0.0065, -0.0363,\n",
       "          0.0125, -0.1226, -0.0096,  0.0693, -0.0888,  0.0789, -0.0353, -0.0966,\n",
       "         -0.1079, -0.0271, -0.0858, -0.0562,  0.0154,  0.0959,  0.0974,  0.0779,\n",
       "         -0.0711, -0.0830,  0.0641, -0.0954, -0.0203,  0.0171, -0.0629, -0.0722,\n",
       "         -0.1133, -0.0574, -0.0861, -0.0630, -0.0717, -0.0072,  0.0575,  0.0537,\n",
       "          0.0648, -0.0124,  0.0203,  0.0985,  0.0122, -0.0222, -0.0154,  0.0211],\n",
       "        [ 0.0383, -0.0344,  0.0799,  0.0622, -0.0695, -0.1111, -0.0339, -0.0717,\n",
       "          0.0368, -0.0667,  0.0044, -0.1111,  0.0961, -0.0714, -0.0937, -0.0837,\n",
       "         -0.0005, -0.0426,  0.0665, -0.0287,  0.0711,  0.0309, -0.0176, -0.0231,\n",
       "         -0.0032, -0.0251, -0.0175, -0.0063,  0.0982,  0.0147, -0.1214, -0.0884,\n",
       "         -0.0309, -0.0158,  0.0514,  0.0383,  0.0981,  0.0014, -0.0576,  0.0648,\n",
       "          0.0648, -0.0381,  0.0577, -0.1220, -0.0129, -0.0764,  0.0598,  0.1098,\n",
       "         -0.1068, -0.0154, -0.0016,  0.0538,  0.0220, -0.0786,  0.0642,  0.0473,\n",
       "         -0.0799, -0.0295, -0.1072, -0.0358, -0.0566, -0.1086,  0.0462, -0.1098],\n",
       "        [-0.0869,  0.0590,  0.0639, -0.0066, -0.0761, -0.1198,  0.0597,  0.0085,\n",
       "         -0.0875, -0.1130,  0.1021, -0.0306, -0.0275, -0.1239, -0.1205,  0.0152,\n",
       "         -0.1001, -0.0525,  0.0208,  0.0055, -0.1084, -0.1239, -0.0166,  0.1115,\n",
       "         -0.0910,  0.0728, -0.0363,  0.1175, -0.0543,  0.1076,  0.0621,  0.0907,\n",
       "         -0.0451, -0.0633,  0.0926,  0.0573, -0.0826,  0.0696, -0.0325,  0.0721,\n",
       "          0.0132,  0.0929, -0.0159, -0.0335,  0.0931,  0.0519,  0.0140, -0.1235,\n",
       "          0.0145, -0.1198, -0.0833,  0.0483, -0.0053,  0.0298,  0.0792, -0.0711,\n",
       "          0.0840, -0.1205, -0.0888, -0.0223,  0.1243,  0.0750, -0.0565,  0.0863],\n",
       "        [-0.0746, -0.0896, -0.0977, -0.0474,  0.0700, -0.0994,  0.0916, -0.0111,\n",
       "          0.0235, -0.0970,  0.0009,  0.0814,  0.0411,  0.0164,  0.0988, -0.0918,\n",
       "          0.0099,  0.1247,  0.1243, -0.0401, -0.0937, -0.0505, -0.0614, -0.0263,\n",
       "         -0.0080,  0.1223,  0.0018,  0.0087,  0.0467, -0.0128, -0.0770,  0.0883,\n",
       "          0.1120,  0.0857, -0.0426,  0.0962, -0.0438,  0.1228,  0.0796, -0.0098,\n",
       "          0.0650, -0.0879, -0.0169, -0.1109, -0.1226,  0.0996,  0.0724, -0.0562,\n",
       "          0.1058,  0.0129,  0.0260, -0.0868,  0.0895, -0.0565, -0.0656,  0.0868,\n",
       "          0.0710, -0.0978, -0.1232,  0.0526, -0.0492, -0.0750,  0.0915, -0.1094],\n",
       "        [-0.0060,  0.1010, -0.1244, -0.0826,  0.0582,  0.1041,  0.0046, -0.0598,\n",
       "          0.0709, -0.0059,  0.0108,  0.0111,  0.0956, -0.0687, -0.0793,  0.0174,\n",
       "          0.0381,  0.0632, -0.0539, -0.0541,  0.0504, -0.0429, -0.1041, -0.1245,\n",
       "         -0.1210,  0.0861,  0.0414,  0.0709, -0.0026, -0.0437,  0.1088, -0.0771,\n",
       "          0.0240,  0.0385,  0.0694, -0.0439, -0.1241, -0.0945,  0.0098, -0.0149,\n",
       "          0.0908, -0.0441,  0.0158,  0.0306,  0.0068, -0.0245,  0.1092,  0.1048,\n",
       "         -0.0538,  0.1173, -0.0758, -0.0488, -0.0967, -0.0025, -0.0170, -0.0321,\n",
       "          0.0734, -0.0954, -0.1208, -0.1127,  0.0859, -0.0950,  0.1225,  0.0045],\n",
       "        [-0.1045,  0.0357,  0.0484,  0.0454, -0.0695,  0.0878, -0.1082,  0.1216,\n",
       "         -0.0238, -0.0362,  0.0988,  0.0100,  0.0051, -0.0389, -0.1043,  0.0129,\n",
       "          0.0329, -0.0891,  0.0599, -0.0329, -0.0068, -0.0835, -0.0611, -0.0706,\n",
       "          0.1012,  0.0303, -0.0152,  0.1162, -0.0080,  0.1141, -0.0064,  0.0813,\n",
       "         -0.0527, -0.1239,  0.0972,  0.0434, -0.0530,  0.0874,  0.1063, -0.0270,\n",
       "          0.0414, -0.1117, -0.0275,  0.0808,  0.1247,  0.0324, -0.1080,  0.0253,\n",
       "         -0.0375,  0.0152, -0.0685, -0.0715,  0.1087, -0.0495, -0.0383,  0.0397,\n",
       "          0.0240,  0.0685, -0.0230, -0.1073, -0.0282, -0.0199, -0.0166, -0.1234]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.out.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "#### state_dict 저장, load\n",
    "state_dict = sample_model.state_dict()\n",
    "print(type(state_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr.weight', 'lr.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0604, -0.0751,  0.0346,  0.1173,  0.0278, -0.0202, -0.0600, -0.0056,\n",
       "          0.1058, -0.0004, -0.0481, -0.1229, -0.1025, -0.0039, -0.0145, -0.0356,\n",
       "          0.0561, -0.1071,  0.1110,  0.0410, -0.1036, -0.0985,  0.0488,  0.0405,\n",
       "          0.0228, -0.0644, -0.0354,  0.1013,  0.1175, -0.0426, -0.0393,  0.0053,\n",
       "         -0.0845, -0.1012,  0.0067,  0.1082,  0.0537, -0.0222, -0.0471,  0.0936,\n",
       "         -0.0441,  0.0436,  0.0543, -0.0111, -0.0381, -0.0128, -0.0565,  0.0742,\n",
       "          0.0429, -0.0781, -0.0088,  0.0818, -0.1061, -0.0348, -0.0035, -0.0739,\n",
       "          0.0526, -0.0355, -0.0639,  0.0298, -0.0043,  0.1197,  0.0988,  0.0104],\n",
       "        [ 0.0324, -0.0218,  0.1191,  0.0195, -0.0769,  0.0806,  0.0238,  0.0496,\n",
       "          0.0556, -0.1210,  0.0379,  0.0094, -0.1067, -0.0264,  0.0368,  0.1144,\n",
       "          0.0932, -0.0140,  0.0369, -0.0371,  0.0051,  0.0121,  0.0783, -0.0279,\n",
       "          0.1240,  0.1120, -0.0090, -0.0803,  0.1102, -0.0858,  0.0063, -0.0901,\n",
       "         -0.1156,  0.1078, -0.0248,  0.0265,  0.0593, -0.0512, -0.0672, -0.0395,\n",
       "         -0.0408, -0.0408,  0.0870,  0.0408,  0.0670, -0.0878,  0.0835,  0.1158,\n",
       "          0.0793,  0.0144, -0.0834, -0.1085, -0.0440, -0.0846,  0.0255, -0.0813,\n",
       "         -0.0034,  0.0583, -0.0514,  0.0237,  0.0772,  0.0921,  0.0129, -0.1099],\n",
       "        [-0.1020,  0.0772,  0.1106,  0.0150,  0.0645,  0.0410,  0.1192,  0.0761,\n",
       "         -0.0887, -0.0846,  0.0446,  0.0684, -0.0788,  0.0619, -0.0847,  0.1233,\n",
       "         -0.0266,  0.1033, -0.1167,  0.0734, -0.0662,  0.1013, -0.0304, -0.0091,\n",
       "         -0.0216, -0.0792,  0.0380,  0.0613, -0.0923, -0.0197, -0.1154, -0.0283,\n",
       "         -0.0406,  0.0205,  0.0041, -0.0926,  0.0999, -0.0885, -0.0400,  0.1050,\n",
       "         -0.0078, -0.0754, -0.0862,  0.0287, -0.0333,  0.0788, -0.0890,  0.0389,\n",
       "         -0.1230, -0.0789, -0.0409, -0.1215, -0.0911,  0.0040,  0.0377, -0.0638,\n",
       "          0.0322, -0.0078, -0.0883,  0.0717, -0.0417,  0.0024,  0.0126,  0.0075],\n",
       "        [ 0.0768,  0.0348, -0.0555, -0.0513, -0.0081,  0.0284,  0.0715, -0.0041,\n",
       "         -0.1011,  0.0138, -0.0105,  0.1204,  0.0675,  0.0827,  0.1212, -0.0213,\n",
       "         -0.0946, -0.0802, -0.0498,  0.0059,  0.1088, -0.0979, -0.0367,  0.0124,\n",
       "         -0.0014,  0.0215,  0.0287,  0.0280,  0.0894,  0.0087, -0.0835,  0.0217,\n",
       "          0.0998,  0.0071,  0.0028, -0.0783,  0.0924, -0.0765, -0.0713,  0.0442,\n",
       "         -0.0344, -0.1022, -0.0342,  0.1062, -0.1115,  0.0665,  0.0142, -0.0844,\n",
       "         -0.0572,  0.0734,  0.1197, -0.0226,  0.0802,  0.0103,  0.0518,  0.0498,\n",
       "          0.0277, -0.0867, -0.1156,  0.0931,  0.0503, -0.0345,  0.0725, -0.0058],\n",
       "        [-0.0091, -0.0702, -0.0967, -0.0738,  0.0977,  0.0613, -0.1137,  0.0629,\n",
       "          0.1198,  0.0161, -0.0921, -0.1213, -0.1077, -0.0087, -0.0577,  0.0298,\n",
       "         -0.0641,  0.0710,  0.0570, -0.1222, -0.1091,  0.0811,  0.0065, -0.0363,\n",
       "          0.0125, -0.1226, -0.0096,  0.0693, -0.0888,  0.0789, -0.0353, -0.0966,\n",
       "         -0.1079, -0.0271, -0.0858, -0.0562,  0.0154,  0.0959,  0.0974,  0.0779,\n",
       "         -0.0711, -0.0830,  0.0641, -0.0954, -0.0203,  0.0171, -0.0629, -0.0722,\n",
       "         -0.1133, -0.0574, -0.0861, -0.0630, -0.0717, -0.0072,  0.0575,  0.0537,\n",
       "          0.0648, -0.0124,  0.0203,  0.0985,  0.0122, -0.0222, -0.0154,  0.0211],\n",
       "        [ 0.0383, -0.0344,  0.0799,  0.0622, -0.0695, -0.1111, -0.0339, -0.0717,\n",
       "          0.0368, -0.0667,  0.0044, -0.1111,  0.0961, -0.0714, -0.0937, -0.0837,\n",
       "         -0.0005, -0.0426,  0.0665, -0.0287,  0.0711,  0.0309, -0.0176, -0.0231,\n",
       "         -0.0032, -0.0251, -0.0175, -0.0063,  0.0982,  0.0147, -0.1214, -0.0884,\n",
       "         -0.0309, -0.0158,  0.0514,  0.0383,  0.0981,  0.0014, -0.0576,  0.0648,\n",
       "          0.0648, -0.0381,  0.0577, -0.1220, -0.0129, -0.0764,  0.0598,  0.1098,\n",
       "         -0.1068, -0.0154, -0.0016,  0.0538,  0.0220, -0.0786,  0.0642,  0.0473,\n",
       "         -0.0799, -0.0295, -0.1072, -0.0358, -0.0566, -0.1086,  0.0462, -0.1098],\n",
       "        [-0.0869,  0.0590,  0.0639, -0.0066, -0.0761, -0.1198,  0.0597,  0.0085,\n",
       "         -0.0875, -0.1130,  0.1021, -0.0306, -0.0275, -0.1239, -0.1205,  0.0152,\n",
       "         -0.1001, -0.0525,  0.0208,  0.0055, -0.1084, -0.1239, -0.0166,  0.1115,\n",
       "         -0.0910,  0.0728, -0.0363,  0.1175, -0.0543,  0.1076,  0.0621,  0.0907,\n",
       "         -0.0451, -0.0633,  0.0926,  0.0573, -0.0826,  0.0696, -0.0325,  0.0721,\n",
       "          0.0132,  0.0929, -0.0159, -0.0335,  0.0931,  0.0519,  0.0140, -0.1235,\n",
       "          0.0145, -0.1198, -0.0833,  0.0483, -0.0053,  0.0298,  0.0792, -0.0711,\n",
       "          0.0840, -0.1205, -0.0888, -0.0223,  0.1243,  0.0750, -0.0565,  0.0863],\n",
       "        [-0.0746, -0.0896, -0.0977, -0.0474,  0.0700, -0.0994,  0.0916, -0.0111,\n",
       "          0.0235, -0.0970,  0.0009,  0.0814,  0.0411,  0.0164,  0.0988, -0.0918,\n",
       "          0.0099,  0.1247,  0.1243, -0.0401, -0.0937, -0.0505, -0.0614, -0.0263,\n",
       "         -0.0080,  0.1223,  0.0018,  0.0087,  0.0467, -0.0128, -0.0770,  0.0883,\n",
       "          0.1120,  0.0857, -0.0426,  0.0962, -0.0438,  0.1228,  0.0796, -0.0098,\n",
       "          0.0650, -0.0879, -0.0169, -0.1109, -0.1226,  0.0996,  0.0724, -0.0562,\n",
       "          0.1058,  0.0129,  0.0260, -0.0868,  0.0895, -0.0565, -0.0656,  0.0868,\n",
       "          0.0710, -0.0978, -0.1232,  0.0526, -0.0492, -0.0750,  0.0915, -0.1094],\n",
       "        [-0.0060,  0.1010, -0.1244, -0.0826,  0.0582,  0.1041,  0.0046, -0.0598,\n",
       "          0.0709, -0.0059,  0.0108,  0.0111,  0.0956, -0.0687, -0.0793,  0.0174,\n",
       "          0.0381,  0.0632, -0.0539, -0.0541,  0.0504, -0.0429, -0.1041, -0.1245,\n",
       "         -0.1210,  0.0861,  0.0414,  0.0709, -0.0026, -0.0437,  0.1088, -0.0771,\n",
       "          0.0240,  0.0385,  0.0694, -0.0439, -0.1241, -0.0945,  0.0098, -0.0149,\n",
       "          0.0908, -0.0441,  0.0158,  0.0306,  0.0068, -0.0245,  0.1092,  0.1048,\n",
       "         -0.0538,  0.1173, -0.0758, -0.0488, -0.0967, -0.0025, -0.0170, -0.0321,\n",
       "          0.0734, -0.0954, -0.1208, -0.1127,  0.0859, -0.0950,  0.1225,  0.0045],\n",
       "        [-0.1045,  0.0357,  0.0484,  0.0454, -0.0695,  0.0878, -0.1082,  0.1216,\n",
       "         -0.0238, -0.0362,  0.0988,  0.0100,  0.0051, -0.0389, -0.1043,  0.0129,\n",
       "          0.0329, -0.0891,  0.0599, -0.0329, -0.0068, -0.0835, -0.0611, -0.0706,\n",
       "          0.1012,  0.0303, -0.0152,  0.1162, -0.0080,  0.1141, -0.0064,  0.0813,\n",
       "         -0.0527, -0.1239,  0.0972,  0.0434, -0.0530,  0.0874,  0.1063, -0.0270,\n",
       "          0.0414, -0.1117, -0.0275,  0.0808,  0.1247,  0.0324, -0.1080,  0.0253,\n",
       "         -0.0375,  0.0152, -0.0685, -0.0715,  0.1087, -0.0495, -0.0383,  0.0397,\n",
       "          0.0240,  0.0685, -0.0230, -0.1073, -0.0282, -0.0199, -0.0166, -0.1234]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['out.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, \"saved_models/sample_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state_dict = torch.load(\"saved_models/sample_model_weights.pth\")\n",
    "type(load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델을 생성하고 load한 state_dict의 파라미터러들로  변경(덮어쓰기)\n",
    "new_model = MyNetwork()\n",
    "new_model.load_state_dict(load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0604, -0.0751,  0.0346,  0.1173,  0.0278, -0.0202, -0.0600, -0.0056,\n",
       "          0.1058, -0.0004, -0.0481, -0.1229, -0.1025, -0.0039, -0.0145, -0.0356,\n",
       "          0.0561, -0.1071,  0.1110,  0.0410, -0.1036, -0.0985,  0.0488,  0.0405,\n",
       "          0.0228, -0.0644, -0.0354,  0.1013,  0.1175, -0.0426, -0.0393,  0.0053,\n",
       "         -0.0845, -0.1012,  0.0067,  0.1082,  0.0537, -0.0222, -0.0471,  0.0936,\n",
       "         -0.0441,  0.0436,  0.0543, -0.0111, -0.0381, -0.0128, -0.0565,  0.0742,\n",
       "          0.0429, -0.0781, -0.0088,  0.0818, -0.1061, -0.0348, -0.0035, -0.0739,\n",
       "          0.0526, -0.0355, -0.0639,  0.0298, -0.0043,  0.1197,  0.0988,  0.0104],\n",
       "        [ 0.0324, -0.0218,  0.1191,  0.0195, -0.0769,  0.0806,  0.0238,  0.0496,\n",
       "          0.0556, -0.1210,  0.0379,  0.0094, -0.1067, -0.0264,  0.0368,  0.1144,\n",
       "          0.0932, -0.0140,  0.0369, -0.0371,  0.0051,  0.0121,  0.0783, -0.0279,\n",
       "          0.1240,  0.1120, -0.0090, -0.0803,  0.1102, -0.0858,  0.0063, -0.0901,\n",
       "         -0.1156,  0.1078, -0.0248,  0.0265,  0.0593, -0.0512, -0.0672, -0.0395,\n",
       "         -0.0408, -0.0408,  0.0870,  0.0408,  0.0670, -0.0878,  0.0835,  0.1158,\n",
       "          0.0793,  0.0144, -0.0834, -0.1085, -0.0440, -0.0846,  0.0255, -0.0813,\n",
       "         -0.0034,  0.0583, -0.0514,  0.0237,  0.0772,  0.0921,  0.0129, -0.1099],\n",
       "        [-0.1020,  0.0772,  0.1106,  0.0150,  0.0645,  0.0410,  0.1192,  0.0761,\n",
       "         -0.0887, -0.0846,  0.0446,  0.0684, -0.0788,  0.0619, -0.0847,  0.1233,\n",
       "         -0.0266,  0.1033, -0.1167,  0.0734, -0.0662,  0.1013, -0.0304, -0.0091,\n",
       "         -0.0216, -0.0792,  0.0380,  0.0613, -0.0923, -0.0197, -0.1154, -0.0283,\n",
       "         -0.0406,  0.0205,  0.0041, -0.0926,  0.0999, -0.0885, -0.0400,  0.1050,\n",
       "         -0.0078, -0.0754, -0.0862,  0.0287, -0.0333,  0.0788, -0.0890,  0.0389,\n",
       "         -0.1230, -0.0789, -0.0409, -0.1215, -0.0911,  0.0040,  0.0377, -0.0638,\n",
       "          0.0322, -0.0078, -0.0883,  0.0717, -0.0417,  0.0024,  0.0126,  0.0075],\n",
       "        [ 0.0768,  0.0348, -0.0555, -0.0513, -0.0081,  0.0284,  0.0715, -0.0041,\n",
       "         -0.1011,  0.0138, -0.0105,  0.1204,  0.0675,  0.0827,  0.1212, -0.0213,\n",
       "         -0.0946, -0.0802, -0.0498,  0.0059,  0.1088, -0.0979, -0.0367,  0.0124,\n",
       "         -0.0014,  0.0215,  0.0287,  0.0280,  0.0894,  0.0087, -0.0835,  0.0217,\n",
       "          0.0998,  0.0071,  0.0028, -0.0783,  0.0924, -0.0765, -0.0713,  0.0442,\n",
       "         -0.0344, -0.1022, -0.0342,  0.1062, -0.1115,  0.0665,  0.0142, -0.0844,\n",
       "         -0.0572,  0.0734,  0.1197, -0.0226,  0.0802,  0.0103,  0.0518,  0.0498,\n",
       "          0.0277, -0.0867, -0.1156,  0.0931,  0.0503, -0.0345,  0.0725, -0.0058],\n",
       "        [-0.0091, -0.0702, -0.0967, -0.0738,  0.0977,  0.0613, -0.1137,  0.0629,\n",
       "          0.1198,  0.0161, -0.0921, -0.1213, -0.1077, -0.0087, -0.0577,  0.0298,\n",
       "         -0.0641,  0.0710,  0.0570, -0.1222, -0.1091,  0.0811,  0.0065, -0.0363,\n",
       "          0.0125, -0.1226, -0.0096,  0.0693, -0.0888,  0.0789, -0.0353, -0.0966,\n",
       "         -0.1079, -0.0271, -0.0858, -0.0562,  0.0154,  0.0959,  0.0974,  0.0779,\n",
       "         -0.0711, -0.0830,  0.0641, -0.0954, -0.0203,  0.0171, -0.0629, -0.0722,\n",
       "         -0.1133, -0.0574, -0.0861, -0.0630, -0.0717, -0.0072,  0.0575,  0.0537,\n",
       "          0.0648, -0.0124,  0.0203,  0.0985,  0.0122, -0.0222, -0.0154,  0.0211],\n",
       "        [ 0.0383, -0.0344,  0.0799,  0.0622, -0.0695, -0.1111, -0.0339, -0.0717,\n",
       "          0.0368, -0.0667,  0.0044, -0.1111,  0.0961, -0.0714, -0.0937, -0.0837,\n",
       "         -0.0005, -0.0426,  0.0665, -0.0287,  0.0711,  0.0309, -0.0176, -0.0231,\n",
       "         -0.0032, -0.0251, -0.0175, -0.0063,  0.0982,  0.0147, -0.1214, -0.0884,\n",
       "         -0.0309, -0.0158,  0.0514,  0.0383,  0.0981,  0.0014, -0.0576,  0.0648,\n",
       "          0.0648, -0.0381,  0.0577, -0.1220, -0.0129, -0.0764,  0.0598,  0.1098,\n",
       "         -0.1068, -0.0154, -0.0016,  0.0538,  0.0220, -0.0786,  0.0642,  0.0473,\n",
       "         -0.0799, -0.0295, -0.1072, -0.0358, -0.0566, -0.1086,  0.0462, -0.1098],\n",
       "        [-0.0869,  0.0590,  0.0639, -0.0066, -0.0761, -0.1198,  0.0597,  0.0085,\n",
       "         -0.0875, -0.1130,  0.1021, -0.0306, -0.0275, -0.1239, -0.1205,  0.0152,\n",
       "         -0.1001, -0.0525,  0.0208,  0.0055, -0.1084, -0.1239, -0.0166,  0.1115,\n",
       "         -0.0910,  0.0728, -0.0363,  0.1175, -0.0543,  0.1076,  0.0621,  0.0907,\n",
       "         -0.0451, -0.0633,  0.0926,  0.0573, -0.0826,  0.0696, -0.0325,  0.0721,\n",
       "          0.0132,  0.0929, -0.0159, -0.0335,  0.0931,  0.0519,  0.0140, -0.1235,\n",
       "          0.0145, -0.1198, -0.0833,  0.0483, -0.0053,  0.0298,  0.0792, -0.0711,\n",
       "          0.0840, -0.1205, -0.0888, -0.0223,  0.1243,  0.0750, -0.0565,  0.0863],\n",
       "        [-0.0746, -0.0896, -0.0977, -0.0474,  0.0700, -0.0994,  0.0916, -0.0111,\n",
       "          0.0235, -0.0970,  0.0009,  0.0814,  0.0411,  0.0164,  0.0988, -0.0918,\n",
       "          0.0099,  0.1247,  0.1243, -0.0401, -0.0937, -0.0505, -0.0614, -0.0263,\n",
       "         -0.0080,  0.1223,  0.0018,  0.0087,  0.0467, -0.0128, -0.0770,  0.0883,\n",
       "          0.1120,  0.0857, -0.0426,  0.0962, -0.0438,  0.1228,  0.0796, -0.0098,\n",
       "          0.0650, -0.0879, -0.0169, -0.1109, -0.1226,  0.0996,  0.0724, -0.0562,\n",
       "          0.1058,  0.0129,  0.0260, -0.0868,  0.0895, -0.0565, -0.0656,  0.0868,\n",
       "          0.0710, -0.0978, -0.1232,  0.0526, -0.0492, -0.0750,  0.0915, -0.1094],\n",
       "        [-0.0060,  0.1010, -0.1244, -0.0826,  0.0582,  0.1041,  0.0046, -0.0598,\n",
       "          0.0709, -0.0059,  0.0108,  0.0111,  0.0956, -0.0687, -0.0793,  0.0174,\n",
       "          0.0381,  0.0632, -0.0539, -0.0541,  0.0504, -0.0429, -0.1041, -0.1245,\n",
       "         -0.1210,  0.0861,  0.0414,  0.0709, -0.0026, -0.0437,  0.1088, -0.0771,\n",
       "          0.0240,  0.0385,  0.0694, -0.0439, -0.1241, -0.0945,  0.0098, -0.0149,\n",
       "          0.0908, -0.0441,  0.0158,  0.0306,  0.0068, -0.0245,  0.1092,  0.1048,\n",
       "         -0.0538,  0.1173, -0.0758, -0.0488, -0.0967, -0.0025, -0.0170, -0.0321,\n",
       "          0.0734, -0.0954, -0.1208, -0.1127,  0.0859, -0.0950,  0.1225,  0.0045],\n",
       "        [-0.1045,  0.0357,  0.0484,  0.0454, -0.0695,  0.0878, -0.1082,  0.1216,\n",
       "         -0.0238, -0.0362,  0.0988,  0.0100,  0.0051, -0.0389, -0.1043,  0.0129,\n",
       "          0.0329, -0.0891,  0.0599, -0.0329, -0.0068, -0.0835, -0.0611, -0.0706,\n",
       "          0.1012,  0.0303, -0.0152,  0.1162, -0.0080,  0.1141, -0.0064,  0.0813,\n",
       "         -0.0527, -0.1239,  0.0972,  0.0434, -0.0530,  0.0874,  0.1063, -0.0270,\n",
       "          0.0414, -0.1117, -0.0275,  0.0808,  0.1247,  0.0324, -0.1080,  0.0253,\n",
       "         -0.0375,  0.0152, -0.0685, -0.0715,  0.1087, -0.0495, -0.0383,  0.0397,\n",
       "          0.0240,  0.0685, -0.0230, -0.1073, -0.0282, -0.0199, -0.0166, -0.1234]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.out.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn\n",
    "### torchinfo 패키지 -> pytorch 모델 구조(정보)를 출력해주는 패키지.\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN (Artificial Neural Network)\n",
    "    - Fully Connected Layer(nn.Linear)로 구성된 네트워크모델(딥러닝 모델)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 딥러닝 모델은 선형함수 기반 -> Feature Scaling 전처리를 하면 성능이 올라간다.\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\"  if torch.backends.mps.is_available() else \"cpu\" # m1, m2\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "# 1. Data 준비 -> DataLoader(Dataset)\n",
    "df = pd.read_csv(\"data/boston_hosing.csv\")\n",
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X, y 분리\n",
    "X_boston = df.drop(columns=\"MEDV\").values\n",
    "y_boston = df['MEDV'].values.reshape(-1, 1)\n",
    "X_boston.shape, y_boston.shape, X_boston.dtype, y_boston.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_boston = X_boston.astype(\"float32\")\n",
    "y_boston = y_boston.astype(\"float32\")\n",
    "X_boston.dtype, y_boston.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (102, 13) (404, 1) (102, 1)\n"
     ]
    }
   ],
   "source": [
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_boston, y_boston, test_size=0.2, random_state=0\n",
    ") # 분류: stratify=y, 회귀는 설정하지 않음.\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Feature간의 scaling-값의 범위/단위 을 통일시킨다.)\n",
    "## 전처리 - trainset기반으로 학습하고 나머지를 변경.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset 생성 - TensorDataset\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled), # X\n",
    "    torch.tensor(y_train)# y\n",
    ")\n",
    "testset = TensorDataset(torch.tensor(X_test_scaled), torch.tensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step수: 2 1\n"
     ]
    }
   ],
   "source": [
    "### DataLoader\n",
    "train_loader = DataLoader(\n",
    "    trainset, # Dataset\n",
    "    200,      # batch_size\n",
    "    shuffle=True, # epoch이 끝날때 마다 전체 데이터를 섞을지 여부.\n",
    "    drop_last=True, # 마지막 batch의 데이터개수가 batch_size 보다 적을 경우 학습하지 않는다\n",
    ")\n",
    "test_loader = DataLoader(testset, len(testset))\n",
    "print(\"step수:\", len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape) # (404, 13: input shape)\n",
    "print(y_train.shape)          # (404, 1: output shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 -> nn.Module을 상속,\n",
    "#     __init__(): 초기화,  forward(): 추론 처리.\n",
    "class BostonModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 1. 상위(부모)클래스의 __init__()을 호출해서 초기화.(필수)\n",
    "        super().__init__()\n",
    "        # instance 변수 초기화 -> forword()에서 사용할 함수들(Layer) 초기화.\n",
    "        # 입력 features 개수: 13 -> in_features: 13\n",
    "        self.lr1 = nn.Linear(in_features=13, out_features=32)\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        self.lr3 = nn.Linear(16, 1)  # output layer(집값 1개를 추론: out_features: 1)\n",
    "        # 활성함수 Layer - 파라미터가 없기 때문에 하나를 여러번 사용해도 된다.\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # __init__()에서 초기화한 Layer함수들을 이용해서 추론작업흐름을 정의\n",
    "        out = self.lr1(X)    # 선형함수-Linear()\n",
    "        out = self.relu(out)# 비선형 함수(활성) - ReLU\n",
    "        out = self.lr2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # lr3의 출력이 모델의 최종 출력(모델이 추정한 값.)\n",
    "        ### 회귀: 추정대상-집값 1개.\n",
    "        #### out_features: 1, activation함수는 사용안함.\n",
    "        #(추정할 값의 scale따라 activation함수를 사용할 수있다.\n",
    "        #  0 ~ 1: logistic 함수(nn.Sigmoid), -1 ~ 1: hyperbolic tangent(nn.Tanh)\n",
    "        out = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델  instance 생성\n",
    "boston_model = BostonModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BostonModel(\n",
      "  (lr1): Linear(in_features=13, out_features=32, bias=True)\n",
      "  (lr2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (lr3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 모델 구조를 확인\n",
    "print(boston_model) \n",
    "# 모델의 attribute 중 Layer들을 출력.\n",
    "# (변수이름): instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 448\n",
       "├─ReLU: 1-2                              [100, 32]                 --\n",
       "├─Linear: 1-3                            [100, 16]                 528\n",
       "├─ReLU: 1-4                              [100, 16]                 --\n",
       "├─Linear: 1-5                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torchinfo.summary(모델)\n",
    "# torchinfo.summary(모델, dummy_input_shape)\n",
    "summary(boston_model, (100, 13))  # (batch: 100, feature수: 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터 개수 계산\n",
    "## Linear(in, out)   in * out + out   (in: 개별 unit의 weight 수,   out: unit을 몇개 만들기 개수. bias는 unit당 한개씩 필요.)\n",
    "## Linear(13, 32)   13 * 32 + 32\n",
    "13 * 32 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000002E37702AEA0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델에서 layer 함수를 조회\n",
    "layer1 = boston_model.lr1\n",
    "## Layer함수의 weight 조회\n",
    "w1 = layer1.weight\n",
    "## layer함수의 bias 조회\n",
    "b = layer1.bias\n",
    "# weight/bias : requires_grad=True\n",
    "### 모델의 전체 weight들과 parameter들 ===> optimizer에 모델의 파라미터들 전달할 때 사용.\n",
    "boston_model.parameters()  # generator: 반복할 때마다 layer 순서대로 weight->bias 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.dtype, b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. Train(학습-fitting) -> 학습 + 검증\n",
    "######### 모델, loss함수, optimizer 준비.\n",
    "epochs = 1000  # 몇 epoch 학습할지. (epoch: 학습할 때 전체 train data 사용 단위.)\n",
    "lr = 0.001        # 학습율->optimizer에 설정할 값\n",
    "\n",
    "model = BostonModel().to(device) # 모델을 계산할 device를 설정. \n",
    "loss_fn = nn.MSELoss()   # 회귀의 loss함수: mean_squared_error\n",
    "optimizer = optim.RMSprop(\n",
    "    model.parameters(), #모델의 파라미터들.\n",
    "    lr=lr\n",
    ") # optimizer생성 - 모델파라미터들, 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/1000] train loss: 587.8971252441406 validation loss: 558.6966552734375\n",
      "[0002/1000] train loss: 575.7878723144531 validation loss: 543.8118286132812\n",
      "[0003/1000] train loss: 562.3133239746094 validation loss: 526.6119384765625\n",
      "[0004/1000] train loss: 543.8435668945312 validation loss: 507.4977111816406\n",
      "[0005/1000] train loss: 519.0035400390625 validation loss: 487.78094482421875\n",
      "[0006/1000] train loss: 499.0879821777344 validation loss: 467.2537536621094\n",
      "[0007/1000] train loss: 481.8866424560547 validation loss: 446.02789306640625\n",
      "[0008/1000] train loss: 458.4764099121094 validation loss: 424.243408203125\n",
      "[0009/1000] train loss: 435.11769104003906 validation loss: 402.14434814453125\n",
      "[0010/1000] train loss: 414.81842041015625 validation loss: 379.9656982421875\n",
      "[0011/1000] train loss: 390.37620544433594 validation loss: 357.920166015625\n",
      "[0012/1000] train loss: 368.2247772216797 validation loss: 336.3553466796875\n",
      "[0013/1000] train loss: 346.30540466308594 validation loss: 315.3241271972656\n",
      "[0014/1000] train loss: 321.18772888183594 validation loss: 295.1075744628906\n",
      "[0015/1000] train loss: 300.77162170410156 validation loss: 275.8275451660156\n",
      "[0016/1000] train loss: 278.8628234863281 validation loss: 257.5650329589844\n",
      "[0017/1000] train loss: 259.0021667480469 validation loss: 240.4922637939453\n",
      "[0018/1000] train loss: 242.19703674316406 validation loss: 224.3586883544922\n",
      "[0019/1000] train loss: 223.75970458984375 validation loss: 209.50726318359375\n",
      "[0020/1000] train loss: 206.62200164794922 validation loss: 195.8209991455078\n",
      "[0021/1000] train loss: 192.3881607055664 validation loss: 183.26878356933594\n",
      "[0022/1000] train loss: 177.04627227783203 validation loss: 171.81565856933594\n",
      "[0023/1000] train loss: 162.0621795654297 validation loss: 161.5853271484375\n",
      "[0024/1000] train loss: 149.53587341308594 validation loss: 152.3497314453125\n",
      "[0025/1000] train loss: 143.5240478515625 validation loss: 143.85482788085938\n",
      "[0026/1000] train loss: 133.92654418945312 validation loss: 136.1580810546875\n",
      "[0027/1000] train loss: 125.19807052612305 validation loss: 129.25875854492188\n",
      "[0028/1000] train loss: 116.61981582641602 validation loss: 123.01692962646484\n",
      "[0029/1000] train loss: 109.8244743347168 validation loss: 117.3839111328125\n",
      "[0030/1000] train loss: 101.63551330566406 validation loss: 112.27857971191406\n",
      "[0031/1000] train loss: 96.83762741088867 validation loss: 107.60205841064453\n",
      "[0032/1000] train loss: 92.37737274169922 validation loss: 103.29011535644531\n",
      "[0033/1000] train loss: 87.05233383178711 validation loss: 99.3875503540039\n",
      "[0034/1000] train loss: 82.72599029541016 validation loss: 95.72216033935547\n",
      "[0035/1000] train loss: 78.55900955200195 validation loss: 92.3649673461914\n",
      "[0036/1000] train loss: 73.65372848510742 validation loss: 89.3003921508789\n",
      "[0037/1000] train loss: 71.14128494262695 validation loss: 86.40115356445312\n",
      "[0038/1000] train loss: 68.13648223876953 validation loss: 83.67908477783203\n",
      "[0039/1000] train loss: 64.33301734924316 validation loss: 81.1898422241211\n",
      "[0040/1000] train loss: 62.140750885009766 validation loss: 78.86299133300781\n",
      "[0041/1000] train loss: 59.55754280090332 validation loss: 76.67405700683594\n",
      "[0042/1000] train loss: 56.66524124145508 validation loss: 74.6069564819336\n",
      "[0043/1000] train loss: 54.595075607299805 validation loss: 72.6905288696289\n",
      "[0044/1000] train loss: 52.321393966674805 validation loss: 70.906005859375\n",
      "[0045/1000] train loss: 50.35394096374512 validation loss: 69.20897674560547\n",
      "[0046/1000] train loss: 48.70669746398926 validation loss: 67.61640167236328\n",
      "[0047/1000] train loss: 46.9556941986084 validation loss: 66.1236801147461\n",
      "[0048/1000] train loss: 45.29427909851074 validation loss: 64.69998168945312\n",
      "[0049/1000] train loss: 43.71759033203125 validation loss: 63.37760543823242\n",
      "[0050/1000] train loss: 42.353593826293945 validation loss: 62.123783111572266\n",
      "[0051/1000] train loss: 41.037282943725586 validation loss: 60.953670501708984\n",
      "[0052/1000] train loss: 39.470985412597656 validation loss: 59.88942337036133\n",
      "[0053/1000] train loss: 38.62764358520508 validation loss: 58.867401123046875\n",
      "[0054/1000] train loss: 37.23270320892334 validation loss: 57.90716552734375\n",
      "[0055/1000] train loss: 36.404714584350586 validation loss: 56.9693489074707\n",
      "[0056/1000] train loss: 35.280290603637695 validation loss: 56.09711837768555\n",
      "[0057/1000] train loss: 34.4052677154541 validation loss: 55.281009674072266\n",
      "[0058/1000] train loss: 33.66229820251465 validation loss: 54.5146369934082\n",
      "[0059/1000] train loss: 33.01265907287598 validation loss: 53.76801300048828\n",
      "[0060/1000] train loss: 32.20981693267822 validation loss: 53.032840728759766\n",
      "[0061/1000] train loss: 31.399956703186035 validation loss: 52.4028434753418\n",
      "[0062/1000] train loss: 30.867886543273926 validation loss: 51.777462005615234\n",
      "[0063/1000] train loss: 30.41309642791748 validation loss: 51.168556213378906\n",
      "[0064/1000] train loss: 29.729172706604004 validation loss: 50.61357498168945\n",
      "[0065/1000] train loss: 29.00924777984619 validation loss: 50.06206130981445\n",
      "[0066/1000] train loss: 28.597856521606445 validation loss: 49.54377365112305\n",
      "[0067/1000] train loss: 27.004127502441406 validation loss: 49.13115692138672\n",
      "[0068/1000] train loss: 27.757200241088867 validation loss: 48.66608428955078\n",
      "[0069/1000] train loss: 26.21055030822754 validation loss: 48.283714294433594\n",
      "[0070/1000] train loss: 26.96000385284424 validation loss: 47.842411041259766\n",
      "[0071/1000] train loss: 26.419240951538086 validation loss: 47.440948486328125\n",
      "[0072/1000] train loss: 26.219682693481445 validation loss: 47.02034378051758\n",
      "[0073/1000] train loss: 25.99100112915039 validation loss: 46.65391540527344\n",
      "[0074/1000] train loss: 25.505260467529297 validation loss: 46.265926361083984\n",
      "[0075/1000] train loss: 24.07570171356201 validation loss: 45.988319396972656\n",
      "[0076/1000] train loss: 24.98104476928711 validation loss: 45.64731216430664\n",
      "[0077/1000] train loss: 24.72101593017578 validation loss: 45.30266189575195\n",
      "[0078/1000] train loss: 24.448508262634277 validation loss: 44.95675277709961\n",
      "[0079/1000] train loss: 24.28823947906494 validation loss: 44.64453125\n",
      "[0080/1000] train loss: 23.710880279541016 validation loss: 44.36124038696289\n",
      "[0081/1000] train loss: 23.73462963104248 validation loss: 44.09685516357422\n",
      "[0082/1000] train loss: 23.617424964904785 validation loss: 43.804683685302734\n",
      "[0083/1000] train loss: 23.455267906188965 validation loss: 43.528682708740234\n",
      "[0084/1000] train loss: 23.178150177001953 validation loss: 43.28025817871094\n",
      "[0085/1000] train loss: 20.63944435119629 validation loss: 43.093963623046875\n",
      "[0086/1000] train loss: 22.904358863830566 validation loss: 42.807106018066406\n",
      "[0087/1000] train loss: 22.667838096618652 validation loss: 42.56178283691406\n",
      "[0088/1000] train loss: 21.954360008239746 validation loss: 42.30312728881836\n",
      "[0089/1000] train loss: 22.362112998962402 validation loss: 42.04964065551758\n",
      "[0090/1000] train loss: 22.140978813171387 validation loss: 41.8038215637207\n",
      "[0091/1000] train loss: 22.092793464660645 validation loss: 41.536170959472656\n",
      "[0092/1000] train loss: 21.464287757873535 validation loss: 41.31717300415039\n",
      "[0093/1000] train loss: 21.775999069213867 validation loss: 41.085697174072266\n",
      "[0094/1000] train loss: 21.3381290435791 validation loss: 40.86486053466797\n",
      "[0095/1000] train loss: 21.33376693725586 validation loss: 40.64149475097656\n",
      "[0096/1000] train loss: 21.28874683380127 validation loss: 40.446266174316406\n",
      "[0097/1000] train loss: 21.18666172027588 validation loss: 40.23468017578125\n",
      "[0098/1000] train loss: 20.781213760375977 validation loss: 39.99898147583008\n",
      "[0099/1000] train loss: 20.85824966430664 validation loss: 39.791683197021484\n",
      "[0100/1000] train loss: 20.844375610351562 validation loss: 39.595741271972656\n",
      "[0101/1000] train loss: 20.714301109313965 validation loss: 39.41299057006836\n",
      "[0102/1000] train loss: 20.60171413421631 validation loss: 39.23739242553711\n",
      "[0103/1000] train loss: 20.445297241210938 validation loss: 39.044281005859375\n",
      "[0104/1000] train loss: 20.253250122070312 validation loss: 38.85575866699219\n",
      "[0105/1000] train loss: 20.299031257629395 validation loss: 38.68963623046875\n",
      "[0106/1000] train loss: 19.725706100463867 validation loss: 38.51581573486328\n",
      "[0107/1000] train loss: 20.066731452941895 validation loss: 38.32422637939453\n",
      "[0108/1000] train loss: 19.86533832550049 validation loss: 38.13642501831055\n",
      "[0109/1000] train loss: 19.70701789855957 validation loss: 37.9936637878418\n",
      "[0110/1000] train loss: 19.71517276763916 validation loss: 37.84229278564453\n",
      "[0111/1000] train loss: 19.635640144348145 validation loss: 37.68739700317383\n",
      "[0112/1000] train loss: 19.51699924468994 validation loss: 37.54525375366211\n",
      "[0113/1000] train loss: 19.42980432510376 validation loss: 37.39204406738281\n",
      "[0114/1000] train loss: 19.107834815979004 validation loss: 37.25261688232422\n",
      "[0115/1000] train loss: 19.02554225921631 validation loss: 37.05576705932617\n",
      "[0116/1000] train loss: 19.11947250366211 validation loss: 36.89173889160156\n",
      "[0117/1000] train loss: 18.851017951965332 validation loss: 36.699371337890625\n",
      "[0118/1000] train loss: 18.805410385131836 validation loss: 36.51529312133789\n",
      "[0119/1000] train loss: 18.768900871276855 validation loss: 36.37091827392578\n",
      "[0120/1000] train loss: 18.55588150024414 validation loss: 36.198856353759766\n",
      "[0121/1000] train loss: 18.686641693115234 validation loss: 36.05413055419922\n",
      "[0122/1000] train loss: 18.492972373962402 validation loss: 35.903446197509766\n",
      "[0123/1000] train loss: 18.44654655456543 validation loss: 35.730472564697266\n",
      "[0124/1000] train loss: 18.29551410675049 validation loss: 35.61045837402344\n",
      "[0125/1000] train loss: 18.16771697998047 validation loss: 35.48040771484375\n",
      "[0126/1000] train loss: 17.86397361755371 validation loss: 35.32312774658203\n",
      "[0127/1000] train loss: 17.22158718109131 validation loss: 35.283790588378906\n",
      "[0128/1000] train loss: 17.976460456848145 validation loss: 35.1525764465332\n",
      "[0129/1000] train loss: 17.89085102081299 validation loss: 35.0227165222168\n",
      "[0130/1000] train loss: 17.823851585388184 validation loss: 34.911495208740234\n",
      "[0131/1000] train loss: 17.340465545654297 validation loss: 34.758907318115234\n",
      "[0132/1000] train loss: 17.174677848815918 validation loss: 34.542518615722656\n",
      "[0133/1000] train loss: 17.51716184616089 validation loss: 34.41484832763672\n",
      "[0134/1000] train loss: 17.43553638458252 validation loss: 34.2682991027832\n",
      "[0135/1000] train loss: 17.175599575042725 validation loss: 34.135860443115234\n",
      "[0136/1000] train loss: 17.128393173217773 validation loss: 34.010643005371094\n",
      "[0137/1000] train loss: 17.164211750030518 validation loss: 33.87775421142578\n",
      "[0138/1000] train loss: 17.033050537109375 validation loss: 33.79676055908203\n",
      "[0139/1000] train loss: 16.742509841918945 validation loss: 33.62990188598633\n",
      "[0140/1000] train loss: 16.779032707214355 validation loss: 33.53238296508789\n",
      "[0141/1000] train loss: 16.67489719390869 validation loss: 33.42464065551758\n",
      "[0142/1000] train loss: 16.5423264503479 validation loss: 33.29547882080078\n",
      "[0143/1000] train loss: 16.631125450134277 validation loss: 33.14799118041992\n",
      "[0144/1000] train loss: 16.468668460845947 validation loss: 33.0178337097168\n",
      "[0145/1000] train loss: 16.317932605743408 validation loss: 32.93269729614258\n",
      "[0146/1000] train loss: 16.386826515197754 validation loss: 32.77708053588867\n",
      "[0147/1000] train loss: 16.346349716186523 validation loss: 32.70809555053711\n",
      "[0148/1000] train loss: 16.171870708465576 validation loss: 32.6044807434082\n",
      "[0149/1000] train loss: 16.11386489868164 validation loss: 32.492950439453125\n",
      "[0150/1000] train loss: 16.05810260772705 validation loss: 32.34429168701172\n",
      "[0151/1000] train loss: 15.993312358856201 validation loss: 32.17928695678711\n",
      "[0152/1000] train loss: 15.739512920379639 validation loss: 32.117862701416016\n",
      "[0153/1000] train loss: 15.163146495819092 validation loss: 32.037628173828125\n",
      "[0154/1000] train loss: 15.61609935760498 validation loss: 31.925676345825195\n",
      "[0155/1000] train loss: 15.572909355163574 validation loss: 31.80522346496582\n",
      "[0156/1000] train loss: 15.55788516998291 validation loss: 31.68075942993164\n",
      "[0157/1000] train loss: 15.226470947265625 validation loss: 31.555524826049805\n",
      "[0158/1000] train loss: 15.418912410736084 validation loss: 31.470592498779297\n",
      "[0159/1000] train loss: 15.364176750183105 validation loss: 31.348997116088867\n",
      "[0160/1000] train loss: 15.225645542144775 validation loss: 31.273069381713867\n",
      "[0161/1000] train loss: 15.229869365692139 validation loss: 31.151113510131836\n",
      "[0162/1000] train loss: 15.112528800964355 validation loss: 31.032920837402344\n",
      "[0163/1000] train loss: 15.148389339447021 validation loss: 30.885482788085938\n",
      "[0164/1000] train loss: 14.892841815948486 validation loss: 30.777389526367188\n",
      "[0165/1000] train loss: 14.783792495727539 validation loss: 30.711137771606445\n",
      "[0166/1000] train loss: 14.72872018814087 validation loss: 30.678333282470703\n",
      "[0167/1000] train loss: 14.802728652954102 validation loss: 30.55675506591797\n",
      "[0168/1000] train loss: 14.606914520263672 validation loss: 30.442296981811523\n",
      "[0169/1000] train loss: 14.657411098480225 validation loss: 30.344194412231445\n",
      "[0170/1000] train loss: 14.591501235961914 validation loss: 30.220474243164062\n",
      "[0171/1000] train loss: 14.433204650878906 validation loss: 30.187610626220703\n",
      "[0172/1000] train loss: 14.27177906036377 validation loss: 30.04500389099121\n",
      "[0173/1000] train loss: 14.324458122253418 validation loss: 29.925939559936523\n",
      "[0174/1000] train loss: 14.292042255401611 validation loss: 29.840112686157227\n",
      "[0175/1000] train loss: 14.11557149887085 validation loss: 29.74028205871582\n",
      "[0176/1000] train loss: 14.061655521392822 validation loss: 29.639429092407227\n",
      "[0177/1000] train loss: 14.08688497543335 validation loss: 29.523794174194336\n",
      "[0178/1000] train loss: 13.957105159759521 validation loss: 29.435291290283203\n",
      "[0179/1000] train loss: 13.967223167419434 validation loss: 29.335527420043945\n",
      "[0180/1000] train loss: 13.899099826812744 validation loss: 29.2578125\n",
      "[0181/1000] train loss: 13.803789615631104 validation loss: 29.24374008178711\n",
      "[0182/1000] train loss: 13.786075115203857 validation loss: 29.148332595825195\n",
      "[0183/1000] train loss: 13.232283592224121 validation loss: 29.12538719177246\n",
      "[0184/1000] train loss: 13.428446769714355 validation loss: 29.07023048400879\n",
      "[0185/1000] train loss: 13.591900825500488 validation loss: 29.0010986328125\n",
      "[0186/1000] train loss: 13.543793678283691 validation loss: 28.89794921875\n",
      "[0187/1000] train loss: 13.502709865570068 validation loss: 28.769775390625\n",
      "[0188/1000] train loss: 13.386485576629639 validation loss: 28.64411735534668\n",
      "[0189/1000] train loss: 13.17911672592163 validation loss: 28.487409591674805\n",
      "[0190/1000] train loss: 13.234803676605225 validation loss: 28.443281173706055\n",
      "[0191/1000] train loss: 13.221038818359375 validation loss: 28.31749725341797\n",
      "[0192/1000] train loss: 13.194181442260742 validation loss: 28.222631454467773\n",
      "[0193/1000] train loss: 12.470939636230469 validation loss: 28.27096939086914\n",
      "[0194/1000] train loss: 12.904741287231445 validation loss: 28.122039794921875\n",
      "[0195/1000] train loss: 12.932417392730713 validation loss: 28.06753921508789\n",
      "[0196/1000] train loss: 12.950758457183838 validation loss: 27.978063583374023\n",
      "[0197/1000] train loss: 12.898034572601318 validation loss: 27.85946273803711\n",
      "[0198/1000] train loss: 12.865499019622803 validation loss: 27.74325180053711\n",
      "[0199/1000] train loss: 12.760096549987793 validation loss: 27.659425735473633\n",
      "[0200/1000] train loss: 12.629316806793213 validation loss: 27.58570098876953\n",
      "[0201/1000] train loss: 12.470274925231934 validation loss: 27.514039993286133\n",
      "[0202/1000] train loss: 12.451950550079346 validation loss: 27.461301803588867\n",
      "[0203/1000] train loss: 12.461247444152832 validation loss: 27.401044845581055\n",
      "[0204/1000] train loss: 12.418099880218506 validation loss: 27.32302474975586\n",
      "[0205/1000] train loss: 12.352340698242188 validation loss: 27.23716163635254\n",
      "[0206/1000] train loss: 12.30492115020752 validation loss: 27.14778709411621\n",
      "[0207/1000] train loss: 12.24189567565918 validation loss: 27.02431869506836\n",
      "[0208/1000] train loss: 12.215227603912354 validation loss: 26.96689796447754\n",
      "[0209/1000] train loss: 12.109385967254639 validation loss: 26.883834838867188\n",
      "[0210/1000] train loss: 12.06752634048462 validation loss: 26.865493774414062\n",
      "[0211/1000] train loss: 11.993602275848389 validation loss: 26.83118438720703\n",
      "[0212/1000] train loss: 11.938123226165771 validation loss: 26.68412971496582\n",
      "[0213/1000] train loss: 11.869729042053223 validation loss: 26.601362228393555\n",
      "[0214/1000] train loss: 11.721328258514404 validation loss: 26.540424346923828\n",
      "[0215/1000] train loss: 11.667803287506104 validation loss: 26.490909576416016\n",
      "[0216/1000] train loss: 11.771517276763916 validation loss: 26.352258682250977\n",
      "[0217/1000] train loss: 11.650024890899658 validation loss: 26.268510818481445\n",
      "[0218/1000] train loss: 11.665998935699463 validation loss: 26.21354103088379\n",
      "[0219/1000] train loss: 11.582750797271729 validation loss: 26.184146881103516\n",
      "[0220/1000] train loss: 11.540551662445068 validation loss: 26.11568832397461\n",
      "[0221/1000] train loss: 11.473852634429932 validation loss: 25.974349975585938\n",
      "[0222/1000] train loss: 11.461309909820557 validation loss: 25.94750213623047\n",
      "[0223/1000] train loss: 11.258891582489014 validation loss: 25.873441696166992\n",
      "[0224/1000] train loss: 11.285429000854492 validation loss: 25.80001449584961\n",
      "[0225/1000] train loss: 11.33183479309082 validation loss: 25.71027183532715\n",
      "[0226/1000] train loss: 11.298823356628418 validation loss: 25.61880874633789\n",
      "[0227/1000] train loss: 11.192402362823486 validation loss: 25.487730026245117\n",
      "[0228/1000] train loss: 11.15349006652832 validation loss: 25.436351776123047\n",
      "[0229/1000] train loss: 11.067275524139404 validation loss: 25.350194931030273\n",
      "[0230/1000] train loss: 11.071029663085938 validation loss: 25.281084060668945\n",
      "[0231/1000] train loss: 10.988895416259766 validation loss: 25.2070369720459\n",
      "[0232/1000] train loss: 10.91986083984375 validation loss: 25.153308868408203\n",
      "[0233/1000] train loss: 10.934255123138428 validation loss: 25.081157684326172\n",
      "[0234/1000] train loss: 10.845446586608887 validation loss: 25.014266967773438\n",
      "[0235/1000] train loss: 10.780722618103027 validation loss: 24.97857093811035\n",
      "[0236/1000] train loss: 10.796154499053955 validation loss: 24.964696884155273\n",
      "[0237/1000] train loss: 10.690927267074585 validation loss: 24.95507049560547\n",
      "[0238/1000] train loss: 10.687615871429443 validation loss: 24.867393493652344\n",
      "[0239/1000] train loss: 10.638774871826172 validation loss: 24.75473976135254\n",
      "[0240/1000] train loss: 10.453832149505615 validation loss: 24.67426109313965\n",
      "[0241/1000] train loss: 10.64887285232544 validation loss: 24.534286499023438\n",
      "[0242/1000] train loss: 10.424087047576904 validation loss: 24.571399688720703\n",
      "[0243/1000] train loss: 10.450671195983887 validation loss: 24.535634994506836\n",
      "[0244/1000] train loss: 10.262821674346924 validation loss: 24.460737228393555\n",
      "[0245/1000] train loss: 10.389373779296875 validation loss: 24.43107795715332\n",
      "[0246/1000] train loss: 10.352630615234375 validation loss: 24.4112548828125\n",
      "[0247/1000] train loss: 10.319048881530762 validation loss: 24.296142578125\n",
      "[0248/1000] train loss: 10.275527477264404 validation loss: 24.296253204345703\n",
      "[0249/1000] train loss: 10.242992401123047 validation loss: 24.187192916870117\n",
      "[0250/1000] train loss: 10.033785820007324 validation loss: 24.138662338256836\n",
      "[0251/1000] train loss: 10.175089359283447 validation loss: 24.05625343322754\n",
      "[0252/1000] train loss: 10.094327449798584 validation loss: 24.050050735473633\n",
      "[0253/1000] train loss: 10.008556365966797 validation loss: 24.009288787841797\n",
      "[0254/1000] train loss: 10.051661968231201 validation loss: 23.96414566040039\n",
      "[0255/1000] train loss: 9.973991394042969 validation loss: 23.930158615112305\n",
      "[0256/1000] train loss: 9.989156723022461 validation loss: 23.77474021911621\n",
      "[0257/1000] train loss: 9.758848190307617 validation loss: 23.77136993408203\n",
      "[0258/1000] train loss: 9.883528709411621 validation loss: 23.729780197143555\n",
      "[0259/1000] train loss: 9.644401550292969 validation loss: 23.655153274536133\n",
      "[0260/1000] train loss: 9.831718444824219 validation loss: 23.6015567779541\n",
      "[0261/1000] train loss: 9.684260368347168 validation loss: 23.55414390563965\n",
      "[0262/1000] train loss: 9.78669261932373 validation loss: 23.613510131835938\n",
      "[0263/1000] train loss: 9.748171329498291 validation loss: 23.56479263305664\n",
      "[0264/1000] train loss: 9.552925109863281 validation loss: 23.348934173583984\n",
      "[0265/1000] train loss: 9.6010160446167 validation loss: 23.33095932006836\n",
      "[0266/1000] train loss: 9.718148708343506 validation loss: 23.2579402923584\n",
      "[0267/1000] train loss: 9.333782196044922 validation loss: 23.10549545288086\n",
      "[0268/1000] train loss: 9.485533714294434 validation loss: 23.180408477783203\n",
      "[0269/1000] train loss: 9.519625186920166 validation loss: 23.016939163208008\n",
      "[0270/1000] train loss: 9.482528686523438 validation loss: 23.04770278930664\n",
      "[0271/1000] train loss: 9.474859714508057 validation loss: 22.92972755432129\n",
      "[0272/1000] train loss: 9.381124258041382 validation loss: 22.893049240112305\n",
      "[0273/1000] train loss: 9.346122741699219 validation loss: 22.86647605895996\n",
      "[0274/1000] train loss: 9.301711559295654 validation loss: 22.903038024902344\n",
      "[0275/1000] train loss: 9.236530303955078 validation loss: 22.69509506225586\n",
      "[0276/1000] train loss: 9.266754150390625 validation loss: 22.754106521606445\n",
      "[0277/1000] train loss: 9.241323947906494 validation loss: 22.687894821166992\n",
      "[0278/1000] train loss: 9.222274780273438 validation loss: 22.789682388305664\n",
      "[0279/1000] train loss: 9.146555662155151 validation loss: 22.742034912109375\n",
      "[0280/1000] train loss: 9.11064887046814 validation loss: 22.69198226928711\n",
      "[0281/1000] train loss: 9.01408338546753 validation loss: 22.56184196472168\n",
      "[0282/1000] train loss: 9.065351724624634 validation loss: 22.590538024902344\n",
      "[0283/1000] train loss: 9.005693912506104 validation loss: 22.59189224243164\n",
      "[0284/1000] train loss: 8.970450401306152 validation loss: 22.57269859313965\n",
      "[0285/1000] train loss: 9.043829917907715 validation loss: 22.43053436279297\n",
      "[0286/1000] train loss: 8.979037761688232 validation loss: 22.399463653564453\n",
      "[0287/1000] train loss: 8.990097999572754 validation loss: 22.509077072143555\n",
      "[0288/1000] train loss: 8.899423599243164 validation loss: 22.44430923461914\n",
      "[0289/1000] train loss: 8.880018711090088 validation loss: 22.262374877929688\n",
      "[0290/1000] train loss: 8.840405941009521 validation loss: 22.268461227416992\n",
      "[0291/1000] train loss: 8.74918532371521 validation loss: 22.173677444458008\n",
      "[0292/1000] train loss: 8.734816789627075 validation loss: 22.138296127319336\n",
      "[0293/1000] train loss: 8.73075246810913 validation loss: 22.112836837768555\n",
      "[0294/1000] train loss: 8.661912441253662 validation loss: 21.928083419799805\n",
      "[0295/1000] train loss: 8.6931791305542 validation loss: 21.9696044921875\n",
      "[0296/1000] train loss: 8.645940780639648 validation loss: 21.958730697631836\n",
      "[0297/1000] train loss: 8.686343669891357 validation loss: 22.018884658813477\n",
      "[0298/1000] train loss: 8.472259998321533 validation loss: 22.027772903442383\n",
      "[0299/1000] train loss: 8.49406385421753 validation loss: 22.114791870117188\n",
      "[0300/1000] train loss: 8.590602159500122 validation loss: 21.90395164489746\n",
      "[0301/1000] train loss: 8.445064067840576 validation loss: 21.974788665771484\n",
      "[0302/1000] train loss: 8.565430879592896 validation loss: 22.01433753967285\n",
      "[0303/1000] train loss: 8.509631633758545 validation loss: 21.84502601623535\n",
      "[0304/1000] train loss: 8.45496392250061 validation loss: 21.824447631835938\n",
      "[0305/1000] train loss: 8.427972316741943 validation loss: 21.869152069091797\n",
      "[0306/1000] train loss: 8.450160264968872 validation loss: 21.714672088623047\n",
      "[0307/1000] train loss: 8.43855333328247 validation loss: 21.65288543701172\n",
      "[0308/1000] train loss: 8.464369297027588 validation loss: 21.84065818786621\n",
      "[0309/1000] train loss: 8.413535833358765 validation loss: 21.660385131835938\n",
      "[0310/1000] train loss: 8.316403150558472 validation loss: 21.63195037841797\n",
      "[0311/1000] train loss: 8.288669109344482 validation loss: 21.644775390625\n",
      "[0312/1000] train loss: 8.361939907073975 validation loss: 21.72667694091797\n",
      "[0313/1000] train loss: 8.248716831207275 validation loss: 21.586118698120117\n",
      "[0314/1000] train loss: 8.084500312805176 validation loss: 21.500255584716797\n",
      "[0315/1000] train loss: 8.236793994903564 validation loss: 21.395076751708984\n",
      "[0316/1000] train loss: 8.27254581451416 validation loss: 21.50347137451172\n",
      "[0317/1000] train loss: 8.222257614135742 validation loss: 21.459897994995117\n",
      "[0318/1000] train loss: 8.172810554504395 validation loss: 21.40448570251465\n",
      "[0319/1000] train loss: 8.256449460983276 validation loss: 21.49740219116211\n",
      "[0320/1000] train loss: 8.23048448562622 validation loss: 21.523588180541992\n",
      "[0321/1000] train loss: 8.05077075958252 validation loss: 21.34812355041504\n",
      "[0322/1000] train loss: 8.149422645568848 validation loss: 21.274124145507812\n",
      "[0323/1000] train loss: 8.010185480117798 validation loss: 21.30660629272461\n",
      "[0324/1000] train loss: 8.07774806022644 validation loss: 21.281328201293945\n",
      "[0325/1000] train loss: 8.22976565361023 validation loss: 21.23404312133789\n",
      "[0326/1000] train loss: 8.084440469741821 validation loss: 21.263887405395508\n",
      "[0327/1000] train loss: 8.082722425460815 validation loss: 21.15732192993164\n",
      "[0328/1000] train loss: 7.9687395095825195 validation loss: 21.281938552856445\n",
      "[0329/1000] train loss: 7.470834493637085 validation loss: 20.958965301513672\n",
      "[0330/1000] train loss: 7.9712889194488525 validation loss: 21.222871780395508\n",
      "[0331/1000] train loss: 7.971127510070801 validation loss: 21.183565139770508\n",
      "[0332/1000] train loss: 7.9621477127075195 validation loss: 21.044301986694336\n",
      "[0333/1000] train loss: 7.935875415802002 validation loss: 21.170942306518555\n",
      "[0334/1000] train loss: 7.97004508972168 validation loss: 21.005687713623047\n",
      "[0335/1000] train loss: 7.811553001403809 validation loss: 20.974241256713867\n",
      "[0336/1000] train loss: 7.65190052986145 validation loss: 21.3117733001709\n",
      "[0337/1000] train loss: 7.889844655990601 validation loss: 21.11977767944336\n",
      "[0338/1000] train loss: 7.482774257659912 validation loss: 21.236038208007812\n",
      "[0339/1000] train loss: 7.787222862243652 validation loss: 21.154560089111328\n",
      "[0340/1000] train loss: 7.8225133419036865 validation loss: 21.07085609436035\n",
      "[0341/1000] train loss: 7.75865626335144 validation loss: 20.93568992614746\n",
      "[0342/1000] train loss: 7.541964292526245 validation loss: 20.916587829589844\n",
      "[0343/1000] train loss: 7.550550222396851 validation loss: 21.255210876464844\n",
      "[0344/1000] train loss: 7.677516222000122 validation loss: 21.009140014648438\n",
      "[0345/1000] train loss: 7.763992786407471 validation loss: 21.047571182250977\n",
      "[0346/1000] train loss: 7.784169673919678 validation loss: 21.15925407409668\n",
      "[0347/1000] train loss: 7.63310694694519 validation loss: 21.036666870117188\n",
      "[0348/1000] train loss: 7.4376726150512695 validation loss: 21.06296157836914\n",
      "[0349/1000] train loss: 7.807262182235718 validation loss: 21.23064422607422\n",
      "[0350/1000] train loss: 7.6140148639678955 validation loss: 20.963930130004883\n",
      "[0351/1000] train loss: 7.607476711273193 validation loss: 21.064233779907227\n",
      "[0352/1000] train loss: 7.600406169891357 validation loss: 20.939361572265625\n",
      "[0353/1000] train loss: 7.566587209701538 validation loss: 20.97003936767578\n",
      "[0354/1000] train loss: 7.624324083328247 validation loss: 20.932371139526367\n",
      "[0355/1000] train loss: 7.613966464996338 validation loss: 21.001745223999023\n",
      "[0356/1000] train loss: 7.5778210163116455 validation loss: 20.959423065185547\n",
      "[0357/1000] train loss: 7.5271360874176025 validation loss: 20.816619873046875\n",
      "[0358/1000] train loss: 7.5495216846466064 validation loss: 20.83601951599121\n",
      "[0359/1000] train loss: 7.568233966827393 validation loss: 20.76515769958496\n",
      "[0360/1000] train loss: 7.4886558055877686 validation loss: 20.74032211303711\n",
      "[0361/1000] train loss: 7.780567169189453 validation loss: 20.87436866760254\n",
      "[0362/1000] train loss: 7.501444101333618 validation loss: 20.900835037231445\n",
      "[0363/1000] train loss: 7.481778860092163 validation loss: 20.6743106842041\n",
      "[0364/1000] train loss: 7.477961301803589 validation loss: 20.78103256225586\n",
      "[0365/1000] train loss: 7.371849298477173 validation loss: 20.690654754638672\n",
      "[0366/1000] train loss: 7.454973459243774 validation loss: 20.76174545288086\n",
      "[0367/1000] train loss: 7.408263444900513 validation loss: 20.64603042602539\n",
      "[0368/1000] train loss: 7.467898845672607 validation loss: 20.680742263793945\n",
      "[0369/1000] train loss: 7.407001256942749 validation loss: 20.78706932067871\n",
      "[0370/1000] train loss: 7.403630018234253 validation loss: 20.624862670898438\n",
      "[0371/1000] train loss: 7.238438606262207 validation loss: 20.618223190307617\n",
      "[0372/1000] train loss: 7.354681968688965 validation loss: 20.685596466064453\n",
      "[0373/1000] train loss: 7.198496103286743 validation loss: 20.825382232666016\n",
      "[0374/1000] train loss: 7.2660510540008545 validation loss: 20.66240882873535\n",
      "[0375/1000] train loss: 7.355870962142944 validation loss: 20.50075912475586\n",
      "[0376/1000] train loss: 7.308061599731445 validation loss: 20.487152099609375\n",
      "[0377/1000] train loss: 7.220672130584717 validation loss: 20.48773765563965\n",
      "[0378/1000] train loss: 7.30173397064209 validation loss: 20.61717987060547\n",
      "[0379/1000] train loss: 7.430203199386597 validation loss: 20.481094360351562\n",
      "[0380/1000] train loss: 7.3680408000946045 validation loss: 20.4930419921875\n",
      "[0381/1000] train loss: 7.278374195098877 validation loss: 20.63844871520996\n",
      "[0382/1000] train loss: 7.2432849407196045 validation loss: 20.733657836914062\n",
      "[0383/1000] train loss: 7.1566503047943115 validation loss: 20.40340805053711\n",
      "[0384/1000] train loss: 7.226111650466919 validation loss: 20.564266204833984\n",
      "[0385/1000] train loss: 7.091527700424194 validation loss: 20.350069046020508\n",
      "[0386/1000] train loss: 7.263129472732544 validation loss: 20.39952850341797\n",
      "[0387/1000] train loss: 7.251046895980835 validation loss: 20.173725128173828\n",
      "[0388/1000] train loss: 7.125518083572388 validation loss: 20.3460636138916\n",
      "[0389/1000] train loss: 7.227985858917236 validation loss: 20.512027740478516\n",
      "[0390/1000] train loss: 7.220890283584595 validation loss: 20.558162689208984\n",
      "[0391/1000] train loss: 6.89228081703186 validation loss: 20.45883560180664\n",
      "[0392/1000] train loss: 7.123572111129761 validation loss: 20.50162696838379\n",
      "[0393/1000] train loss: 7.087467432022095 validation loss: 20.423171997070312\n",
      "[0394/1000] train loss: 7.092466354370117 validation loss: 20.27960777282715\n",
      "[0395/1000] train loss: 7.173840284347534 validation loss: 20.34444808959961\n",
      "[0396/1000] train loss: 7.02292275428772 validation loss: 20.37556266784668\n",
      "[0397/1000] train loss: 7.050523996353149 validation loss: 20.43228530883789\n",
      "[0398/1000] train loss: 6.992793321609497 validation loss: 20.299055099487305\n",
      "[0399/1000] train loss: 7.130192756652832 validation loss: 20.268388748168945\n",
      "[0400/1000] train loss: 7.067526817321777 validation loss: 20.361188888549805\n",
      "[0401/1000] train loss: 7.04219913482666 validation loss: 20.3927001953125\n",
      "[0402/1000] train loss: 7.061196565628052 validation loss: 20.507287979125977\n",
      "[0403/1000] train loss: 6.405061960220337 validation loss: 20.492481231689453\n",
      "[0404/1000] train loss: 7.001672267913818 validation loss: 20.418378829956055\n",
      "[0405/1000] train loss: 6.9187915325164795 validation loss: 20.29509162902832\n",
      "[0406/1000] train loss: 6.906306505203247 validation loss: 20.227258682250977\n",
      "[0407/1000] train loss: 7.020604133605957 validation loss: 20.26486587524414\n",
      "[0408/1000] train loss: 6.671077489852905 validation loss: 20.352338790893555\n",
      "[0409/1000] train loss: 6.852572679519653 validation loss: 20.16457176208496\n",
      "[0410/1000] train loss: 6.9520604610443115 validation loss: 20.27083969116211\n",
      "[0411/1000] train loss: 6.848232984542847 validation loss: 20.238563537597656\n",
      "[0412/1000] train loss: 6.940128326416016 validation loss: 20.012611389160156\n",
      "[0413/1000] train loss: 6.908846139907837 validation loss: 20.089963912963867\n",
      "[0414/1000] train loss: 6.930375337600708 validation loss: 20.11479949951172\n",
      "[0415/1000] train loss: 6.91094183921814 validation loss: 20.025285720825195\n",
      "[0416/1000] train loss: 6.847697734832764 validation loss: 20.13106918334961\n",
      "[0417/1000] train loss: 6.775256633758545 validation loss: 20.224945068359375\n",
      "[0418/1000] train loss: 6.935910701751709 validation loss: 20.225933074951172\n",
      "[0419/1000] train loss: 6.907726764678955 validation loss: 20.068572998046875\n",
      "[0420/1000] train loss: 6.70609974861145 validation loss: 20.085329055786133\n",
      "[0421/1000] train loss: 6.677349805831909 validation loss: 20.042509078979492\n",
      "[0422/1000] train loss: 6.846231698989868 validation loss: 20.221004486083984\n",
      "[0423/1000] train loss: 6.829448938369751 validation loss: 20.16661834716797\n",
      "[0424/1000] train loss: 6.740715265274048 validation loss: 19.985326766967773\n",
      "[0425/1000] train loss: 6.708736896514893 validation loss: 20.130088806152344\n",
      "[0426/1000] train loss: 6.788970232009888 validation loss: 20.10525131225586\n",
      "[0427/1000] train loss: 6.833679437637329 validation loss: 20.309478759765625\n",
      "[0428/1000] train loss: 6.814748287200928 validation loss: 20.2375545501709\n",
      "[0429/1000] train loss: 6.810837984085083 validation loss: 20.247865676879883\n",
      "[0430/1000] train loss: 6.698682546615601 validation loss: 20.079936981201172\n",
      "[0431/1000] train loss: 6.122344732284546 validation loss: 19.774078369140625\n",
      "[0432/1000] train loss: 6.6763012409210205 validation loss: 20.02351951599121\n",
      "[0433/1000] train loss: 6.722190856933594 validation loss: 20.06317901611328\n",
      "[0434/1000] train loss: 6.734589576721191 validation loss: 19.92462730407715\n",
      "[0435/1000] train loss: 6.618736505508423 validation loss: 19.83088493347168\n",
      "[0436/1000] train loss: 6.605269193649292 validation loss: 20.04422378540039\n",
      "[0437/1000] train loss: 6.754793167114258 validation loss: 20.128721237182617\n",
      "[0438/1000] train loss: 6.641074180603027 validation loss: 19.957847595214844\n",
      "[0439/1000] train loss: 6.72025728225708 validation loss: 19.84410858154297\n",
      "[0440/1000] train loss: 6.528153419494629 validation loss: 19.837305068969727\n",
      "[0441/1000] train loss: 6.1461708545684814 validation loss: 19.925548553466797\n",
      "[0442/1000] train loss: 6.563194990158081 validation loss: 20.035032272338867\n",
      "[0443/1000] train loss: 6.5361857414245605 validation loss: 19.876075744628906\n",
      "[0444/1000] train loss: 6.5791521072387695 validation loss: 19.815263748168945\n",
      "[0445/1000] train loss: 6.598675727844238 validation loss: 19.75520896911621\n",
      "[0446/1000] train loss: 6.661394357681274 validation loss: 20.09450340270996\n",
      "[0447/1000] train loss: 6.619774580001831 validation loss: 19.868288040161133\n",
      "[0448/1000] train loss: 6.5828635692596436 validation loss: 19.8487491607666\n",
      "[0449/1000] train loss: 6.537759780883789 validation loss: 19.84737205505371\n",
      "[0450/1000] train loss: 6.539931535720825 validation loss: 19.604358673095703\n",
      "[0451/1000] train loss: 6.486424446105957 validation loss: 19.787334442138672\n",
      "[0452/1000] train loss: 6.4765918254852295 validation loss: 19.63526153564453\n",
      "[0453/1000] train loss: 6.49253249168396 validation loss: 19.721158981323242\n",
      "[0454/1000] train loss: 6.485810279846191 validation loss: 19.83493423461914\n",
      "[0455/1000] train loss: 6.430612087249756 validation loss: 19.775571823120117\n",
      "[0456/1000] train loss: 6.470764636993408 validation loss: 19.91470718383789\n",
      "[0457/1000] train loss: 6.48169469833374 validation loss: 19.693822860717773\n",
      "[0458/1000] train loss: 6.381956577301025 validation loss: 19.60824966430664\n",
      "[0459/1000] train loss: 6.468391418457031 validation loss: 19.919586181640625\n",
      "[0460/1000] train loss: 6.5128912925720215 validation loss: 19.91520881652832\n",
      "[0461/1000] train loss: 6.399223804473877 validation loss: 19.592355728149414\n",
      "[0462/1000] train loss: 6.511595010757446 validation loss: 19.486909866333008\n",
      "[0463/1000] train loss: 6.348183870315552 validation loss: 19.770679473876953\n",
      "[0464/1000] train loss: 6.436458587646484 validation loss: 19.59087562561035\n",
      "[0465/1000] train loss: 6.371047496795654 validation loss: 19.550479888916016\n",
      "[0466/1000] train loss: 6.555313348770142 validation loss: 19.697175979614258\n",
      "[0467/1000] train loss: 6.44249963760376 validation loss: 19.767684936523438\n",
      "[0468/1000] train loss: 6.355106830596924 validation loss: 19.69229507446289\n",
      "[0469/1000] train loss: 6.379618883132935 validation loss: 19.697799682617188\n",
      "[0470/1000] train loss: 6.351302862167358 validation loss: 19.450721740722656\n",
      "[0471/1000] train loss: 6.385796785354614 validation loss: 19.562667846679688\n",
      "[0472/1000] train loss: 6.126229286193848 validation loss: 19.80512046813965\n",
      "[0473/1000] train loss: 6.388430833816528 validation loss: 19.77878189086914\n",
      "[0474/1000] train loss: 6.314112901687622 validation loss: 19.443492889404297\n",
      "[0475/1000] train loss: 6.1394758224487305 validation loss: 19.685504913330078\n",
      "[0476/1000] train loss: 6.258531808853149 validation loss: 19.54130744934082\n",
      "[0477/1000] train loss: 6.307341814041138 validation loss: 19.631832122802734\n",
      "[0478/1000] train loss: 6.28190016746521 validation loss: 19.49686622619629\n",
      "[0479/1000] train loss: 6.2458412647247314 validation loss: 19.425554275512695\n",
      "[0480/1000] train loss: 6.335537672042847 validation loss: 19.509811401367188\n",
      "[0481/1000] train loss: 6.261723518371582 validation loss: 19.477691650390625\n",
      "[0482/1000] train loss: 6.3077392578125 validation loss: 19.711692810058594\n",
      "[0483/1000] train loss: 6.423341274261475 validation loss: 19.60226821899414\n",
      "[0484/1000] train loss: 6.084169626235962 validation loss: 19.81117820739746\n",
      "[0485/1000] train loss: 6.3409740924835205 validation loss: 19.86722183227539\n",
      "[0486/1000] train loss: 6.297948837280273 validation loss: 19.407644271850586\n",
      "[0487/1000] train loss: 6.223290681838989 validation loss: 19.47905921936035\n",
      "[0488/1000] train loss: 6.137058734893799 validation loss: 19.536291122436523\n",
      "[0489/1000] train loss: 6.225705146789551 validation loss: 19.5041446685791\n",
      "[0490/1000] train loss: 6.186038494110107 validation loss: 19.72284507751465\n",
      "[0491/1000] train loss: 6.217800855636597 validation loss: 19.65876579284668\n",
      "[0492/1000] train loss: 6.165647506713867 validation loss: 19.339435577392578\n",
      "[0493/1000] train loss: 6.1228015422821045 validation loss: 19.436811447143555\n",
      "[0494/1000] train loss: 6.242526054382324 validation loss: 19.286293029785156\n",
      "[0495/1000] train loss: 6.107512950897217 validation loss: 19.37618064880371\n",
      "[0496/1000] train loss: 6.129539489746094 validation loss: 19.484111785888672\n",
      "[0497/1000] train loss: 6.173770189285278 validation loss: 19.373565673828125\n",
      "[0498/1000] train loss: 6.207491159439087 validation loss: 19.260643005371094\n",
      "[0499/1000] train loss: 6.186125993728638 validation loss: 19.613250732421875\n",
      "[0500/1000] train loss: 6.164639472961426 validation loss: 19.51717758178711\n",
      "[0501/1000] train loss: 6.0804572105407715 validation loss: 19.57003402709961\n",
      "[0502/1000] train loss: 6.095539093017578 validation loss: 19.235912322998047\n",
      "[0503/1000] train loss: 6.0902180671691895 validation loss: 19.35254669189453\n",
      "[0504/1000] train loss: 6.114185571670532 validation loss: 19.691198348999023\n",
      "[0505/1000] train loss: 6.074889898300171 validation loss: 19.543703079223633\n",
      "[0506/1000] train loss: 6.097954750061035 validation loss: 19.47383689880371\n",
      "[0507/1000] train loss: 6.145010709762573 validation loss: 19.334264755249023\n",
      "[0508/1000] train loss: 6.079953670501709 validation loss: 19.173778533935547\n",
      "[0509/1000] train loss: 6.0654003620147705 validation loss: 19.35714340209961\n",
      "[0510/1000] train loss: 6.04019832611084 validation loss: 19.30923080444336\n",
      "[0511/1000] train loss: 6.048218250274658 validation loss: 19.27841567993164\n",
      "[0512/1000] train loss: 6.086658239364624 validation loss: 19.15526008605957\n",
      "[0513/1000] train loss: 6.088160991668701 validation loss: 19.52140235900879\n",
      "[0514/1000] train loss: 6.115274667739868 validation loss: 19.67021369934082\n",
      "[0515/1000] train loss: 6.049280166625977 validation loss: 19.424983978271484\n",
      "[0516/1000] train loss: 5.964423894882202 validation loss: 19.469358444213867\n",
      "[0517/1000] train loss: 6.090637445449829 validation loss: 19.4578857421875\n",
      "[0518/1000] train loss: 5.972586393356323 validation loss: 19.38969612121582\n",
      "[0519/1000] train loss: 5.9875617027282715 validation loss: 19.176807403564453\n",
      "[0520/1000] train loss: 5.870560884475708 validation loss: 19.228591918945312\n",
      "[0521/1000] train loss: 5.959867238998413 validation loss: 19.52996253967285\n",
      "[0522/1000] train loss: 6.005078554153442 validation loss: 19.498815536499023\n",
      "[0523/1000] train loss: 5.57564902305603 validation loss: 19.05376434326172\n",
      "[0524/1000] train loss: 5.934915781021118 validation loss: 18.966676712036133\n",
      "[0525/1000] train loss: 5.890817165374756 validation loss: 19.36940574645996\n",
      "[0526/1000] train loss: 5.949071645736694 validation loss: 19.147153854370117\n",
      "[0527/1000] train loss: 5.811946630477905 validation loss: 19.491668701171875\n",
      "[0528/1000] train loss: 5.960863351821899 validation loss: 19.402589797973633\n",
      "[0529/1000] train loss: 5.881160259246826 validation loss: 19.455913543701172\n",
      "[0530/1000] train loss: 5.868548631668091 validation loss: 19.18451690673828\n",
      "[0531/1000] train loss: 5.925145626068115 validation loss: 19.306716918945312\n",
      "[0532/1000] train loss: 5.941173315048218 validation loss: 19.329221725463867\n",
      "[0533/1000] train loss: 5.897093296051025 validation loss: 19.158802032470703\n",
      "[0534/1000] train loss: 5.931681871414185 validation loss: 19.510705947875977\n",
      "[0535/1000] train loss: 5.920635223388672 validation loss: 19.27233123779297\n",
      "[0536/1000] train loss: 5.906813383102417 validation loss: 19.223691940307617\n",
      "[0537/1000] train loss: 5.876761198043823 validation loss: 19.24901580810547\n",
      "[0538/1000] train loss: 5.907201766967773 validation loss: 19.32187843322754\n",
      "[0539/1000] train loss: 5.944806098937988 validation loss: 19.44132423400879\n",
      "[0540/1000] train loss: 5.8905322551727295 validation loss: 19.268728256225586\n",
      "[0541/1000] train loss: 5.863794326782227 validation loss: 19.042512893676758\n",
      "[0542/1000] train loss: 6.012864351272583 validation loss: 19.207260131835938\n",
      "[0543/1000] train loss: 5.848963022232056 validation loss: 19.40915870666504\n",
      "[0544/1000] train loss: 5.818472146987915 validation loss: 19.151336669921875\n",
      "[0545/1000] train loss: 5.706034898757935 validation loss: 19.17054557800293\n",
      "[0546/1000] train loss: 5.831045866012573 validation loss: 19.2413330078125\n",
      "[0547/1000] train loss: 5.817054748535156 validation loss: 19.178142547607422\n",
      "[0548/1000] train loss: 5.776942014694214 validation loss: 19.31587791442871\n",
      "[0549/1000] train loss: 5.818044424057007 validation loss: 19.1297550201416\n",
      "[0550/1000] train loss: 5.748715400695801 validation loss: 19.438249588012695\n",
      "[0551/1000] train loss: 5.914803504943848 validation loss: 19.47835922241211\n",
      "[0552/1000] train loss: 5.771064043045044 validation loss: 19.09017562866211\n",
      "[0553/1000] train loss: 5.776381969451904 validation loss: 19.192398071289062\n",
      "[0554/1000] train loss: 5.808866500854492 validation loss: 19.15665626525879\n",
      "[0555/1000] train loss: 5.745685815811157 validation loss: 19.142730712890625\n",
      "[0556/1000] train loss: 5.709981679916382 validation loss: 19.338132858276367\n",
      "[0557/1000] train loss: 5.758676528930664 validation loss: 18.92920684814453\n",
      "[0558/1000] train loss: 5.63703465461731 validation loss: 19.190311431884766\n",
      "[0559/1000] train loss: 5.688622713088989 validation loss: 19.212657928466797\n",
      "[0560/1000] train loss: 5.76439356803894 validation loss: 18.881376266479492\n",
      "[0561/1000] train loss: 5.415846109390259 validation loss: 19.33751678466797\n",
      "[0562/1000] train loss: 5.721068859100342 validation loss: 19.34154510498047\n",
      "[0563/1000] train loss: 5.6762707233428955 validation loss: 19.15705680847168\n",
      "[0564/1000] train loss: 5.744074583053589 validation loss: 19.37155532836914\n",
      "[0565/1000] train loss: 5.697918891906738 validation loss: 19.026670455932617\n",
      "[0566/1000] train loss: 5.685116291046143 validation loss: 19.13597869873047\n",
      "[0567/1000] train loss: 5.769948244094849 validation loss: 19.239513397216797\n",
      "[0568/1000] train loss: 5.811449289321899 validation loss: 19.29728126525879\n",
      "[0569/1000] train loss: 5.7283501625061035 validation loss: 19.047595977783203\n",
      "[0570/1000] train loss: 5.682845115661621 validation loss: 19.154687881469727\n",
      "[0571/1000] train loss: 5.679614305496216 validation loss: 19.03080177307129\n",
      "[0572/1000] train loss: 5.760253667831421 validation loss: 18.969249725341797\n",
      "[0573/1000] train loss: 5.6061201095581055 validation loss: 19.198406219482422\n",
      "[0574/1000] train loss: 5.743868589401245 validation loss: 19.093151092529297\n",
      "[0575/1000] train loss: 5.656371116638184 validation loss: 18.90065574645996\n",
      "[0576/1000] train loss: 5.593419313430786 validation loss: 19.01873207092285\n",
      "[0577/1000] train loss: 5.501797199249268 validation loss: 19.016389846801758\n",
      "[0578/1000] train loss: 5.668755531311035 validation loss: 18.891756057739258\n",
      "[0579/1000] train loss: 5.62049412727356 validation loss: 19.019357681274414\n",
      "[0580/1000] train loss: 5.602367401123047 validation loss: 19.037961959838867\n",
      "[0581/1000] train loss: 5.5370330810546875 validation loss: 18.905227661132812\n",
      "[0582/1000] train loss: 5.616184711456299 validation loss: 19.213476181030273\n",
      "[0583/1000] train loss: 5.673965215682983 validation loss: 19.172876358032227\n",
      "[0584/1000] train loss: 5.598280191421509 validation loss: 18.8928165435791\n",
      "[0585/1000] train loss: 5.702315330505371 validation loss: 18.76679229736328\n",
      "[0586/1000] train loss: 5.397750616073608 validation loss: 18.945266723632812\n",
      "[0587/1000] train loss: 5.6527533531188965 validation loss: 19.21150779724121\n",
      "[0588/1000] train loss: 5.66723895072937 validation loss: 18.885690689086914\n",
      "[0589/1000] train loss: 5.555785655975342 validation loss: 19.022689819335938\n",
      "[0590/1000] train loss: 5.574466943740845 validation loss: 18.844945907592773\n",
      "[0591/1000] train loss: 5.418237209320068 validation loss: 19.00339126586914\n",
      "[0592/1000] train loss: 5.553363084793091 validation loss: 18.898181915283203\n",
      "[0593/1000] train loss: 5.595203876495361 validation loss: 18.762954711914062\n",
      "[0594/1000] train loss: 5.51533055305481 validation loss: 18.766584396362305\n",
      "[0595/1000] train loss: 5.560336351394653 validation loss: 18.89706039428711\n",
      "[0596/1000] train loss: 5.498824834823608 validation loss: 19.06787872314453\n",
      "[0597/1000] train loss: 5.359969854354858 validation loss: 19.028467178344727\n",
      "[0598/1000] train loss: 5.580634117126465 validation loss: 19.18654441833496\n",
      "[0599/1000] train loss: 5.527620792388916 validation loss: 18.833946228027344\n",
      "[0600/1000] train loss: 5.478754758834839 validation loss: 19.03496742248535\n",
      "[0601/1000] train loss: 5.369158983230591 validation loss: 18.950620651245117\n",
      "[0602/1000] train loss: 5.500854253768921 validation loss: 18.87057876586914\n",
      "[0603/1000] train loss: 5.494704484939575 validation loss: 18.934206008911133\n",
      "[0604/1000] train loss: 5.508424997329712 validation loss: 18.80398178100586\n",
      "[0605/1000] train loss: 5.4501869678497314 validation loss: 18.855302810668945\n",
      "[0606/1000] train loss: 5.47930121421814 validation loss: 18.91249656677246\n",
      "[0607/1000] train loss: 5.483059406280518 validation loss: 18.79092788696289\n",
      "[0608/1000] train loss: 5.504754066467285 validation loss: 18.77658462524414\n",
      "[0609/1000] train loss: 5.334964990615845 validation loss: 18.89252281188965\n",
      "[0610/1000] train loss: 5.488694429397583 validation loss: 18.797243118286133\n",
      "[0611/1000] train loss: 5.488856077194214 validation loss: 19.09014129638672\n",
      "[0612/1000] train loss: 5.5429792404174805 validation loss: 18.95737075805664\n",
      "[0613/1000] train loss: 5.627343654632568 validation loss: 19.338394165039062\n",
      "[0614/1000] train loss: 5.560476303100586 validation loss: 18.980894088745117\n",
      "[0615/1000] train loss: 5.428473472595215 validation loss: 18.829492568969727\n",
      "[0616/1000] train loss: 5.265012502670288 validation loss: 19.19259262084961\n",
      "[0617/1000] train loss: 5.130754709243774 validation loss: 18.9531192779541\n",
      "[0618/1000] train loss: 5.435022354125977 validation loss: 18.953720092773438\n",
      "[0619/1000] train loss: 5.404350280761719 validation loss: 18.729475021362305\n",
      "[0620/1000] train loss: 5.383593559265137 validation loss: 18.94199562072754\n",
      "[0621/1000] train loss: 5.4145426750183105 validation loss: 18.950124740600586\n",
      "[0622/1000] train loss: 5.247864723205566 validation loss: 19.01071548461914\n",
      "[0623/1000] train loss: 5.412311553955078 validation loss: 18.90326690673828\n",
      "[0624/1000] train loss: 5.461146593093872 validation loss: 19.043846130371094\n",
      "[0625/1000] train loss: 5.378702640533447 validation loss: 18.798847198486328\n",
      "[0626/1000] train loss: 5.347941160202026 validation loss: 18.798843383789062\n",
      "[0627/1000] train loss: 5.308683395385742 validation loss: 18.81561851501465\n",
      "[0628/1000] train loss: 5.38234806060791 validation loss: 18.890626907348633\n",
      "[0629/1000] train loss: 5.355863571166992 validation loss: 18.53936004638672\n",
      "[0630/1000] train loss: 5.359605312347412 validation loss: 18.83732032775879\n",
      "[0631/1000] train loss: 5.357111930847168 validation loss: 18.858259201049805\n",
      "[0632/1000] train loss: 5.392536401748657 validation loss: 18.830799102783203\n",
      "[0633/1000] train loss: 5.3327250480651855 validation loss: 18.76998519897461\n",
      "[0634/1000] train loss: 5.317743301391602 validation loss: 18.83437728881836\n",
      "[0635/1000] train loss: 5.333461284637451 validation loss: 18.604053497314453\n",
      "[0636/1000] train loss: 5.3842127323150635 validation loss: 18.48846435546875\n",
      "[0637/1000] train loss: 5.291866302490234 validation loss: 19.041702270507812\n",
      "[0638/1000] train loss: 5.372506856918335 validation loss: 19.01038932800293\n",
      "[0639/1000] train loss: 5.289998292922974 validation loss: 18.643619537353516\n",
      "[0640/1000] train loss: 5.3053882122039795 validation loss: 18.667333602905273\n",
      "[0641/1000] train loss: 5.266353130340576 validation loss: 18.67953109741211\n",
      "[0642/1000] train loss: 5.231067180633545 validation loss: 18.73674964904785\n",
      "[0643/1000] train loss: 5.108693599700928 validation loss: 18.37206268310547\n",
      "[0644/1000] train loss: 5.368206739425659 validation loss: 18.694671630859375\n",
      "[0645/1000] train loss: 5.391110897064209 validation loss: 18.448362350463867\n",
      "[0646/1000] train loss: 5.406111478805542 validation loss: 18.530927658081055\n",
      "[0647/1000] train loss: 5.249610662460327 validation loss: 18.67215347290039\n",
      "[0648/1000] train loss: 5.27090311050415 validation loss: 18.736635208129883\n",
      "[0649/1000] train loss: 5.235912561416626 validation loss: 18.628379821777344\n",
      "[0650/1000] train loss: 5.248744249343872 validation loss: 18.8597354888916\n",
      "[0651/1000] train loss: 5.285388946533203 validation loss: 18.635080337524414\n",
      "[0652/1000] train loss: 5.2957868576049805 validation loss: 18.412261962890625\n",
      "[0653/1000] train loss: 5.299255132675171 validation loss: 18.482465744018555\n",
      "[0654/1000] train loss: 5.1755876541137695 validation loss: 18.796112060546875\n",
      "[0655/1000] train loss: 5.231627941131592 validation loss: 18.387853622436523\n",
      "[0656/1000] train loss: 5.14354944229126 validation loss: 18.71981430053711\n",
      "[0657/1000] train loss: 5.19470477104187 validation loss: 18.673534393310547\n",
      "[0658/1000] train loss: 5.221664905548096 validation loss: 18.751596450805664\n",
      "[0659/1000] train loss: 5.1603639125823975 validation loss: 18.547452926635742\n",
      "[0660/1000] train loss: 5.212798595428467 validation loss: 18.607803344726562\n",
      "[0661/1000] train loss: 5.182416200637817 validation loss: 18.98015785217285\n",
      "[0662/1000] train loss: 5.054628849029541 validation loss: 18.641984939575195\n",
      "[0663/1000] train loss: 5.124223232269287 validation loss: 18.503232955932617\n",
      "[0664/1000] train loss: 5.258513450622559 validation loss: 18.672767639160156\n",
      "[0665/1000] train loss: 5.196366786956787 validation loss: 18.58803939819336\n",
      "[0666/1000] train loss: 5.125519514083862 validation loss: 18.533689498901367\n",
      "[0667/1000] train loss: 5.03442907333374 validation loss: 18.676782608032227\n",
      "[0668/1000] train loss: 5.1962056159973145 validation loss: 18.63495635986328\n",
      "[0669/1000] train loss: 5.120083570480347 validation loss: 18.766510009765625\n",
      "[0670/1000] train loss: 5.081961154937744 validation loss: 18.351844787597656\n",
      "[0671/1000] train loss: 5.146756649017334 validation loss: 18.56723976135254\n",
      "[0672/1000] train loss: 5.150310039520264 validation loss: 18.390520095825195\n",
      "[0673/1000] train loss: 5.1415627002716064 validation loss: 18.51769256591797\n",
      "[0674/1000] train loss: 5.046069622039795 validation loss: 18.522539138793945\n",
      "[0675/1000] train loss: 5.2330708503723145 validation loss: 18.618389129638672\n",
      "[0676/1000] train loss: 5.105268955230713 validation loss: 18.605243682861328\n",
      "[0677/1000] train loss: 5.119240045547485 validation loss: 18.713903427124023\n",
      "[0678/1000] train loss: 5.132225751876831 validation loss: 18.598819732666016\n",
      "[0679/1000] train loss: 5.152589797973633 validation loss: 18.755659103393555\n",
      "[0680/1000] train loss: 5.262995004653931 validation loss: 18.701019287109375\n",
      "[0681/1000] train loss: 5.116399049758911 validation loss: 18.402347564697266\n",
      "[0682/1000] train loss: 5.090590953826904 validation loss: 18.401098251342773\n",
      "[0683/1000] train loss: 5.111072540283203 validation loss: 18.27352523803711\n",
      "[0684/1000] train loss: 5.068416357040405 validation loss: 18.546131134033203\n",
      "[0685/1000] train loss: 5.07552695274353 validation loss: 18.553010940551758\n",
      "[0686/1000] train loss: 5.115947842597961 validation loss: 18.749658584594727\n",
      "[0687/1000] train loss: 5.077675104141235 validation loss: 18.523334503173828\n",
      "[0688/1000] train loss: 5.1121392250061035 validation loss: 18.677854537963867\n",
      "[0689/1000] train loss: 5.124178647994995 validation loss: 18.58420181274414\n",
      "[0690/1000] train loss: 5.1801066398620605 validation loss: 18.798751831054688\n",
      "[0691/1000] train loss: 5.1037917137146 validation loss: 18.43572425842285\n",
      "[0692/1000] train loss: 5.1061484813690186 validation loss: 18.759878158569336\n",
      "[0693/1000] train loss: 4.982522487640381 validation loss: 18.442195892333984\n",
      "[0694/1000] train loss: 4.975141525268555 validation loss: 18.569068908691406\n",
      "[0695/1000] train loss: 4.836594581604004 validation loss: 18.559797286987305\n",
      "[0696/1000] train loss: 5.08028244972229 validation loss: 18.473608016967773\n",
      "[0697/1000] train loss: 5.00550103187561 validation loss: 18.593799591064453\n",
      "[0698/1000] train loss: 4.91865086555481 validation loss: 18.36515998840332\n",
      "[0699/1000] train loss: 4.992281436920166 validation loss: 18.368501663208008\n",
      "[0700/1000] train loss: 5.036424160003662 validation loss: 18.28923988342285\n",
      "[0701/1000] train loss: 5.029650449752808 validation loss: 18.39234733581543\n",
      "[0702/1000] train loss: 5.0346879959106445 validation loss: 18.20534324645996\n",
      "[0703/1000] train loss: 4.987292528152466 validation loss: 18.68433380126953\n",
      "[0704/1000] train loss: 4.928282260894775 validation loss: 18.2180233001709\n",
      "[0705/1000] train loss: 5.049661636352539 validation loss: 18.67624282836914\n",
      "[0706/1000] train loss: 4.977695941925049 validation loss: 18.5424747467041\n",
      "[0707/1000] train loss: 5.028841018676758 validation loss: 18.631879806518555\n",
      "[0708/1000] train loss: 5.096899747848511 validation loss: 18.163110733032227\n",
      "[0709/1000] train loss: 5.039347887039185 validation loss: 18.290063858032227\n",
      "[0710/1000] train loss: 4.981048822402954 validation loss: 18.484514236450195\n",
      "[0711/1000] train loss: 4.97930383682251 validation loss: 18.180248260498047\n",
      "[0712/1000] train loss: 4.983797788619995 validation loss: 18.22577667236328\n",
      "[0713/1000] train loss: 4.932676076889038 validation loss: 18.564062118530273\n",
      "[0714/1000] train loss: 4.770331144332886 validation loss: 18.338991165161133\n",
      "[0715/1000] train loss: 4.916239261627197 validation loss: 18.6632022857666\n",
      "[0716/1000] train loss: 4.980386972427368 validation loss: 18.635236740112305\n",
      "[0717/1000] train loss: 4.9107019901275635 validation loss: 18.455793380737305\n",
      "[0718/1000] train loss: 4.854696035385132 validation loss: 18.78694725036621\n",
      "[0719/1000] train loss: 4.918341159820557 validation loss: 18.287904739379883\n",
      "[0720/1000] train loss: 4.89787483215332 validation loss: 18.199331283569336\n",
      "[0721/1000] train loss: 4.91956090927124 validation loss: 18.293285369873047\n",
      "[0722/1000] train loss: 5.019554853439331 validation loss: 18.30333709716797\n",
      "[0723/1000] train loss: 4.998707294464111 validation loss: 18.33856201171875\n",
      "[0724/1000] train loss: 4.969799041748047 validation loss: 18.54410743713379\n",
      "[0725/1000] train loss: 4.916253566741943 validation loss: 18.346731185913086\n",
      "[0726/1000] train loss: 4.8246541023254395 validation loss: 18.261323928833008\n",
      "[0727/1000] train loss: 4.935547828674316 validation loss: 18.338115692138672\n",
      "[0728/1000] train loss: 4.867473602294922 validation loss: 18.56402587890625\n",
      "[0729/1000] train loss: 4.866113901138306 validation loss: 18.293825149536133\n",
      "[0730/1000] train loss: 4.8917155265808105 validation loss: 18.403738021850586\n",
      "[0731/1000] train loss: 4.787222385406494 validation loss: 18.412025451660156\n",
      "[0732/1000] train loss: 4.9389328956604 validation loss: 18.502248764038086\n",
      "[0733/1000] train loss: 4.903194904327393 validation loss: 18.384294509887695\n",
      "[0734/1000] train loss: 4.718676805496216 validation loss: 18.256818771362305\n",
      "[0735/1000] train loss: 4.848373889923096 validation loss: 18.407684326171875\n",
      "[0736/1000] train loss: 4.813954591751099 validation loss: 18.368663787841797\n",
      "[0737/1000] train loss: 4.935699701309204 validation loss: 18.581727981567383\n",
      "[0738/1000] train loss: 4.886048316955566 validation loss: 18.4760799407959\n",
      "[0739/1000] train loss: 4.835376977920532 validation loss: 18.217947006225586\n",
      "[0740/1000] train loss: 5.0169970989227295 validation loss: 17.99108123779297\n",
      "[0741/1000] train loss: 4.830949068069458 validation loss: 18.649866104125977\n",
      "[0742/1000] train loss: 4.81665825843811 validation loss: 18.3756046295166\n",
      "[0743/1000] train loss: 4.792465448379517 validation loss: 18.280261993408203\n",
      "[0744/1000] train loss: 4.760292053222656 validation loss: 18.615341186523438\n",
      "[0745/1000] train loss: 4.809465646743774 validation loss: 18.365394592285156\n",
      "[0746/1000] train loss: 4.845078468322754 validation loss: 18.577688217163086\n",
      "[0747/1000] train loss: 4.8090980052948 validation loss: 18.30921173095703\n",
      "[0748/1000] train loss: 4.8003456592559814 validation loss: 18.360639572143555\n",
      "[0749/1000] train loss: 4.785890579223633 validation loss: 18.20586585998535\n",
      "[0750/1000] train loss: 4.822523593902588 validation loss: 18.206356048583984\n",
      "[0751/1000] train loss: 4.78927743434906 validation loss: 18.370441436767578\n",
      "[0752/1000] train loss: 4.824258327484131 validation loss: 18.47249412536621\n",
      "[0753/1000] train loss: 4.797329902648926 validation loss: 18.297840118408203\n",
      "[0754/1000] train loss: 4.776020288467407 validation loss: 18.483301162719727\n",
      "[0755/1000] train loss: 4.768404722213745 validation loss: 18.435096740722656\n",
      "[0756/1000] train loss: 4.77928364276886 validation loss: 18.748071670532227\n",
      "[0757/1000] train loss: 4.76256799697876 validation loss: 18.14704132080078\n",
      "[0758/1000] train loss: 4.7301106452941895 validation loss: 18.338054656982422\n",
      "[0759/1000] train loss: 4.698068141937256 validation loss: 18.35917854309082\n",
      "[0760/1000] train loss: 4.883325815200806 validation loss: 18.03572654724121\n",
      "[0761/1000] train loss: 4.881325006484985 validation loss: 18.26670265197754\n",
      "[0762/1000] train loss: 4.75465726852417 validation loss: 18.4836368560791\n",
      "[0763/1000] train loss: 4.786629915237427 validation loss: 18.371593475341797\n",
      "[0764/1000] train loss: 4.735293865203857 validation loss: 18.141836166381836\n",
      "[0765/1000] train loss: 4.789710283279419 validation loss: 18.051713943481445\n",
      "[0766/1000] train loss: 4.830358028411865 validation loss: 18.167125701904297\n",
      "[0767/1000] train loss: 4.733820676803589 validation loss: 18.371835708618164\n",
      "[0768/1000] train loss: 4.741095304489136 validation loss: 18.171539306640625\n",
      "[0769/1000] train loss: 4.635227560997009 validation loss: 18.193920135498047\n",
      "[0770/1000] train loss: 4.688355207443237 validation loss: 18.25080680847168\n",
      "[0771/1000] train loss: 4.701694369316101 validation loss: 18.22281837463379\n",
      "[0772/1000] train loss: 4.7649619579315186 validation loss: 18.492048263549805\n",
      "[0773/1000] train loss: 4.552477955818176 validation loss: 18.4215030670166\n",
      "[0774/1000] train loss: 4.660548448562622 validation loss: 18.31551742553711\n",
      "[0775/1000] train loss: 4.712934970855713 validation loss: 18.0812931060791\n",
      "[0776/1000] train loss: 4.765154838562012 validation loss: 18.32257652282715\n",
      "[0777/1000] train loss: 4.679427862167358 validation loss: 18.284406661987305\n",
      "[0778/1000] train loss: 4.688527584075928 validation loss: 18.43721580505371\n",
      "[0779/1000] train loss: 4.689908742904663 validation loss: 18.584152221679688\n",
      "[0780/1000] train loss: 4.746410131454468 validation loss: 18.59151840209961\n",
      "[0781/1000] train loss: 4.645842552185059 validation loss: 18.00482940673828\n",
      "[0782/1000] train loss: 4.686169862747192 validation loss: 18.29726791381836\n",
      "[0783/1000] train loss: 4.640573740005493 validation loss: 18.318628311157227\n",
      "[0784/1000] train loss: 4.612450838088989 validation loss: 18.371599197387695\n",
      "[0785/1000] train loss: 4.645800828933716 validation loss: 18.1038818359375\n",
      "[0786/1000] train loss: 4.625781774520874 validation loss: 18.15729331970215\n",
      "[0787/1000] train loss: 4.697479724884033 validation loss: 18.460201263427734\n",
      "[0788/1000] train loss: 4.67248272895813 validation loss: 18.390823364257812\n",
      "[0789/1000] train loss: 4.640195846557617 validation loss: 18.41180419921875\n",
      "[0790/1000] train loss: 4.589282512664795 validation loss: 18.096921920776367\n",
      "[0791/1000] train loss: 4.589353561401367 validation loss: 18.303709030151367\n",
      "[0792/1000] train loss: 4.726950407028198 validation loss: 18.41422462463379\n",
      "[0793/1000] train loss: 4.610389947891235 validation loss: 18.00200653076172\n",
      "[0794/1000] train loss: 4.730284929275513 validation loss: 17.997652053833008\n",
      "[0795/1000] train loss: 4.722913146018982 validation loss: 18.001636505126953\n",
      "[0796/1000] train loss: 4.6607747077941895 validation loss: 17.98105239868164\n",
      "[0797/1000] train loss: 4.529096364974976 validation loss: 18.54375648498535\n",
      "[0798/1000] train loss: 4.601907730102539 validation loss: 18.09581756591797\n",
      "[0799/1000] train loss: 4.566941976547241 validation loss: 18.087778091430664\n",
      "[0800/1000] train loss: 4.581651926040649 validation loss: 18.138751983642578\n",
      "[0801/1000] train loss: 4.3920111656188965 validation loss: 18.32175064086914\n",
      "[0802/1000] train loss: 4.5471460819244385 validation loss: 18.249792098999023\n",
      "[0803/1000] train loss: 4.531630516052246 validation loss: 18.00558090209961\n",
      "[0804/1000] train loss: 4.540859937667847 validation loss: 18.085494995117188\n",
      "[0805/1000] train loss: 4.5320725440979 validation loss: 18.324697494506836\n",
      "[0806/1000] train loss: 4.577706813812256 validation loss: 18.177724838256836\n",
      "[0807/1000] train loss: 4.553406000137329 validation loss: 18.148157119750977\n",
      "[0808/1000] train loss: 4.532408714294434 validation loss: 18.055845260620117\n",
      "[0809/1000] train loss: 4.645729064941406 validation loss: 18.05214500427246\n",
      "[0810/1000] train loss: 4.4909987449646 validation loss: 18.21197509765625\n",
      "[0811/1000] train loss: 4.558020353317261 validation loss: 18.172239303588867\n",
      "[0812/1000] train loss: 4.646434545516968 validation loss: 17.98108673095703\n",
      "[0813/1000] train loss: 4.448674440383911 validation loss: 18.200803756713867\n",
      "[0814/1000] train loss: 4.46509850025177 validation loss: 18.310712814331055\n",
      "[0815/1000] train loss: 4.527259111404419 validation loss: 18.297264099121094\n",
      "[0816/1000] train loss: 4.50176477432251 validation loss: 18.095861434936523\n",
      "[0817/1000] train loss: 4.279495000839233 validation loss: 18.069074630737305\n",
      "[0818/1000] train loss: 4.438656568527222 validation loss: 18.459714889526367\n",
      "[0819/1000] train loss: 4.562896966934204 validation loss: 18.19443702697754\n",
      "[0820/1000] train loss: 4.464002847671509 validation loss: 17.9785213470459\n",
      "[0821/1000] train loss: 4.5576547384262085 validation loss: 17.917821884155273\n",
      "[0822/1000] train loss: 4.543447256088257 validation loss: 18.05181121826172\n",
      "[0823/1000] train loss: 4.510960340499878 validation loss: 18.09352684020996\n",
      "[0824/1000] train loss: 4.427540302276611 validation loss: 17.947158813476562\n",
      "[0825/1000] train loss: 4.512753009796143 validation loss: 18.44043731689453\n",
      "[0826/1000] train loss: 4.531974792480469 validation loss: 17.94037628173828\n",
      "[0827/1000] train loss: 4.431505918502808 validation loss: 18.17102813720703\n",
      "[0828/1000] train loss: 4.388951539993286 validation loss: 18.219181060791016\n",
      "[0829/1000] train loss: 4.514907360076904 validation loss: 18.478517532348633\n",
      "[0830/1000] train loss: 4.411463737487793 validation loss: 18.09174156188965\n",
      "[0831/1000] train loss: 4.492436170578003 validation loss: 18.786041259765625\n",
      "[0832/1000] train loss: 4.621095418930054 validation loss: 18.432493209838867\n",
      "[0833/1000] train loss: 4.40616250038147 validation loss: 18.250255584716797\n",
      "[0834/1000] train loss: 4.462785959243774 validation loss: 17.847972869873047\n",
      "[0835/1000] train loss: 4.4729163646698 validation loss: 18.051774978637695\n",
      "[0836/1000] train loss: 4.407790660858154 validation loss: 17.950511932373047\n",
      "[0837/1000] train loss: 4.480074167251587 validation loss: 18.45767593383789\n",
      "[0838/1000] train loss: 4.507869720458984 validation loss: 18.1585636138916\n",
      "[0839/1000] train loss: 4.419982671737671 validation loss: 18.032562255859375\n",
      "[0840/1000] train loss: 4.32163143157959 validation loss: 18.138233184814453\n",
      "[0841/1000] train loss: 4.449512720108032 validation loss: 17.9720401763916\n",
      "[0842/1000] train loss: 4.425562143325806 validation loss: 18.178476333618164\n",
      "[0843/1000] train loss: 4.3377087116241455 validation loss: 18.060012817382812\n",
      "[0844/1000] train loss: 4.389582872390747 validation loss: 18.16269302368164\n",
      "[0845/1000] train loss: 4.46994161605835 validation loss: 18.17241859436035\n",
      "[0846/1000] train loss: 4.447649240493774 validation loss: 18.137880325317383\n",
      "[0847/1000] train loss: 4.409620523452759 validation loss: 18.257200241088867\n",
      "[0848/1000] train loss: 4.459509611129761 validation loss: 18.424402236938477\n",
      "[0849/1000] train loss: 4.574372172355652 validation loss: 18.560083389282227\n",
      "[0850/1000] train loss: 4.464279294013977 validation loss: 18.220134735107422\n",
      "[0851/1000] train loss: 4.383659362792969 validation loss: 18.1605224609375\n",
      "[0852/1000] train loss: 4.384094715118408 validation loss: 17.97959327697754\n",
      "[0853/1000] train loss: 4.45936131477356 validation loss: 17.839202880859375\n",
      "[0854/1000] train loss: 4.395662069320679 validation loss: 18.03032112121582\n",
      "[0855/1000] train loss: 4.362836956977844 validation loss: 18.204526901245117\n",
      "[0856/1000] train loss: 4.363497495651245 validation loss: 18.141820907592773\n",
      "[0857/1000] train loss: 4.37436580657959 validation loss: 18.486736297607422\n",
      "[0858/1000] train loss: 4.439215660095215 validation loss: 18.192304611206055\n",
      "[0859/1000] train loss: 4.437626481056213 validation loss: 18.238019943237305\n",
      "[0860/1000] train loss: 4.270204067230225 validation loss: 17.99228858947754\n",
      "[0861/1000] train loss: 4.340807199478149 validation loss: 18.061243057250977\n",
      "[0862/1000] train loss: 4.489411115646362 validation loss: 17.772605895996094\n",
      "[0863/1000] train loss: 4.397794485092163 validation loss: 18.21588706970215\n",
      "[0864/1000] train loss: 4.4017417430877686 validation loss: 18.25661849975586\n",
      "[0865/1000] train loss: 4.323805093765259 validation loss: 18.038442611694336\n",
      "[0866/1000] train loss: 4.330845832824707 validation loss: 18.065582275390625\n",
      "[0867/1000] train loss: 4.355052709579468 validation loss: 17.91147804260254\n",
      "[0868/1000] train loss: 4.146810531616211 validation loss: 17.98016929626465\n",
      "[0869/1000] train loss: 4.359508275985718 validation loss: 17.796838760375977\n",
      "[0870/1000] train loss: 4.323760032653809 validation loss: 18.05483055114746\n",
      "[0871/1000] train loss: 4.221193432807922 validation loss: 18.131662368774414\n",
      "[0872/1000] train loss: 4.219001412391663 validation loss: 17.958702087402344\n",
      "[0873/1000] train loss: 4.247501373291016 validation loss: 18.25968360900879\n",
      "[0874/1000] train loss: 4.308245420455933 validation loss: 18.115039825439453\n",
      "[0875/1000] train loss: 4.302187561988831 validation loss: 18.063735961914062\n",
      "[0876/1000] train loss: 4.285791397094727 validation loss: 18.387834548950195\n",
      "[0877/1000] train loss: 4.242188096046448 validation loss: 17.983959197998047\n",
      "[0878/1000] train loss: 4.308850407600403 validation loss: 18.329938888549805\n",
      "[0879/1000] train loss: 4.425899267196655 validation loss: 17.76593589782715\n",
      "[0880/1000] train loss: 4.344817638397217 validation loss: 18.0710391998291\n",
      "[0881/1000] train loss: 4.30011510848999 validation loss: 18.21163558959961\n",
      "[0882/1000] train loss: 4.270906329154968 validation loss: 17.923614501953125\n",
      "[0883/1000] train loss: 4.198384761810303 validation loss: 18.16989517211914\n",
      "[0884/1000] train loss: 4.27525794506073 validation loss: 17.809070587158203\n",
      "[0885/1000] train loss: 4.255352735519409 validation loss: 17.805267333984375\n",
      "[0886/1000] train loss: 4.2457826137542725 validation loss: 17.921279907226562\n",
      "[0887/1000] train loss: 4.203462600708008 validation loss: 17.80293083190918\n",
      "[0888/1000] train loss: 4.293507814407349 validation loss: 17.920120239257812\n",
      "[0889/1000] train loss: 4.313881993293762 validation loss: 18.163650512695312\n",
      "[0890/1000] train loss: 4.281766891479492 validation loss: 17.743026733398438\n",
      "[0891/1000] train loss: 4.237477660179138 validation loss: 17.83407211303711\n",
      "[0892/1000] train loss: 4.149527072906494 validation loss: 18.274600982666016\n",
      "[0893/1000] train loss: 4.215831875801086 validation loss: 17.90630340576172\n",
      "[0894/1000] train loss: 4.254846811294556 validation loss: 17.981884002685547\n",
      "[0895/1000] train loss: 4.193084001541138 validation loss: 17.821176528930664\n",
      "[0896/1000] train loss: 4.190140604972839 validation loss: 17.900619506835938\n",
      "[0897/1000] train loss: 4.177292823791504 validation loss: 17.94891357421875\n",
      "[0898/1000] train loss: 4.142229795455933 validation loss: 18.004810333251953\n",
      "[0899/1000] train loss: 4.2256858348846436 validation loss: 17.7918701171875\n",
      "[0900/1000] train loss: 4.233743906021118 validation loss: 17.811992645263672\n",
      "[0901/1000] train loss: 4.206034064292908 validation loss: 17.750776290893555\n",
      "[0902/1000] train loss: 4.210495352745056 validation loss: 17.801774978637695\n",
      "[0903/1000] train loss: 4.18442702293396 validation loss: 18.041217803955078\n",
      "[0904/1000] train loss: 4.1710991859436035 validation loss: 17.901079177856445\n",
      "[0905/1000] train loss: 4.104088068008423 validation loss: 18.124284744262695\n",
      "[0906/1000] train loss: 4.23041033744812 validation loss: 18.2078800201416\n",
      "[0907/1000] train loss: 4.190674781799316 validation loss: 17.93672752380371\n",
      "[0908/1000] train loss: 4.201619863510132 validation loss: 17.96260643005371\n",
      "[0909/1000] train loss: 4.08769154548645 validation loss: 17.97793960571289\n",
      "[0910/1000] train loss: 4.190208673477173 validation loss: 18.149646759033203\n",
      "[0911/1000] train loss: 4.207919359207153 validation loss: 17.951452255249023\n",
      "[0912/1000] train loss: 4.226212024688721 validation loss: 18.175498962402344\n",
      "[0913/1000] train loss: 4.011537671089172 validation loss: 17.45657730102539\n",
      "[0914/1000] train loss: 4.274300813674927 validation loss: 17.700462341308594\n",
      "[0915/1000] train loss: 4.143290281295776 validation loss: 17.79574966430664\n",
      "[0916/1000] train loss: 4.244454860687256 validation loss: 17.519393920898438\n",
      "[0917/1000] train loss: 4.0834304094314575 validation loss: 17.716936111450195\n",
      "[0918/1000] train loss: 4.129162788391113 validation loss: 17.92811393737793\n",
      "[0919/1000] train loss: 4.161548614501953 validation loss: 18.14824676513672\n",
      "[0920/1000] train loss: 4.114178419113159 validation loss: 17.71968650817871\n",
      "[0921/1000] train loss: 4.111597418785095 validation loss: 17.986536026000977\n",
      "[0922/1000] train loss: 4.075158596038818 validation loss: 17.709688186645508\n",
      "[0923/1000] train loss: 4.078974723815918 validation loss: 18.04851722717285\n",
      "[0924/1000] train loss: 4.240992426872253 validation loss: 18.037477493286133\n",
      "[0925/1000] train loss: 4.140367031097412 validation loss: 17.89019203186035\n",
      "[0926/1000] train loss: 4.094951629638672 validation loss: 17.83590316772461\n",
      "[0927/1000] train loss: 4.111264109611511 validation loss: 17.6087703704834\n",
      "[0928/1000] train loss: 4.142713785171509 validation loss: 17.845348358154297\n",
      "[0929/1000] train loss: 4.116688847541809 validation loss: 17.906524658203125\n",
      "[0930/1000] train loss: 4.06667172908783 validation loss: 17.922012329101562\n",
      "[0931/1000] train loss: 4.0196613073349 validation loss: 17.785873413085938\n",
      "[0932/1000] train loss: 3.901944637298584 validation loss: 17.75420379638672\n",
      "[0933/1000] train loss: 4.107762694358826 validation loss: 18.28217315673828\n",
      "[0934/1000] train loss: 4.080508470535278 validation loss: 17.723682403564453\n",
      "[0935/1000] train loss: 4.055480122566223 validation loss: 17.77527618408203\n",
      "[0936/1000] train loss: 3.982604146003723 validation loss: 18.125486373901367\n",
      "[0937/1000] train loss: 4.135159134864807 validation loss: 17.907047271728516\n",
      "[0938/1000] train loss: 4.0395777225494385 validation loss: 17.81543731689453\n",
      "[0939/1000] train loss: 3.983670473098755 validation loss: 17.798995971679688\n",
      "[0940/1000] train loss: 4.035253524780273 validation loss: 17.575164794921875\n",
      "[0941/1000] train loss: 4.052386283874512 validation loss: 17.588973999023438\n",
      "[0942/1000] train loss: 4.025480389595032 validation loss: 17.755373001098633\n",
      "[0943/1000] train loss: 4.097435712814331 validation loss: 17.688854217529297\n",
      "[0944/1000] train loss: 4.190595388412476 validation loss: 17.442066192626953\n",
      "[0945/1000] train loss: 4.050484657287598 validation loss: 17.8159236907959\n",
      "[0946/1000] train loss: 3.9909640550613403 validation loss: 17.77218246459961\n",
      "[0947/1000] train loss: 3.9326589107513428 validation loss: 17.849597930908203\n",
      "[0948/1000] train loss: 3.96743643283844 validation loss: 17.998334884643555\n",
      "[0949/1000] train loss: 4.099965810775757 validation loss: 18.033370971679688\n",
      "[0950/1000] train loss: 4.015999913215637 validation loss: 17.482839584350586\n",
      "[0951/1000] train loss: 4.010962724685669 validation loss: 17.62006950378418\n",
      "[0952/1000] train loss: 3.975803017616272 validation loss: 17.893985748291016\n",
      "[0953/1000] train loss: 4.012694954872131 validation loss: 17.394887924194336\n",
      "[0954/1000] train loss: 3.9801218509674072 validation loss: 17.92067527770996\n",
      "[0955/1000] train loss: 4.026126503944397 validation loss: 17.872087478637695\n",
      "[0956/1000] train loss: 3.958343267440796 validation loss: 17.444665908813477\n",
      "[0957/1000] train loss: 3.8728641271591187 validation loss: 17.40015411376953\n",
      "[0958/1000] train loss: 3.9956616163253784 validation loss: 17.759361267089844\n",
      "[0959/1000] train loss: 3.945178985595703 validation loss: 17.613515853881836\n",
      "[0960/1000] train loss: 3.888183116912842 validation loss: 17.461864471435547\n",
      "[0961/1000] train loss: 4.2137579917907715 validation loss: 17.538925170898438\n",
      "[0962/1000] train loss: 4.032686114311218 validation loss: 17.315404891967773\n",
      "[0963/1000] train loss: 3.9994622468948364 validation loss: 17.574909210205078\n",
      "[0964/1000] train loss: 3.925531029701233 validation loss: 17.79961585998535\n",
      "[0965/1000] train loss: 3.9721912145614624 validation loss: 17.562074661254883\n",
      "[0966/1000] train loss: 3.8829641342163086 validation loss: 17.82501220703125\n",
      "[0967/1000] train loss: 3.9009417295455933 validation loss: 17.56096076965332\n",
      "[0968/1000] train loss: 3.9021350145339966 validation loss: 17.5632266998291\n",
      "[0969/1000] train loss: 3.9063504934310913 validation loss: 17.440170288085938\n",
      "[0970/1000] train loss: 3.8935587406158447 validation loss: 17.61135482788086\n",
      "[0971/1000] train loss: 3.9735357761383057 validation loss: 17.2628231048584\n",
      "[0972/1000] train loss: 3.9394946098327637 validation loss: 17.54483985900879\n",
      "[0973/1000] train loss: 3.885612964630127 validation loss: 17.694183349609375\n",
      "[0974/1000] train loss: 3.894864320755005 validation loss: 17.699003219604492\n",
      "[0975/1000] train loss: 3.8851383924484253 validation loss: 17.477336883544922\n",
      "[0976/1000] train loss: 3.885940909385681 validation loss: 17.4103946685791\n",
      "[0977/1000] train loss: 3.8713696002960205 validation loss: 17.601133346557617\n",
      "[0978/1000] train loss: 3.8638559579849243 validation loss: 17.84233856201172\n",
      "[0979/1000] train loss: 3.8604787588119507 validation loss: 17.496788024902344\n",
      "[0980/1000] train loss: 3.8878204822540283 validation loss: 17.543930053710938\n",
      "[0981/1000] train loss: 3.8849234580993652 validation loss: 17.29995346069336\n",
      "[0982/1000] train loss: 3.936385154724121 validation loss: 17.501558303833008\n",
      "[0983/1000] train loss: 3.880013346672058 validation loss: 17.51590919494629\n",
      "[0984/1000] train loss: 3.8523190021514893 validation loss: 17.58026885986328\n",
      "[0985/1000] train loss: 3.8564085960388184 validation loss: 17.467880249023438\n",
      "[0986/1000] train loss: 3.9130425453186035 validation loss: 17.33249282836914\n",
      "[0987/1000] train loss: 3.8775554895401 validation loss: 17.393993377685547\n",
      "[0988/1000] train loss: 3.733795166015625 validation loss: 17.645959854125977\n",
      "[0989/1000] train loss: 3.858462691307068 validation loss: 17.303014755249023\n",
      "[0990/1000] train loss: 3.874308943748474 validation loss: 17.56769371032715\n",
      "[0991/1000] train loss: 3.830723524093628 validation loss: 17.70688247680664\n",
      "[0992/1000] train loss: 3.825610041618347 validation loss: 17.714794158935547\n",
      "[0993/1000] train loss: 3.7896440029144287 validation loss: 17.8587646484375\n",
      "[0994/1000] train loss: 3.841130018234253 validation loss: 17.500823974609375\n",
      "[0995/1000] train loss: 3.795202374458313 validation loss: 17.393413543701172\n",
      "[0996/1000] train loss: 3.8323285579681396 validation loss: 17.953792572021484\n",
      "[0997/1000] train loss: 4.0322383642196655 validation loss: 17.811372756958008\n",
      "[0998/1000] train loss: 3.950517416000366 validation loss: 17.660259246826172\n",
      "[0999/1000] train loss: 3.82654345035553 validation loss: 17.57992172241211\n",
      "[1000/1000] train loss: 3.7931796312332153 validation loss: 17.472978591918945\n",
      "학습에 걸린 시간: 6.09477972984314 초\n"
     ]
    }
   ],
   "source": [
    "####### train + evaluation\n",
    "import time # 시간을 재기 위해서\n",
    "## epoch별 검증 결과를 저장할 변수\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "# 1 epoch 학습 두단계: 학습(train_loader) -> 검증(test_loader)\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # epoch별 처리\n",
    "    ##########################\n",
    "    # 학습\n",
    "    ##########################\n",
    "    # 1. 모델을 train 모드로 변환 -> 모델을 구성하는 layer함수들을 train 모드로 변환.\n",
    "    model.train()\n",
    "    train_loss = 0.0 # 현재 epoch의 train_loss를 저장할 변수\n",
    "    # batch 단위로 학습 -> dataloader를 순환반복.\n",
    "    for X_train, y_train in train_loader:\n",
    "        ### 한 step 학습 \n",
    "        # 2. X, y를 device로 옮긴다. => 모델과 같은 device로 이동.\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 3. X -입력-> model -추정> pred : 모델 추정 - forward propagation (model.forward() 호출)\n",
    "        pred_train = model(X_train)\n",
    "        # 4. loss 계산 \n",
    "        loss = loss_fn(pred_train, y_train)  #(모델추정값,  정답)\n",
    "        # 5. 모델의 파라미터들의 gradient 계산\n",
    "        loss.backward()\n",
    "        # 6. 파라미터들 업데이트  (w.data = w.data - w.grad * lr)\n",
    "        optimizer.step()\n",
    "        # 7. 파라미터의 gradient값 초기화(0으로 변경.)\n",
    "        optimizer.zero_grad()\n",
    "        ## 로그출력을 위해 loss를 train_loss에 누적\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # 한 에폭 학습이 완료 - train loss 평균 계산.\n",
    "    train_loss /= len(train_loader)  #train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    ###########################\n",
    "    # 검증 - 한 에폭학습한 결과에 대한 검증 \n",
    "    ##########################\n",
    "    # 1. 모델을 evalution 모드로 변환 - 검증/최종평가/추론 \n",
    "    model.eval()\n",
    "    val_loss = 0.0 # 현재 epoch에 대한 검증 결과(loss)를 저장할 변수.\n",
    "    # 검증(평가, 추론)은 gradient를 계산할 필요가 없으므로 forward시 grad_fn을 만들 필요가 없다.\n",
    "    with torch.no_grad(): # with 문 내에서 하는 연산에서는 grad_fn을 만들지 않는다. \n",
    "        for X_val, y_val in test_loader:\n",
    "            #1. device로 옮기기\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            #2. 모델을 이용해 추정\n",
    "            pred_val = model(X_val)\n",
    "            #3. 검증 - MSE\n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            # 로그출력을 위해서 검증 결과를 val_loss에 누적\n",
    "            val_loss += loss_val.item()\n",
    "        # val_loss 평균\n",
    "        val_loss /= len(test_loader)\n",
    "    #### 한 에폭의 학습/검증 완료 -> train_loss, val_loss 평균를 list에 추가.\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(val_loss)\n",
    "    print(f\"[{epoch+1:04d}/{epochs}] train loss: {train_loss} validation loss: {val_loss}\")\n",
    "e = time.time()\n",
    "print(f\"학습에 걸린 시간: {e-s} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApkAAAGwCAYAAADi5H4xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACNkElEQVR4nO3dd3wUdf7H8de29E5ICBBC70VEBFTAih4KVryznp6917Oc59nLz9PTU892xa54Z1cQkCZoUIqhhE5oAQKkJ6Tu7MzvjyEbAiRsyJb5Zj/Px4NHZmd3Z76TdzZ88v3OfMdmGIaBEEIIIYQQfmQPdQOEEEIIIUT7I0WmEEIIIYTwOykyhRBCCCGE30mRKYQQQggh/E6KTCGEEEII4XdSZAohhBBCCL+TIlMIIYQQQvidFJlCCCGEEMLvpMgUQgghhBB+16oi89ZbbyUxMZHu3bt7/23btg2AnJwcRo8eTVZWFgMHDuT7778PSIOFEEIIIYT12VpzW8lbb72VDh068NhjjzVZX1lZyYABA3jnnXc4/fTT+eGHHzj33HNZt24dnTp18nujhRBCCCGEtbV6uDwpKemQdR9//DEjR47k9NNPB2D8+PGMGzeOTz75pM0NFEIIIYQQ6nG29g2HKzIXLVrEiSee2GTdqFGjWL58ebPbqauro66uzvtY13VKSkro0KEDNputtc0SQgghhBABZhgGlZWVdO7cGbu95b7KVheZDz74II888gi9evXiwQcfZMKECRQUFHDqqac2eV1aWhq//PJLs9t55plnDhl2F0IIIYQQ1pefn0/Xrl1bfE2risyXX36ZV199FY/Hw8yZM7n44ouZM2cOmqZx8KmdHo+nxR7JBx98kLvvvtv7uLy8nG7durF161aSk5PxeDwAOByOJsuapmGz2bzLdrsdu93e7LLb7cbhcHiXnU4nNpuNuro6du/eTdeuXdF1HafT/FZomobL5cIwDO+yrut4PB7vcsPrm1v2eDwYhsGNH+bw8+YSnjl/EJOO6driMdlz3sM++y/ovc6Ai/59VMfUsNxwHIE4poblwx1HIHJq6ZgcDgf5+fl06tSJyMjIdnFM7TGnlo5J0zTy8/PJysoCaBfH1B5zaumYNE1j9+7dZGRkYLPZ2sUxtcecWjomm83G9u3b6dKlCy6Xq10cU3vMqbljOji/QB9TaWkpPXr0ID4+niNpVZHZ0C3qcDiYOHEil1xyCV9++SUpKSkUFRU1eW1hYWGLF/1ERkYSGRl5yPrk5GQSEhJa06yjomkaGzduJD4+3vtN9LeEhETskbXYo+KOfEzdBkOkDep3wWFOSRCH0jTNe9FZoDIUgaVpGlVVVcTFxUmGitI0jQ0bNtC/f3/JUFGaprFv3z4SExMlQwUFO7+GDkRfTm1s0zyZmqYRERHBiBEjyM7ObvJcdnY2Y8aMacvmA8rpdHLCCScENJDoCAcANfWeI784pYf5tWQL6HrA2tSeBCNDEViSofokQ/VJhmqzcn6tKjJnzpyJvr8AmjVrFp999hkXXnghl112GXPmzGHu3LkATJ8+nbVr1zJlyhT/t9hPPB4PmzZt8nY3B0KMa3+R6fZhH0lZ4IgErQbKtgWsTe1JMDIUgSUZqk8yVJ9kqDYr59eqIvPFF1+kU6dOdO/enSeffJIvvviCgQMH0rVrV6ZOncrNN99MWloaTz75JN988w2xsbGBanebNZxX0IppQlutVT2ZDid07Gcu71kdsDa1J8HIUASWZKg+yVB9kqHarJxfqyZjD6SKigoSExMpLy8PyjmZwfB/M9bx+vw8rjmpBw+fM/DIb/jiJljxEZz8Jzj5/sA3UAghhAgwXdepr68PdTOEj1wuFw6Ho9nnW1OvWW8AP0g8Hg8bN26kT58+LX4z2yJ6/3B5tS89mQDpg8yve3ID0p72JhgZisCSDNUnGaovkBnW19ezZcsW76l2wv8armJvuELcH5KSkujUqVObtxe2RSZATU1NQLcfs3+4vNaXczLhgCJThst9FegMReBJhuqTDNUXiAwNw6CgoACHw0FmZuYRJ+4WR8cwDGpra4mKimpzUWgYBtXV1ezduxeAjIyMNm0vbItMh8PB8OHDA7qPKG9PpubbG9IHm19LNkN9NUTEBKhl7UMwMhSBJRmqTzJUX6Ay1DSN6upqOnfuTEyM/H8WSNHR0X7f1t69e0lLS2tT73bY/lnh8XjIzc0N7NXlEa0cLo/rCLEdAQMK1wasXe1FMDIUgSUZqk8yVF+gMmzYXkREhF+3K5oyDIOamhq/XvjT8EeB2+1u03bCtsgMhpjWXF3eQIbMhRBCtCP+Ok9QBI+/MgvbItPhcDB48OCAnqgeHWGejeBzTyY0DplLkXlEwchQBJZkqD7JUH2SodpsNhvR0dGWLObDtsj0eDzk5OQEZbjcp8nYG0hPps+CkaEILMlQfZKh+iTDRpdffjndu3ene/fuOJ1OMjIyvI8LCwtbta3nn3+e11577ajbsnXrVqKioo74uoaLdSwyI2UTYXvhD/j3RNnDaSgyq+p8vPAHmhaZhgEW/MvESgKdoQg8yVB9kqH6JEPTBx984F3u3r07U6dOZfTo0Ue1rXvvvddfzToiK/ZiQhgXmQ6Hg/79+wd0HzH7h8tbdU5maj+wOaCmBCp3Q0Lbpg9oz4KRoQgsyVB9kqH6JMPW03XdMlMyNQyXW5E1vkMhoGkaS5YsQdNa0cvYSt6ry90e37uxXVHQobe5LEPmLQpGhiKwJEP1SYbqC1aGhmFQXa+F5J8/hpJPPvlknn/+eU444QT69TNvA/3xxx8zbNgwunXrRq9evZr0hF511VU8++yzAMyfP5/+/fvzn//8h8GDB9OxY0euvvrqo7562+Px8PzzzzNo0CCysrIYNGgQ77//vvf5PXv2cMEFF9C7d2/S09N56aWXWlwfKGHbk2mz2UhOTg5oF3PDvcs9ukG9RyfS6eNJ1emDoGi9eeefPqcHrH2qC0aGIrAkQ/VJhuoLVoY1bg8D/zIzoPtozprHz/SOLrbFRx99xPTp00lLS/OumzFjBhkZGSxdupRx48YxadIkEhMTD3nv9u3b2bFjB6tWraKkpIRRo0bxwQcfcPXVV7e6HY899hgLFixg/vz5pKamsmLFCs4991xSUlI4++yz+eMf/8iAAQP4/PPP0TSN7du3AzS7PlDCtifT4XDQu3fvgF5NF+Nq3HZ1nVz842/ByFAElmSoPslQfZKh76ZMmUKnTp28Q+WXXHIJqamprFmzhoKCApxOJ3l5eYd9b2RkJH/+85+x2Wx06NCBCy+8kKVLlx5VO/7+97/z+uuv07FjR2w2G8cccwz33Xcf//znP737Wr9+PaWlpTidTnr27Nni+kAJ255MTdNYvHgxxx9/PE5nYL4NToedCKedek2n2u0h2dc3NkxjtHdNQNrVXgQjQxFYkqH6JEP1BSvDaJeDNY+fGbDtH2nf/pCVldXk8d13382MGTMYOnSo94r0+vr6w743PT29yXmcycnJ7Nmzp9VtKCwspLKykr59+wLmaQhVVVX06NGD/Px8AP72t7/x8MMP079/f84//3yefvppUlJSml0fKGHbk2m32+nSpUvAT9xtnJC9NVeYDzS/Fq4H7fA/rCJ4GYrAkQzVJxmqL1gZ2mw2YiKcIfnnt8nFD/gezZ07l+nTp7Ny5UqmTp3KM888E5RpoDp06EBUVFSTHlOXy8WWLVu8PZPx8fG89NJLbNq0CU3TuOyyy1pcHyhh+1vBbreTlZUV8A9V7P5zQKpaM1yemAmRCaC7oXhjgFqmvmBlKAJHMlSfZKg+yfDo1NXVUVdX552j8umnn6ampibg+7Xb7dx0003cdNNNFBUVYbPZ2Lp1K3/729+44447APjhhx9wu93Ex8czduxY9u3b1+L6gLU1oFu3ME3TWLBgQcCvpotu7f3LwZwbU87LPKJgZSgCRzJUn2SoPsnw6Jx55pmcccYZ9O3bl379+pGUlETnzp39uo+6ujrvZPAN/wCeeeYZxo4dy5gxY+jRoweXXXYZr7zyCieddBIAX375JV26dKFPnz68//773nM1m1sfKDbDIlPEV1RUkJiYSHl5OQkJCQHfn67rFBQUkJGREdC/3ia/+iMrd5Tzn6uO49T+6b6/cdo9sORfcOIdcMbjAWufyoKVoQgcyVB9kqH6ApVhbW0tW7ZsoUePHj7duUYcHcMwcLvduFwuv50W0FJ2ranXwvYs7YZzUAKt4WTjVg2XwwE9mXLxT3OClaEIHMlQfZKh+iRDtdlsNiIiIkLdjMMK2z87NU1j7ty5AR8eiI08irv+AKTJcPmRBCtDETiSofokQ/VJhmozDIOKigpL3rs8bItMu93O4MGDAz6803hOZis/vGkDzK+Vu6C6xM+tah+ClaEIHMlQfZKh+iRD9cltJS3GbreTlpYW+CmMXI23lmyVqARI2j8fl/RmHlawMhSBIxmqTzJUn2SoNpvN5tfzMf0pbH+i3G43M2fOPOr7hvrKe//y1p6TCY2Tsu/J9WOL2o9gZSgCRzJUn2SoPslQbbquU15ejq7roW7KIcK2yHQ4HIwcOTLgt9GK2X9OZqumMGrQebj5dfsiP7ao/QhWhiJwJEP1SYbqkwzVZrPZiI2NlZ5MK7Hb7aSkpARtuLzGfRQnVPcYZ37dsgAs+BdKqAUrQxE4kqH6JEP1SYZqs9lsOJ3+u6uRP4XtT5Tb7WbatGkBHx5ouPCn1VMYAXQ5FlwxUFMKRev93DL1BStDETiSofokQ/VJhmrTdZ2ysjIZLrcSp9PJ2LFjcToDO1VobFuGyx0u6HysubxjiR9b1T4EK0MROJKh+iRD9UmGarPZbMTHx0tPppXYbDYSEhICHkrDhT9HNVwO0PU486sUmYcIVoYicCRD9UmG6pMMG5177rk88MADh31u8uTJPPnkky2+32azsXv3bgCef/55XnvttWZfO3XqVE4++eRmn9+6datPd0qy2Ww4HA5L5he2Rabb7earr74K/HD50d7xp0HXkebXHcv81KL2I1gZisCRDNUnGapPMmx0xRVXMHXq1EMmNi8qKmLWrFlceeWVPm/r3nvv5eabb/Z3Ew8hw+UW5HQ6mTBhQsCHB2IijvKOPw0aejL3roG6Sj+1qn0IVoYicCRD9UmG6pMMG02aNIny8nKys7ObrP/kk0848cQT6datW4ha1jwr90SHbZEJBOUDFRPZMBn7UQ6Xx3eCxG6AATt/9V/D2gn5pag+yVB9kqH6gpKhYUB9VWj++XjLxcjISKZMmcJHH33UZP0HH3zA73//e9xuNzfccAPdu3cnMzOT8ePHs3nz5sNu66qrruLZZ5/1Pp4zZw7HHXccmZmZHHfccaxcubJV376amhoefPBB+vfvT1ZWFiNHjmTmzJne5zds2MAZZ5xBr169yMjI4H//+1+L64MhbH8zaJrG9OnTmThxIi6XK2D78Z6TebQ9mQBdR0D5dvO8zJ7j/dQy9QUrQxE4kqH6JEP1BS1DdzU83Tlw22/Jn3ZBRKxPL73iiiu48MIL+fvf/47T6SQvL4/c3FwuvPBC3G43o0aN4tVXX8XlcnH77bfz0EMP8fHHH7e4zbVr1/Lb3/6Wb7/9ltGjR7NlyxbOPPNMOnf2/ftxww03UFdXx9KlS4mLi2PRokVMmjSJ77//nh49enDddddxxRVXcN1111FTU0NRUREA11577WHXB0PY9mQ6nU4mTpwY+OFyl7n9oz4nEw44L3OpH1rUfgQrQxE4kqH6JEP1SYZNnXTSScTFxTF79mzA7MW88MILiY2NJSYmhj/84Q/s27ePX375hbi4OFavPvKtn19//XWuueYaRo8eDUCPHj246667fG5TcXExU6dO5a233iIuLg6AMWPGcPXVV/POO++QkJBAZGQkK1eupLq6mujoaDIzMwGaXR8MYf0TpWlawD9U0d6ryz3ouoHdfhTnTHiLzCVml78Fz7sIlWBkKAJLMlSfZKi+oGToijF7FEPBFePzS202G5dffjkff/wxZ511Fh988AH//Oc/AdiyZQtXXnkluq4zYMAANE2jvr7+iNvMy8tjypQpTdYlJyf73KbNmzeTkZFBYmJik/U9e/b0FsPvvfceDz74ID179uSqq67ikUceITo6mvfff58HHnjgkPXBELY9mZqmMWvWLDTtKM+V9FFsZONtumq1o+zN7DQU7C6oLoLSrf5pWDsQrAxF4EiG6pMM1Re0DG02c8g6FP9a2Tlz+eWX8+WXXzJ//nzcbjfjx5unqj3yyCOceeaZ/PTTT/zrX/9i8uTJPm0vNTWV7du3N1nX3Lmch5OZmcnu3bvZt29fk/VbtmyhR48eVFRUkJ6ezjvvvMOqVavIzc3l7rvvBqBTp06HXR8MYVtkulwuzj333ICfQxTlbCwyj3rI3BUFGUPN5e0/+6FV7UOwMhSBIxmqTzJUn2R4qL59+zJgwADuu+8+rrzySu+V23V1dZSWlgLmtEYvvviiT9u7+OKL+cc//uEdWl+xYgX//ve/fW5Pp06dOOecc7j++uu9heYvv/zChx9+yE033URSUhLz5s1D13U6duzIyJEjva+bM2fOYdcHQ9gWmYZhUFFRcchcWP5mt9u8c2W26eKf7mPNr1t+8EOr2odgZSgCRzJUn2SoPsnw8K644gqWLFnSZG7MRx99lIULF9K1a1cmTZrE7373O5+2dfbZZ/PQQw9xzjnn0K1bN/7yl79wzz33tKo977zzDqmpqQwdOpSePXvywAMP8MUXX9CzZ088Hg9vvPEGnTp1om/fvuTk5PDcc88BNLs+GGyGRX6qKioqSExMpLy8nISEhIDvz+12M2vWLCZMmBDwv95GPPE9xVX1zLhzLP07HeWxbZ4P750LcZ3gnnVyXibBzVAEhmSoPslQfYHKsLa21juc68uda8TR0XWdiooKEhISsNv903fYUnatqdfCtifT5XJx9tlnB+WXYsNcmW26wjxzNDijYN9uKFznp5apLZgZisCQDNUnGapPMlSb3W4nKSnJbwWmP1mvRUGi6zolJSVBuQ1TwzRGbRoud0VBtzHm8ub5bW9UOxDMDEVgSIbqkwzVJxmqzTAMNE2z5OkOYVtkejwelixZgsfThsLPRw3TGFXXt/HKvV6nmF/z5rWxRe1DMDMUgSEZqk8yVJ9kqDbDMKiqqrJkkRm2E5u5XC7OPPPMoOyrYRqjGncbP8A99xeZW38ErR6cEW1smdqCmaEIDMlQfZKh+iRDtdnt9kPmz7SKsO3J1HWdvXv3BmV4INofd/0BSB8MMangrjInZg9zwcxQBIZkqD7JUH2SodoMw8DtdluyJzOsi8zc3NygfKjivBf+tHG43G5vvHe5nJcZ1AxFYEiG6pMM1RfoDK1Y/LQ3NTU1ft2ev34Wwna43Ol0cuqppwZlX7GR5rd5X1uLTDCHzHM/g83z4NSH2r49hQUzQxEYkqH6JEP1BSpDl8uFzWajsLCQjh07eic0F/4XERFBXV1dm7djGAb19fUUFhZit9uJiGjbaXlhW2Tquk5BQQEZGRkBv+w/LrJhuNwPRWbDxT87l0F1CcSktH2bigpmhiIwJEP1SYbqC1SGDoeDrl27smPHDrZu3eq37YqmDMPA4/HgcDj8VsjHxMTQrVu3Nv88hHWRmZeXR3p6esB/Mfq1JzOxK6QNgr2rzR7N469r+zYVFcwMRWBIhuqTDNUXyAzj4uLo06cPbrfbr9sVjTRNIycnh+HDh+N0tr2sczgcOJ1OvxSsYXvHn2D6z49bePzbNZwzNINXLz227Rv8+XWY8YA5Qfs1M9u+PSGEEEIIH8gdf3yg6zrbtm0L0oU/fhwuB+g30fy6YwnUlvtnmwoKZoYiMCRD9UmG6pMM1Wbl/MK6yNy5c2dQQomN9NMURg2Ss6BDbzA8sGWhf7apoGBmKAJDMlSfZKg+yVBtVs4vbItMp9PJCSec4JfzF44kLsrcR6W/ejIBeu2/EjBvjv+2qZhgZigCQzJUn2SoPslQbVbOL2yLTI/Hw6ZNm4JyGy2/zZN5oF6nmV83zQZrnFYbdMHMUASGZKg+yVB9kqHarJxf2BaZhmFQWloalEliY/19TiZAj7HgiISy7VC43n/bVUgwMxSBIRmqTzJUn2SoNivnF7ZFptPpZOTIkUHpXo6N8OMURg0iYs1CE2BjeF5hHswMRWBIhuqTDNUnGarNyvmFbZHp8XhYt25dULqX4/efk1mn6bg9fjwxt8+Z5tcN4VlkBjNDERiSofokQ/VJhmqzcn5hW2SC/+/12ZyG4XLw85B53wnm1+0/Q02p/7arkGBlKAJHMlSfZKg+yVBtVs0vbItMh8PB8OHDcTgcAd+Xy2Enwml+q/06ZJ7cHTr2N6cyypvrv+0qIpgZisCQDNUnGapPMlSblfML2yLT4/GQm5sbtO7lOH/Pldmgz/7ezDAcMg92hsL/JEP1SYbqkwzVZuX8wrbIDLY47/3L/Xz/1r77z8vc+D3o1vsBE0IIIUR4Ctsi0+FwMHjw4KB1L8d6i0w/F4KZoyAqEWpKYMdS/27b4oKdofA/yVB9kqH6JEO1WTm/sC0yPR4POTk5QRwuD8CE7AAOV+PE7Btm+HfbFhfsDIX/SYbqkwzVJxmqzcr5hW2RCRAdHR20fTX2ZPq5yATo9xvz67pp/t+2xQUzQxEYkqH6JEP1SYZqs2p+YVtkOhwO+vfvH7TuZe85mbUBKDL7TAC7C4rWh9Xdf4KdofA/yVB9kqH6JEO1WTm/sC0yNU1jyZIlaFoAir7DiAvErSUbRCdBz5PN5TVf+3/7FhXsDIX/SYbqkwzVJxmqzcr5hW2RabPZSE5OxmazBWV/3uHy+gD9EAw81/y65qvAbN+Cgp2h8D/JUH2SofokQ7VZOb+wLTIdDge9e/cO+nB5QHoyAfqfDTYH7FkFxXmB2YfFBDtD4X+SofokQ/VJhmqzcn5hW2RqmkZ2dnbQh8sDck4mQEwK9BhrLq/+PDD7sJhgZyj8TzJUn2SoPslQbVbO76iLzJtuuon+/ft7H+fk5DB69GiysrIYOHAg33//vV8aGCh2u50uXbpgtwenzg7YPJkHGjLF/Lryv2AYgduPRQQ7Q+F/kqH6JEP1SYZqs3J+R9Wi/Px83nvvPe/jyspKJk2axJNPPsm2bdt4/fXXmTJlCrt37/ZbQ/3NbreTlZUVxCIzQPNkHmjAZHBGQdEGKFgeuP1YRLAzFP4nGapPMlSfZKg2K+d3VC266667uPrqq72PP/74Y0aOHMnpp58OwPjx4xk3bhyffPKJf1oZAJqmsWDBgqB1L8dH7T8nM1AX/gBEJTTOmbnq08DtxyKCnaHwP8lQfZKh+iRDtVk5v1YXmdOmTaO4uJiLLrrIu27RokWceOKJTV43atQoli9f3ux26urqqKioaPIP8M5Y7/F4DrusaVqTZV3XW1x2u91Nlo39w8gej4eePXtis9m86w3DwO027y1+4LKu602WG4Jsbtnj8TRZ9ng8xEaYRWZlrTtgx+R2uzEGnW+2f81XGLoe0GMKRk4HZ3Pgst1up2fPnt79tIdjao85tXRMAN27d8dut7ebY2qPObV0TLqu06tXLwzDaDfH1B5zaumY7HY7PXr0aPL/o+rH1B5zam7ZbrfTvXt3b7uCdUy+aFWRWVxczO23387rr7/eZH1BQQHp6elN1qWlpVFcXNzstp555hkSExO9/zIzMwHIzc0FYO3ataxduxaAlStXsnHjRsA893PLli0ALF68mPz8fACys7MpKCgAYMGCBRQVFQEwd+5cysrKAJg1axaVlZUAzJgxgw4dOqDrOtOnT0fTNGpra5k+fTpgngIwa9YsAMrKypg7dy4ARUVFLFiwwHvc2dnZgHkKweLFiwHYsmULOTk5AGzcuJGVK1d6z8ksr6oL2DFNnz6d2q5jMVyx2Mrz8WxfEtBjCkZO06dPp7a2Fk3TDsnJbreTkJDA7Nmz280xtcecWjqmkpISNm3ahN1ubzfH1B5zaumYli5dSpcuXdi2bVu7Oab2mFNLx2S32ykvL2f9+vXt5pjaY07NHZPdbmfPnj1s27YtKMfUcBw+MXyk67px7rnnGi+//LJhGIYxb948o1+/foZhGMZpp51mvPvuu01e//rrrxuTJ09udnu1tbVGeXm5919+fr4BGCUlJYZhGIamaYamaYcsu93uJssej6fF5fr6+ibLuq4bhmEY1dXVxuzZs436+nrvel3Xjfr6eu/xNix7PJ4my263u8VlTdOaLGuaZmwp3Gdk3f+tMfDh7wJ2TN7j+O9VhvFIgqF/92BAjykYOR2czYHLbrfbmD17tlFdXd1ujqk95tTSMdXV1RmzZ8/27qM9HFN7zKmlY6qpqTHmzJlj1NbWtptjao85tXRMDb9La2tr280xtcecmls+OL9AH1NRUZEBGOXl5caR2AzDt8uQn3nmGX788Ue+/fZbbDYb8+fP58Ybb2TdunVcfPHFjB49mrvvvtv7+ieeeIIdO3bw5ptv+lTsVlRUkJiYSHl5OQkJCb5XyUdJ13WKiopITU0NysmyRfvqOO5Js8dt89MTsdsDOGnqumkw9VKI7Qh3rQFnROD2FULBzlD4n2SoPslQfZKh2oKdX2vqNZ9b8/LLL7Nw4UKSk5NJSkrinHPOYePGjSQlJTFixAhvd2+D7OxsxowZc3RHEAR2u520tLSgfaAa5smEAF/8A+a9zOPSoaoQNnwX2H2FULAzFP4nGapPMlSfZKg2K+fnc4sKCgqoqKigrKyMsrIyvv32W/r06UNZWRmXXXYZc+bM8Y7TT58+nbVr1zJlypSANbyt3G43M2fObNUJrG0R6bTjcpi9lxWBmpC9gcMFx1xmLv/6XsuvVViwMxT+JxmqTzJUn2SoNivn55eyt2vXrkydOpWbb76ZtLQ0nnzySb755htiY2P9sfmAcDgcjBw5Mmi3YbLZbKTEmsPWJfvqA7/D4ZebXzfNgbLtgd9fCAQ7Q+F/kqH6JEP1SYZqs3J+Pp+TGWjBPiczFCb+fSFrCip4++qRnNIvLfA7fOcc2LoQxj8ApzwY+P0JIYQQol0LyDmZ7Y3b7WbatGlB7V5OjY8EoDgYPZkAI64yv+a8Dx7rTdLaVqHIUPiXZKg+yVB9kqHarJxf2BaZTqeTsWPH4nQ6j/xiP0ndP1xevK8uODvsfw5Ep0DFTthk7XvJH41QZCj8SzJUn2SoPslQbVbOL2yLTJvNRkJCAjZbAKcSOkiHuP1FZlWQejJdUXDMpeby0reDs88gCkWGwr8kQ/VJhuqTDNVm5fzCtsh0u9189dVXQe1e7hBnDpcXVQapJxNgxP57zG/6Hsryg7ffIAhFhsK/JEP1SYbqkwzVZuX8wrbIdDqdTJgwIajdy4nRLgAqaoP4g5DaG3qMA0Nvd9MZhSJD4V+SofokQ/VJhmqzcn5hW2QCQQ+k4f7l++qCfBFOQ2/mr++Bx3p/6bSFFT9UonUkQ/VJhuqTDNVm1fzCtsg88ObvwRIfqiKz/znmLSb37YYNM4K77wAKRYbCvyRD9UmG6pMM1Wbl/MJ2nkzDMNA0DafTGbSTZRdvKeHiNxfRIzWWefeeHJR9es1+FH58EbJOgqunBXffARKKDIV/SYbqkwzVJxmqLdj5yTyZPgp21d9w//LKQN9W8nBGXgeOCNj2I2z9Mfj7DxAr/uUmWkcyVJ9kqD7JUG1WzS9si0xN05g1a1ZQg2koMquCPVwOkNgFhl9hLs9/Nvj7D4BQZCj8SzJUn2SoPslQbVbOL2yHy0OheF8dI56cDcCmp36D0xHkGr8sH14eDrobrv4Osk4I7v6FEEIIoTQZLveBYRhUVFQQzBo7Lqrx6q+qek/Q9uuVlAnHtp/ezFBkKPxLMlSfZKg+yVBtVs4vbItMTdNYuHBhULuXI50OXA7zpNygX2He4KS7we6CLT/AtkWhaYOfhCJD4V+SofokQ/VJhmqzcn4yXB5kwx+fRWm1m1l3jaNvenxoGvHNHbDsHeh5Mlz5VWjaIIQQQgjlyHC5D3Rdp6SkBF3Xg7rfhiHzymDe9edgY+8BuxM2z4fdq0LXjjYKVYbCfyRD9UmG6pMM1Wbl/MK2yPR4PCxZsgSPJ7jnRqbERABQvK8+qPttIqkb9D/bXF72buja0UahylD4j2SoPslQfZKh2qycnwyXB9k17yxhzrq9PH3+EC4d1S10DcmbC++fD5GJcM86iIgJXVuEEEIIoQQZLveBruvs3bs36N3LqXGRgDmdUUj1OBmSsqCuHNZ8Gdq2HKVQZSj8RzJUn2SoPslQbVbOL6yLzNzc3KCH0iHOHC4vCnWRabfDiN+by0vfDm1bjlKoMhT+IxmqTzJUn2SoNivnJ8PlQfafH7fw+LdrOHtIBv+47NjQNqZyD7w4EHQNbsqG9EGhbY8QQgghLE2Gy32g6zo7d+4M/nB5vDlcXhjqnkyA+PTGC4CyXwltW45CqDIU/iMZqk8yVJ9kqDYr5xfWRWZeXl7QQ2m4ury8OoRTGB3oxDvMryv/C6XbQtuWVgpVhsJ/JEP1SYbqkwzVZuX8ZLg8yFbuKGPyqz/RKSGKn/90WqibY3rvPNg8D0ZeC2e/EOrWCCGEEMKiZLjcB7qus23btqBX/onRLgAqQjkZ+8HG3mN+/fV9qCgIbVtaIVQZCv+RDNUnGapPMlSblfML6yIzFOcwNBSZ1fUe3B6L/EB0Pwm6jQFPHfz0Uqhb4zMrn4cifCMZqk8yVJ9kqDYr5yfD5UHm0Q16/Wk6AEv/fLp33syQ2zwf3jsXHJFwxwpIyAh1i4QQQghhMTJc7gOPx8OmTZuCfhsmh91GfKR5//KKGgsNmfcY39ib+eOLoW6NT0KVofAfyVB9kqH6JEO1WTm/sC0yDcOgtLSUUHTkJuwfMi+3UpFps8HJD5jLy96Bil0hbY4vQpmh8A/JUH2SofokQ7VZOb+wLTKdTicjR47E6XQGfd+WLDKhaW/mt3eDx2LtO0goMxT+IRmqTzJUn2SoNivnF7ZFpsfjYd26dSHpXk6MNn8QLFdk2mxwyp/M5Q3fwQ/PhbY9RxDKDIV/SIbqkwzVJxmqzcr5hW2RCVBTUxOS/XbYf7FP8b76kOy/RT3GwcTnzeXsV6B8Z2jbcwShylD4j2SoPslQfZKh2qyaX9gWmQ6Hg+HDh+NwOIK+7477i8wiK9xa8nBGXmsOm2s1MPfJULemWaHMUPiHZKg+yVB9kqHarJxf2BaZHo+H3NzckHQvp8aZt5YsrLRokWmzwYSnzOUVH8Gu5SFtTnNCmaHwD8lQfZKh+iRDtVk5v7AtMkMp1eo9mQBdR8Dgi8zlhc+Hti1CCCGEUE7YFpkOh4PBgweHZrg8vqHItOA5mQc68Q7z64ZZUFsR2rYcRigzFP4hGapPMlSfZKg2K+cXtkWmx+MhJycnRMPlCvRkAnQaAqn9zCmNsl8JdWsOEcoMhX9IhuqTDNUnGarNyvmFbZEJEB0dHZL9psY3FplWnDzVy2aDU/9sLme/DKXbQtuewwhVhsJ/JEP1SYbqkwzVZtX8wrbIdDgc9O/fPyTdyx1izQt/3B7DenNlHmzAJOg+FrRa+P7hULemiVBmKPxDMlSfZKg+yVBtVs4vbItMTdNYsmQJmqYFfd9RLgcJUeaE7JYfMrfZ4Df/BzY7rPkKtiwIdYu8Qpmh8A/JUH2SofokQ7VZOb+wLTJtNhvJycnYbLaQ7L9hyLyw0uIX/wCkD4Lj/mAuz3gQPNb4QQ51hqLtJEP1SYbqkwzVZuX8wrbIdDgc9O7dO2Tdyw0X/xRavSezwSkPQVQS7MmFX98JdWuA0Gco2k4yVJ9kqD7JUG1Wzi9si0xN08jOzg5Z97L3rj9WnZD9YDEpZqEJ5l2AqktC2x5Cn6FoO8lQfZKh+iRDtVk5v7AtMu12O126dMFuD823oGGuzD2VtSHZ/1E57g/QcQDUlMK8p0PdmpBnKNpOMlSfZKg+yVBtVs7Pei0KErvdTlZWVshC6ZwUBUBBmUJFpsNpXgQEsPTfsCsnpM0JdYai7SRD9UmG6pMM1Wbl/KzXoiDRNI0FCxaErHs5I9Gc06qgvCYk+z9qPcfDkClg6DDtHgjhPJ+hzlC0nWSoPslQfZKh2qycX9gWmXa7nV69eoW+J7NcoZ7MBhOeAlcs7FwGa78JWTNCnaFoO8lQfZKh+iRDtVk5P+u1KEhCfQ5Dp/09mXsqatF1C9/153Di02HMLeby7EdDdl/zUGco2k4yVJ9kqD7JUG1Wzs96LQoSTdOYO3duyLqX0/Zf+OP2GJRUKzBX5sFOuBXiOkFJHsx5PCRNCHWGou0kQ/VJhuqTDNVm5fzCtsi02+0MHjw4ZJW/y2EnZf/tJS1/15/DiUqE898wl5e9A9t/DnoTQp2haDvJUH2SofokQ7VZOT/rtShI7HY7aWlpIQ0lNc4sMgtVmSvzYD1Phj5ngu6G/11lTm0URFbIULSNZKg+yVB9kqHarJyf9VoUJG63m5kzZ+J2u0PWhoa7/ijZkwnmfc2nvAMd+kBlAcz4U1B3b4UMRdtIhuqTDNUnGarNyvmFbZHpcDgYOXJkSG/D5C0yVbh/eXMiYuC8183llVOhZHPQdm2FDEXbSIbqkwzVJxmqzcr5hW2RabfbSUlJCWn3csNdf5S5f3lzMkdC7zPMuTN/+nvQdmuFDEXbSIbqkwzVJxmqzcr5Wa9FQeJ2u5k2bZo1hstVPSfzQCfdZX5d9g4s/mdQdmmFDEXbSIbqkwzVJxmqzcr5hW2R6XQ6GTt2LE6nM2Rt8F74o3pPJkD3E2HMreby9D/CjqUB36UVMhRtIxmqTzJUn2SoNivnF7ZFps1mIyEhAZvNFrI2pMY3XPij8DmZB5rwJAy5GDDg2zvBE9g5u6yQoWgbyVB9kqH6JEO1WTm/sC0y3W43X331VUi7lzvuHy5Xdgqjg9lscObTEJUEu1fB4jcDujsrZCjaRjJUn2SoPslQbVbOz2YYhiXuaVhRUUFiYiLl5eUkJCQEfH+GYVBbW0tUVFTIqv89FbWMenoOdhtsfGoiDrv1/go5KsvegW/ugIh4uHUxJHQOyG6skKFoG8lQfZKh+iRDtQU7v9bUa2HbkwmE/PyFhjv+6AaUqnhryeYMvxI6Hwv1lfDfK0H3BGxXoc5QtJ1kqD7JUH2Sodqsml/YFpmapjF9+vSQ3uvzwFtL7qmoDVk7/M5uh4v+A5EJsGMJ/PBcQHZjhQxF20iG6pMM1ScZqs3K+YX1cLmmaTidzpAOD0x+9UdW7ijnzStGcOagTiFrR0D8+h58fZu5fM1scz5NP7JKhuLoSYbqkwzVJxmqLdj5yXC5j6xQ9WemxACQX1Id4pYEwLFXwjGXmcvT7w3IsLkVMhRtIxmqTzJUn2SoNqvmF7ZFpqZpzJo1K+TBdNtfZG5vj0UmwOmPmsPmBcsh532/btoqGYqjJxmqTzJUn2SoNivnF7bD5Vbx0S/b+dMXqzilX0fevvr4UDcnMBa9BjMfhOgUuGEBJGWGukVCCCGEOAoyXO4DwzCoqKgg1DV2u+/JBDj+OkgbCDUl8Npo2LbIL5u1Sobi6EmG6pMM1ScZqs3K+YVtkalpGgsXLgx593JDkZlfWoOuW+8HxC8cLrhkKnTsD/X74OtbQWv7BPRWyVAcPclQfZKh+iRDtVk5v1YXmc899xx9+/alW7duDBkyhK+//tr7XE5ODqNHjyYrK4uBAwfy/fff+7Wx/uRyuTj77LNxuVwhbUdGUhQOu416TW8f9zBvTnIWXDML4tKheBPMe7rNm7RKhuLoSYbqkwzVJxmqzcr5tbrIHDVqFKtXr2b79u384x//4Le//S3FxcVUVlYyadIknnzySbZt28brr7/OlClT2L17dyDa3Wa6rlNSUoKu6yFth8thJyMxCmjnQ+YAUYkw8Xlz+ae/w6bZbdqcVTIUR08yVJ9kqD7JUG1Wzq/VReb48eO91fK4ceOIiYmhsLCQjz/+mJEjR3L66ad7Xzdu3Dg++eQT/7bYTzweD0uWLMHjCdzdaHzlPS+zuJ0XmQADJ8PQ3wIGfHwJ7Pz1qDdlpQzF0ZEM1ScZqk8yVJuV8zvqczJra2t56aWXGDlyJP3792fRokWceOKJTV4zatQoli9fftj319XVUVFR0eQf4P0meTyewy5rmtZkuaFyb27Z7XY3WT7wxNgJEybgdDq96w3D8N5g/sBlXdebLDec99DcssfjabJ8pGPqmmT2ZOaXVrfpmA4+jlAeU4s5nfksdBkBnnqMGQ/C/na19phcLhcTJkzw5hnSY2qPOQXhmBwOB6eddhoul6vdHFN7zKmlY7LZbJx55pnY7fZ2c0ztMaeWjsnlcnH66adjt9vbzTG1x5yaW3a5XJx22mne/IJ1TL5odZGZl5dHZmYmMTExTJ06lddeew2AgoIC0tPTm7w2LS2N4uLiw27nmWeeITEx0fsvM9Oc1iY3NxeAtWvXsnbtWgBWrlzJxo0bAfO8zy1btgCwePFi8vPzAcjOzqagoACABQsWUFRUBMDcuXMpKysDYNasWVRWVgIwffp0duzYQX19vfd2TLW1tUyfPh2AyspKZs2aBUBZWRlz584FoKioiAULFniPOTs7G4D8/HwWL14MwJYtW8jJyQFg48aNrFy5ssVjctSa7dteUt3mY6qtrW1yi6lQHVOLOZXVwMXv47FHYMv/GXI+OKpj0nWdbdu2WeOY2mNOQTimvXv3MnfuXHRdbzfH1B5zOtIx7d27l82bN7erY2qPOTV3TLqus2zZMtasWdNujqk95tTcMem6zqJFi9i8eXNQjqnhOHxiHKWamhrjww8/NNLS0owNGzYYp512mvHuu+82ec3rr79uTJ48+bDvr62tNcrLy73/8vPzDcAoKSkxDMMwNE0zNE07ZNntdjdZ9ng8LS7X19c3WdZ13TAMw6iurjZmz55t1NfXe9frum7U19cbhmE0WfZ4PE2W3W53i8uapjVZPtxxHLj8xbLtRtb93xoXvf5Tm47p4OMI5TH5kpM2/znDeCTBMB7vaNRvW9LqY3K73cbs2bON6upqyxxTe8wpkMdUV1dnzJ4927uP9nBM7TGnlo6ppqbGmDNnjlFbW9tujqk95tTSMTX8Lq2trW03x9Qec2pu+eD8An1MRUVFBmCUl5cbR9LmydivueYaMjIy2LBhA6NHj+buu+/2PvfEE0+wY8cO3nzzzSNuJ1wnYwfI2V7K+a9lk5EYxaIHTwt1c4JH12HqJbBhBiRlmRO1RyeFulVCCCGEaEZQJ2OPjIwkOjqaESNGeLt8G2RnZzNmzJi27iIgdF1n586dlrgaq9P+q8sLK+va71yZh2O3w/lvQFI3KNsGX90Crfibx0oZiqMjGapPMlSfZKg2K+fXqiJz586dfPzxx96TURcsWMAXX3zBlClTuOyyy5gzZ453rH769OmsXbuWKVOm+L/VfqDrOnl5eZYIJTUuEpsNNN2gpLo+1M0JruhkmPIuOCJg3bew6B8+v9VKGYqjIxmqTzJUn2SoNivn16rh8qKiIn73u9+xcuVK4uPj6d69O0899RSjR48GYObMmdxxxx2UlJTQu3dv3nzzTYYMGeLTtsN5uBzguCdnU7Svjm9vO4nBXRJD3ZzgW/xPmH6vuTz8cpj0MtgdoW2TEEIIIZpoTb3mbM2GU1NTmT27+Qm0zzzzTNatW9eaTYaMruvk5+eTmZnpvew/lNITIinaV8feylogDIvMkdfC3jWw9D+Q8wHkL4FLP4GUHs2+xWoZitaTDNUnGapPMlSblfOzVmuCyGrnMKQnmOdl7i5vx7eWbInNBue8COe+BhFxULQe/n0GlO9s9i1Wy1C0nmSoPslQfZKh2qycX5uvLveXcB8uf/Tr1byTvZVrT+rBn88ZGOrmhFZFAXxwIexdDf0mwiUfh7pFQgghhCDIV5eryuPxsGnTJsvchql/p3gA1u2uDHFLLCAhA6a8DTYHrJ8O2xYd9mVWy1C0nmSoPslQfZKh2qycX9gWmYZhUFpaikU6cumfYf41IEXmfh37wbFXmMuzHz3s1EZWy1C0nmSoPslQfZKh2qycnwyXW0RJVT3HPvE9AJue+g1OR9jW/40qdsHLw0GrhdG3wFlPh7pFQgghRFiT4XIfeDwe1q1bZ5nu5djIxul6qt3WaFPIJXSGU/9sLv/8D/jyFqgp8z5ttQxF60mG6pMM1ScZqs3K+YVtkQlQU1MT6iZ4RTjsOO02AKrrrPeDEjIn3GZObwSw/AP49A9NnrZShuLoSIbqkwzVJxmqzar5yXC5hQx5dCaVtRpz7hlPr45xoW6OddRXwZvjoXij+XjcfXDqQ6FtkxBCCBGGZLjcBx6Ph9zcXEt1L8dGmHPj19Rbp02WEBELty2F0x4xHy94DtZNs2SGonUkQ/VJhuqTDNVm5fzCtsi0opj952V+tbz5CcjD2ti7zeFzgOl/hDq5El8IIYSwKhkut5BzXllI7s4KAH68/xS6JseEuEUWVF8Nr42Gsm3mRO0Xvw+OVt0dVQghhBBHSYbLfeDxeMjJybFU97LngDtClVTVh64hVhYRA+e/Cc4oWD+dsn9OxlNTHupWiaNkxc+haB3JUH2SodqsnF/YFpkA0dHRoW5CE1V1mnfZo1uig9massbAlHcxHJEk7f4J+/vnQXVJqFsljpLVPoei9SRD9UmGarNqfmFbZDocDvr374/D4Tjyi4Nk3wFFZrVc/NOyfmdhu+pbiE7BtutXeP888yp0oRQrfg5F60iG6pMM1Wbl/MK2yNQ0jSVLlqBp2pFfHCQHFpkH9mqKw9MyjmXVyOcwYlKhYAV8efNhbz8prMuKn0PROpKh+iRDtVk5v7AtMm02G8nJydhstlA3xateazwpc+HGohC2RA02m43obsPRL34P7C5Y8yUseyfUzRKtYMXPoWgdyVB9kqHarJyfXF1uIY98lcu7i7Z5H3920xhGZKWEsEUKmfMELHzeXB5+BZzyECRkhLZNQgghRDsjV5f7QNM0srOzLdW9/ODEASTFuLyPZ67eE8LWWF+TDE/5E4y8znwi53344ELQ9ZY3IELOip9D0TqSofokQ7VZOb+wLTLtdjtdunTBbrfOtyDK5eDMgZ28j+XOPy1rkqHdARP/Cr/7GJzRsHc1/Pg3OUfT4qz4ORStIxmqTzJUm5Xzs16LgsRut5OVlWW5UKrqG/8Sqah1h7Al1ndIhjYb9J8IZzxuPp77BHx1qxSaFmbVz6HwnWSoPslQbVbOz3otChJN01iwYIHlupcPvKp8S5FMydOSZjM8/jqY8CTYHLD8A1g3LTQNFEdk1c+h8J1kqD7JUG1Wzi9si0y73U6vXr0sV/lX1TUOka/aWc7u8toQtsbams3QZjPvcX7i7ebjr26G5R/JOZoWZNXPofCdZKg+yVBtVs7Pei0KEquewzC2T6p32TBg2qqCELbG2o6Y4fj7IXM01JbDlzfBjAeC20BxRFb9HArfSYbqkwzVZuX8rNeiINE0jblz51que/m6cT159oIh3DCuJwDfrNgV4hZZ1xEzdEXD77+Gkx80Hy9+E5b8K3gNFEdk1c+h8J1kqD7JUG1Wzi9si0y73c7gwYMtV/lHuRz87vhu/P6E7oA5ZH7gJO2ikU8ZOiPh5Adg6G/Nx9PugSX/Dk4DxRFZ9XMofCcZqk8yVJuV87Nei4LEbreTlpZmyVAAMhKjiI1w4NENtpdUh7o5ltSqDCe/Asdfby5PuwdWfxHYxgmfWP1zKI5MMlSfZKg2K+dnvRYFidvtZubMmbjd1pwmyGaz0bNjHACbC/eFuDXW1KoMnZHwm+dg5LWAAV/cBEWbAt5G0TKrfw7FkUmG6pMM1Wbl/MK2yHQ4HIwcORKHwxHqpjSrR2osAJtlKqPDanWGNptZaPYYD1oNfHYNVMk94kNJhc+haJlkqD7JUG1Wzi9si0y73U5KSoolu5cbpCdEAlBSVR/illjTUWVod8DZL0BEPBQsh/fPh5qyQDVRHIEKn0PRMslQfZKh2qycn/VaFCRut5tp06ZZsnu5QWykE4B9dda7YswKjjrD1D5w3RyISYXdK+Ht30D5jsA0UrRIhc+haJlkqD7JUG1Wzi9si0yn08nYsWNxOp2hbkqz4vYXmRU1bgy5NeIh2pRhx35w5VcQ1wn2roH/nAW7c+UWlEGmwudQtEwyVJ9kqDYr5xe2RabNZiMhIQGbzRbqpjSroSfz25UFvJu9NbSNsaA2Z9hpMFw7Gzr0gfJ8eONE+NtAyJvn34aKZqnwORQtkwzVJxmqzcr5hW2R6Xa7+eqrryzZvdygocgEePSbNSFsiTX5JcOkTLj6O+hzpvm4chf89/dQIZPgB4MKn0PRMslQfZKh2qycn82wyDhsRUUFiYmJlJeXk5CQEPD9GYZBbW0tUVFRlqz+Aeau28Mf3lnqfbzowVPJSIwOYYusxe8Z1pTBe5OhYAV0HQlXTQdnRNu3K5qlwudQtEwyVJ9kqLZg59eaei1sezIBS56/cKDYiKbtW7hRpts5mF8zjE6CC/8NUUmwYwk82RF+fc9/2xeHZfXPoTgyyVB9kqHarJpf2BaZmqYxffp0S97rs8GBw+UA89fvDVFLrCkgGab2gdMebnz89W3w1smwZ7X/9iG8VPgcipZJhuqTDNVm5fzCerhc0zScTqdlhwe2FlVx8vPzvY+ddhs/3n8qnRKjQtcoCwlYhoYBG2dB7mew8r/A/o/IBf+CoVP8tx+hxOdQtEwyVJ9kqLZg5yfD5T6yYtV/oAN7Ml0OG5pu8NMmGTI/UEAytNmg75lwwVtwew5Ep5jrv7oF1k3z//7CnNU/h+LIJEP1SYZqs2p+YVtkaprGrFmzLBsMNM6TCTC8WzIAO0prQtUcywlKhik94NYlkD4EPHUw9VJ4+2zIXxy4fYYRFT6HomWSofokQ7VZOb+wHS5XxR1Tc6iq8zCsayIvfL+Bi0Z05fkpw0LdrPDjccM3d8DyDxvXTXwejr8udG0SQgghgkyGy31gGAYVFRWWv5PO3383nH/9/jgyU2IA+HTZDqrkNpNAkDN0uOC81+C2X2Ho78x10++FuU+aBag4Kqp8DkXzJEP1SYZqs3J+YVtkaprGwoULLdm9fDgNRSbAw1/mhrAl1hGSDDv0MovN1H7m4wV/hTfGwrd3m7elFK2i2udQHEoyVJ9kqDYr5yfD5YrQdYMbP1jGrDV7APj+rnH0SY8PcavCWE0prP4CvrsfPPXmuoh4uG0ZxKeHtm1CCCFEgMhwuQ90XaekpARd10PdFJ/Y7TbeuvI4zhxkFjAf/rI9xC0KvZBmGJ0Mx/3BvPd515HmuvpKmPGAOQWS8Ilqn0NxKMlQfZKh2qycX9gWmR6PhyVLluDxeELdlFY5f3hXABZuLAxxS0LPEhlmDDMLzbOeNR+v/hy+uBGqikPXJoVYIkPRJpKh+iRDtVk5PxkuV0x5tZvhT8xCN2DJQ6fTMT4y1E0SAO4amP0Y/PJ647q7VkNi19C1SQghhPAzGS73ga7r7N2715Ldyy1JjHGRFBMBwM6y8J4z01IZuqLhN89Cz5Mb1704CArXg1YfsmZZnaUyFEdFMlSfZKg2K+cX1kVmbm6uJUM5kkinGdt5//iJTXsrQ9ya0LFkhhf+G/qd3fj4H8fDM11gVw4U58l0RwexZIaiVSRD9UmGarNyfjJcrqDxf53HtuJqAI7LSubTm04IcYvEIYrz4NOroWBF0/XpQ+Ci/0DHvqFplxBCCNEGMlzuA13X2blzpyUr/yOJcDTGtnRbaQhbElqWzrBDL7j+Bzj3tabr96yCf4yED6fAuungsd68ZsFk6QyFTyRD9UmGarNyfmFdZObl5VkylCOJdIVtbE1YPkObDY65FM5+AfpNhNMegZhU87mNs2DqJfBEB/jsOtCtd1VgMFg+Q3FEkqH6JEO1WTk/GS5X0AWv/cSv28u8jycN68wrlwwPXYOE7/KXwKbvYc1XULiucf24++DUh0LXLiGEEMIHMlzuA13X2bZtmyUr/yOJcDaN7ZsVu9i4J/wuAFIyw8yRcMqf4MqvYOC5jesXPAc/PNf0SnRr/P0XUEpmKJqQDNUnGarNyvmFdZFp1XMYjiTC6Thk3Z++WIVFOqWDRuUMie8EF78H926ChC7munlPwb9Og3174evb4OnOkPsZlOWHtq0BpHSGApAM2wPJUG1Wzk+GyxV09duLmbf+0Dv+zLv3ZHqkxoagRaJNDAN++jssfAHqKg593hVr3lUotQ84XMFvnxBCCLGfDJf7wOPxsGnTJkvehulI9Gb+LPhs2Y7gNiTEVM6wCZsNTroTxtxy+OfdVfD6GHi6C7wyArYsAAv+xXo02k2GYUwyVJ9kqDYr5xe2RaZhGJSWlio/xDzjzrGM7WNesfzqvE0szy8LbYOCqL1k6DX2Hjj/Lbjya/hzIZz1fzDwPLDtPz3CUwfFm+DdSfB/3eHr20PZWr9odxmGIclQfZKh2qycnwyXK+jK/yxmwQZzuHzrs2c3GT7/89kDuHZsz1A2T/hbRQF8eydsmHHoc+e/CV1GQHm+eS7n0N+aPaNCCCFEAMhwuQ88Hg/r1q2zZPfykRz8d4F2wPj59pLqYDcnZFTOsFUSMuB3H8FvnoMRV0Hf3zQ+98UN8Opx8P755vLKT0LWzKMRNhm2Y5Kh+iRDtVk5P2eoGxBKNTU1oW6CXzz4mwEs3LgQgPcWbaNXxzh+f0L30DYqSNpLhkdkd8CoGxof11bAWydDSV7T131xA6z6H3ToY57L2f8c6DEeXFGwYiqs+BjG3gvdT4LyHVC0AXqfZr63ZAskdg36xUVhk2E7JhmqTzJUm1Xzk+FyBV3+r1/4cVMRYA6XA+wqq+GcV36kpMqcZ/GT60czqmeHkLVRBEHpVlj+MaQNgOoi+OGvsG/34V8b0wGqixsfH3895M2D4o3Qebh5Bfu2H2HMrXDmU0FpvhBCCPXIcLkPPB4Pubm5luxePhKDQ/8u6JwUzZKHTmfikE4AfJfbTLHRjqicoV8kd4dTHoRB58HIa+GaWeatKweed+hrDywwARa/ZRaYALtyzAITYNGr8MVNsPpL897qAf4bNOwzbAckQ/VJhmqzcn5hW2SqrLn/9x12G6f2TwfgneytbCuuCmKrRMglZ8HYu+Hid+GOFc2/Lm1Qy9tZ8RH87/fmvdU/u9acDH76fbDk34e+1qOBVte2dgshhGiXZLhcQZe89TOLNps9Uw3D5Q1yd5Zzzitmr1Sk086Xt5zIgAz5foal5R/Bz6/BxOehZDOs/gKOuRT6T4J3JkL+L2B3mUWpKwY2zTZ7MlsSEQfHX2eev5naB/IXw+6VcMMCSOpmvqZ8J+xdA33OCPwxCiGECKrW1GthW2R6PB5WrlzJ0KFDcTgOvU2jlf3urUX8vLkEOLTIrHV7OOHZud5zM8f2SeX9a0YFvY3BoHKGIafVmxf9dOhtXhTUwDBA90B9JXx5C6yf5tv2jr/BvICotgKm3QN15XDFl9DrFPN5XTenVjpoeiXJUH2SofokQ7UFOz85J9NH0dHRoW7CUTl7aGcAuqXEHPJclMvBN7edxEfXjcLlsLFwY5F3Ts32SNUMQ84ZAZ0GNy0wwSwCHU6IToZLPoLbc8xJ4odc3PL2Fr8JH10Mn19rFpgA758HW38ye1Sf7gyPJcGqT83nPJpZ0JZupcfKF7Atezvg53+KwJHPofokQ7VZNb9W92TOnTuXhx9+mD179mAYBnfeeSe33XYbAFu3buW6665jw4YNuFwuHn30US6//HKftivD5b7z6AYLNxYyrGsSybERzb7u8W/W8J+ftjC4SwJf33ISdrtM0i3aoL4KImJh5zLz8cbvzZ7QH1+EPbm+byd9sDl8332sefFRyWZz/ckPQsYwiE2DriOOvJ2aUohKksnnhRAiiAI6XH7HHXdw8803069fPzZv3sy4ceP417/+xRlnnMExxxzDPffcw1VXXcWaNWs46aSTmDt3Lsccc4xfG+0PmqaRk5PD8OHDcTrb53ShJVX1jHxqNh7dIDnGxa8Pn4GtHf2HHA4ZKqFkC+R+as676dHMyeMX/LVt28w6Ec57zbyCfstC8/zRY6+EghWQ0BkqC+CDC83zQM/+m5z/GULyOVSfZKi2YOfXmnqt1a35+9//7l3u2bMnF198MXPnzsVut+N0OrnqqqsAGDhwIJdffjnvvvuuT0VmsNlsNpKTk9tV0XWwlNgIfjO4E9+uLKC02s3agkoGdm4/vcThkKESUnrAuD82XTfqRpj7JCx723w86WUYfAG8OQ5qy837sVftxUjuzvaxL9Jtw7+xrfu28f3bfoK/D4OO/aFwnblu7hOH7rtsuzkB/fXzwRkFm38wb7959gsQlWj2clbsMtcPvdic1F74lXwO1ScZqs3K+bX5wp8rrriC/v37o2kahYWFvPpq49WpH374If/617+YN2/eIe+rq6ujrq5x6pOKigoyMzMpKSkhOTnZO9+Tw+FosqxpGjabzbtst9ux2+3NLrvdbhwOh3fZ6XRis9m8y2D+FXDgssvlwjAM77Ku63g8Hu+yrus4nc5mlz0eD4ZheJcPdxzBOqaaeg8DH5np/T4v/OM4MjvEK31M7TGndnlMhoHx399jrynGuOxTNJsLl+FGtznwYMdVvg09pgN6RDxOQ0P/5U3YsRj7gcXmUdJPvAv7jiUY+3aD3YmtcB2e8Q9iG39f4zEZHuwOF+59RThcEdijk8IzJzkmOSY5JjmmVhxTSUkJqampgb/wZ/HixXz77bdceumlFBQUkJ6e3uT5tLQ0iouLD/veZ555hsTERO+/zMxMAHJzzXO71q5dy9q1awFYuXIlGzeaE0fn5OSwZcsW7/7z8/MByM7OpqCgAIAFCxZQVGTeEWfu3LmUlZUBMGvWLCorKwGYPn06P/74I7W1tUyfPh1N07zLAJWVlcyaNQuAsrIy5s6dC0BRURELFiwAoKCggOzsbADy8/NZvHgxAFu2bCEnJweAjRs3snLlyqAdU21tLZqmeY/JpruJczb+HfF//12g/DEdmNnChQvbRU7t8Wevtq6Ob6IvgqunU1mrmccUEUNZZZV5TKm92VNey4wZM9BsTgp6XMSPnW+AR8qo7TiMBjUdj2FnnyuhQ28MW+OvrB0jHqSoz285HPtPL8K2H7EVb8K2vyfU8cMzlM97Gdw1rP/f49ifSoPHk3H9rQ+2F/pB0aZDj6k4H60o77A51Sz9kL2vT4Z9hWrn1MafvV9++YXs7Gzy8vLazTG1x5xaOiZN05gzZw6rV69uN8fUHnNq7pg0TeP7778nLy8vKMfUcBy+OOqezKlTp3LnnXfy1ltvMXnyZK699lq6devGX/7yF+9rZsyYwf3338+KFYdODB3qnsy6ujoKCgrIzMz0/uUA7fMvkB4PTm/yvf/w2lGM6Zmi9DFpmobD4WD79u1kZGQQGRmpfE7t8WfvSMekaRrbt2+ne/fuAI3HVFuJXlOGM64jHpsTAxqPKf9nHFEJeDoOxLb6c+yfX3vI75ejpfc8DVtcB2z5S/Acczn2JW9h27cHffBF2PYPwWt11bi+uRVWf26+6axn0ZN7YOxYimPcPeiOyEOzKduKXrcPPX1w05yq9uCJSgJnlKVzaulnT9M0CgoK6Ny5MzabTZmfvfb4eTraY7LZbGzbto2uXbvicrnaxTG1x5yaO6aD87NST2ari0yPx8Ntt93GvHnzmDp1KsOGmT0O9913H7W1tbz88sve177//vt88MEHzJw5s7nNecnV5YHz9PS1vLVgs/exy2Hj0xtPYFhmUugaJYQ/1FbAP0+Fjv1g2CVQusWcbmnJvyCtvzkx/OxHzCvWm7uve2uMuhFSesJ39zWuGzAJ1n5jLg++EC76j7mc+xn88hb0+415BX5tGdyUDen777iUN8+8eGnIRXDBW+a6+irz3FI5d1QIYVEBnSfzzjvvZPPmzSxdutRbYAKMGDHC2+XbIDs7mzFjxrR2F0GhaRoLFixA07RQNyXg7pnQlzcuP5YPrx1FZko0bo/B/Z+tpLpe7WMPpwzbqzZnGJUAty2F330IA86BE26D+HQ49SGz4Dvxdrh7Hdy5CiY1XrRInwlwyxLz4qDW+OWNpgUmNBaYYBaWqz41i8dP/wD5P5tFbm2Z+fyXN4G7pnHZ8MDKT+CbO8x7xT+TCf84HqqKYOZD8Ew3WPiCOUH+BxeZ29U9sG4a7F0HdZVQXdK4/y0L4ZMrzDsxBYl8DtUnGarNyvm1qieztraWuLg48vPzycjIaPJcdXU1vXv35rnnnuPyyy9n6dKlTJ48mcWLF9O1a9cjbjvYPZm6rlNQUEBGRgZ2e/jMSV+8r44JLy6guKqeP57Zj1tO6R3qJh21cM2wPQlqhoYBm+eb83TGdAC73bzyvLoE5j0F66dDfGcYeK55Jfo/T2l5e6NvNm/beTSG/tYsLn0VnQI1+4vJ466Bpf+GuHRz3tLacrh1KeTNhc+uMV/TbyKc/hgs/Q+cfD/s/NW89/z6aTDyOjjpTvPYM48/uvYfoNkMq0vMqaxGXgsderV5PyJw5Hep2oKdX8DmyVyzZg2DBw+mW7duTdb369ePmTNnsmzZMq677jp27NhBp06dePnllzn55JP93mjRNv/5cQuPf7umXd9yUohW8WhQtB7SBjZO7r7yv2YhB1C8CYZfDvOeNouzsfeYE9H/7/dmb+jpj5k9k5u+b34fCV2gYueh6wdd0Hh+Z7A1DO17NBh2wEVUyz+GvavhpLth2TuQmAlDpzR9745lUFUI/c46/LanXgYNMwWc8xKMuEomzheiHZB7l/ugoXt53Lhx3hNbw8WaXRVMfHkhAF2Sopl+x1gSo10hblXrhXOG7UW7yrByDyx61ZwwfvCFMP9ZszBd9ja4ouG6efDXg3r00gbCzYvgq1sh5/3QtLvBWc+aPaB7VsPUS8x1MalQbV6Nyp25EN8JHC7znNdp9wDgOe8tfijp2DTDXcvhrfFNtz/xeTj+OnOIf1cOpPSCxC6HtqN0qzmxf7cxcm5qkLSrz2EYCnZ+UmT6QNd1ioqKSE1NDbvhAY9uMOrp2RTtqwfgbxcP44Jjj3xKg9WEc4btRVhkWFsONjtExsP2X8zzNodMgfnPwEl3QY+x5jmYP70EkYkw78nG9147B7b8AK4Yc0L5cX807zev1TUO58d0gIg481zP2jLw1AfuWGwOuOjf8L+rmqx2dxyE4/S/YLc7oMsIeHl443moB+o2BrYvMpc7D4crvgRDh5iUxte8MsLsOT7uD3DOi761q2y72aYxt5gFvmiVsPgctmPBzk+KTHFEuTvLOeeVHwEYkZXMPy49lk6JUSFulRBhzqPBu5NgezZc+l/oe+bhX2cYkP0KRMaZxdiB9u2F6mKzF/L5g865vn4+zHnCLFbXfn3055T6W2QC1FVAYjco326ui0uHe9abyw3D7Gu+ghl/ggv/tb+3c5RZZP/vKthozkfIo+Xm19py819S09O7qCk199fQS1pfDc5I8y5Tdfug2+imRa+vtv4EiV0hOav1722OrgOG9Xt03bWwcSb0GA/RSaFujQgwKTJ94Ha7mTt3Lqeeeioul3pDxf6wZlcFk1/9EU03fwT+78Ih/HZktyO8yzokQ/VJhofhcUPRhqbnhx6tddPNqZuWvQujb4JhvztoX5q5r83zYPvPZqEXm2b2LpZshl2/Nr/t0/4Ccx5vef9j74WFzzddl9TN7Hn0RXSyWRSe8mezPSs+OvJ7Tv2zef7rlzeB3QW3/2ruc8dSeP988xiTu8MfZprb/uep4K5uuo2710HCARe3FqyAn98w1w2YDJ2GNBZ+u1eZF1gt/Y/Z3vu3+nZsADuXmee7xqSaF6Id7Nu7YcXHcPPP/i1eD9Lmz+H8Z82e+d6nw+Wf+b+BokXB/j0qRaYPdF2nrKyMpKSksB4eWLWjnEmv/uh9PO/ek+mRGhvCFvlOMlSfZGhh1SXwze3Q+wwY8XuzMNw0Gxb/CwadD+P/CHtWoyd1p7ykkMRtM7BHxsH676CmDMbdC71OMc+/jM+AtyeaU05dOweKNprnoC5qvA0xUYlmz2MgDL7QPE3hQGNuNXtyDb359w2ZAhf8Ex5LBg74r3LsPWbhu/hNmPFA0/c8XAy7V8DuXPP0gOyX4ZSHQKs1vw9F6+G7B8wpt2b+qel7f/8N9BhnLnvc8ESquTzuj2bx7NHAcdA5d7puFs4NPYg//d3M6LirYezdzR9byRazN3vktejO6LZ9Dl/oD5XmXWW8PcmHU19lFqR9z4LuJ5rrDMPMwOq9tRYW7N+jUmSKVrn7k+V8nmNe9Tq8WxJTrx9NpFM+8EIIP9L3F3MH/if43QPwy+uQ2hdu/Ame7Nj4XP9zGq9OD6WL/mPOeeqrW5fCq8cd/rlOQ2H3ypbf/0C+OR/qjgPmOj3hdrMwe2+yWRyf9giUbYOUHvDVLbBiKkx62SxQXxpsvic2DW75xSz+vrzZPG/3pDth71rzBgBvTzRvXjDqJvjNs0c+rp/fMKfAmvKueTrB8o/MUwy6jTHb0OBPBeZFYdFJcNYzTbfx9W3w63tmW/60f6aFb+6A3M/hxoVmD3NLijaZF54FsFdXHJkUmT5wu93MmjWLCRMmhP0wXXm1mxdnb+DdRVsxDHh00kCuOrFHqJt1RJKh+iRD9bUpQ3eNeaX6oAvMK83z5sK0e+HMp82pkZ7vC/v2mK/tPhau+MKcRurHvzVuY/TNkNwDup8Ecx4zr4CPzzDft/Q/ja8bdx+Mvx8K18IbJzWud0bDsVeaV737MhxvBYMvgtxPzeP54f+af50j4vAXgh37e/j1Xe9DvdsYVtsGMCimGHtKdzhj/2kQug7fP2ye/7rmS3Pd6FvgrKfh0WZuZHDqwzD3CXP57rWQ0Nlcrq2AZzMbX/fgTnPWhcf3n/868Dy4+F3zLl21ZY13xgLzwrgdS+Dj/ad79DoVLv3fob26YJ72YXNA5sjmvy9Ha+tP5vfiwOm+LCDYv0elyPSBYRhUVlYSHx+PTeZuA+D9RVt5+KvVpMZFsOC+U4iJsPZUFpKh+iRD9QU0w4IV5p2Phl9uDnk7XGaxsvQ/5tD73rVwxhPmFfcHK98J/54AFTvg3H+Y22gw53HzTkoAtyw2b0uq62bPnN0JGUPNCew/uMC3q/UnPm8OPW9Z0PxrbPaWh+aDyRVz6HmoBxp1I2yYafZ0Hqzr8XD5p/BsM+fv2xzmnazAPEfTEQlVe80i8UBnPWuewnDg+glPmtnoHrh+HmQMM8/FfWMc1Fceui9HJFzwpnn6xq7lsOp/jadg/GmXebOCA5XvgKmXmts99irzlA1npPkz5Iww76K1d/XhZygwDHgsyVy+4J9moRubevjvQZAF+/eoFJniqLg9Oqc8P58dpTU8e8EQzhvehSiXDJsLIdqh1V+YV5Mfe0Xzr3HXmnctarh4qe9ZZtFTtMF8bHfBVdPMq9z3roW3f2Pee/6cF82va7+BniebxUjnY2HWQ2aBk9IT3jnb3Ebn4eZFVB0HwPvnQeE687Un3GZOe/XhRUc+lq7HNw6vj7m16bmuVpPS0/we+mLi8/Dd/Y1Fa3P+XNj0VAuAi9+DbifAnlXQ8xTzXOGG+V8PNullGH4FPJ5sPh51o3njhC4j4MQ7zQvwqorhrz0b39Oxv3k6woF0jzkvbm2FOT1ZO/3DWYpMH7jdbqZPn87EiRNlmO4Aj369mneytwKQkRjFx9eNprtFLwSSDNUnGaovLDJ015i9ZjabWWCWboP4dPN2n0kHDAHX7TN7W52RLW/Po8ETHczl899svOpf183iJj7DHAo2DLPXduUnkP+LeVpBSZ7Zw3vcH6D/2dDjZNA1eHPc/kn/55q9jPX7IG2QWaANugDSBphF7sFX9jsiMKISsVUVmo/TBsLeNU1f0/uMlu9m1XGA2QO8Z1Xzr8k6ybzY6fjrzXNWDyw0A10Yn/03mNbCRVCnP2ZOXfWfw0wZ9tsPzJkJcj5ovDFBgxt/gk6DGx8v+CvM3T/P7Qm3Q8Fy8zzcUx6CiBjf21tXaZ632lyRWrHLvHDr0z/ACbfhHnlDUD+DUmT6wDAMamtriYqKkmG6A3y3qoCbPmyctiSrQwzf3zWeCKf1rvyVDNUnGapPMjxKqz41z0E95yVzqLYl7hpYN828GErXYE8uZI5qWoQYRuPjghXmbVFPftCcS/VAOR+a0yJVF8Mxl8HomzB0Dc+Mh3H0Goet/9nmELahm3Oxdh5uXk1fVQQbZsDXtzbd3rBL4Pw3mg4ngzn0nfu52d7f/B9kndD43AF3jOKKL8xC+fUxZi8uQNaJ5qkLBxr/gNkL3FxvZCjEdDAvwrLZYNE/Gtt/sIHnmtkNmWIW+R9OMWcZGP8AnPKg+Rp9f2/tqv/BFzc0/ePjQEUb4Y2xoNV4Vxn3rKfWmRi0z6AUmT4wDANN03A6nfKL8QCb9lZy+t+anlc0/faxDOxsvVMYJEP1SYbqkwzV16oM964zz3WM7Wier+qKNntvAZa+Das/h/PeOPwtQxto9fDxb81zQy9+35xxwF0DT3Uyn79ngzlH68e/M/dz+/LGYjn7FZj1Z7Onb+C5sPzDQ7ffcYB5gVfWSeaNDRrOhY3tCFd/Z14QldDFnF5qzmPNt9MZ3aSYa7OJz5t/LGye17jupkVmz/G3d5vfh5rSxuceLTensgLYPB82fm9Om3UQY+R1aBOeCdpnUIpMH4TFEM9RqKrTGPTIzCbrsjrE8MS5gxnXt2Mz7woNyVB9kqH6JEP1WSbDXTlmsdnQ61mxf+7NAyfG92iw/ANzCD+hMxSuN4tcRwQs/Tek9oNjLjHPp3VFmT1/v75nzsM65tamF4mt+AS+uL5pGxp6UUdcbRalC54z1183F5a9A9uyzd7brBPMc3APdvz1sPgt/3w/up1gXoiU3MMcem/B4u63MvyyR2W4vDnSk2kd3R+Ydtj1VuvRlAzVJxmqTzJUX9hmuG560+H3Yy6H8/7R+Li+yuw17Xc29Dm96XsNAz65HLYvMk89ABh2qXkR2Dd3BLbdmaPMc3TBO/eqEZ0CNy/CFt8psPumdfWa9U60CyJN00LdBKV8vWJXqJtwCMlQfZKh+iRD9YVlhr1ONXtEx91nXo1+9gtNn4+INWcKOLjABPM8zN99CPdtNs8/Tehi3uVq6G9h5LXgOuiCWdsBM7VEJ5tD4af9pelruuyfxL/72Jbbff7+IXO7Cy75GKPTUDx9zzZPP7CYsC0yNU1j1qxZ4fnB8tGAjKZ/obzxQx7XvbcUt8cac71JhuqTDNUnGaovbDN0RZlzfp76kHl+5+HmW/XFCbfB3WugQy/zHNWzX4B71sFln8Gf95q3C71/K1wy1Zw5YMr+ifDH3tNYaA671Dxf9KZs8/WDLzSL0duXmzMJdB5uvu7EO807Pd32K1wzExK7ol35LdPsZ6A5otv4DfG/sB0uF837dXspUxdv5/6z+vP4t2uYkbsbTTfw6OaPypPnDeby0XJbLyGEEKJNPJo5y0CPsWaB2sAwzCvOD3dXoxCTczJ9IHca8U3Dj0fRvnqu/M9i1hZUMKZnBz6+fnSIWyYZtgeSofokQ/VJhmqz8h1/wnq4fOHCheE3PNBKNpsNm81Gx/hIXrnE7K5ftLmYv85sZj6wIJIM1ScZqk8yVJ9kqDYr5xe2PZmi9QzDYOxz89hRas4b5rDb+P6ucfTsGHeEdwohhBCiPZCeTB/ouk5JSQm6bo2LWFRgs9n46NrRdO9gXsHm0Q2emrY2ZO2RDNUnGapPMlSfZKg2K+cXtkWmx+NhyZIleDyeUDdFKd06xPDpTScQ7TKnY5izbi8DHp7Bqh3lQW+LZKg+yVB9kqH6JEO1WTk/GS4XR+2695by/Zo9APTvFM93d4yVk8aFEEKIdkyGy32g6zp79+61ZPeyKv5yzkBO658GwLrdlTw1bS1aEOfQlAzVJxmqTzJUn2SoNivnF9ZFZm5uriVDUUVmSgz/vmokfz57AAD/+nELd/13RdD2LxmqTzJUn2SoPslQbVbOT4bLhV9MW1nArR//imHAzw+eRqfEo7xzghBCCCEsS4bLfaDrOjt37rRk5a+is4dmMKxrEgA/bNgblH1KhuqTDNUnGapPMlSblfML6yIzLy/PkqGoalTPFADu/2wV1723NOBXnEuG6pMM1ScZqk8yVJuV85PhcuE3H/6yjYe+yG2y7stbTuSYzKTQNEgIIYQQfiXD5T7QdZ1t27ZZsvJXVWZyzCHrzvvHT0xfVRCQ/UmG6pMM1ScZqk8yVJuV8wvrItOq5zCoKjPl0CIT4OYPf+WqtxdTUlXv1/1JhuqTDNUnGapPMlSblfOT4XLhN3Wah35/ngHARSO68mXOTjS98cfr9AHp/Ov3x4WqeUIIIYRoIxku94HH42HTpk2WvA2TqiKdDv555XG8fMlwnp8yjI1P/YZNT/2GfunxAMxfv5fqes1v+5MM1ScZqk8yVJ9kqDYr5xe2RaZhGJSWlmKRjtx244yB6Uwe1hkAm82G02Fnxp1j6ZocjaYbDPzLTN5akOeXfUmG6pMM1ScZqk8yVJuV85PhchEU7/+8jYe/bLzy/MoxWTwyaRAOu9zrXAghhFCFDJf7wOPxsG7dOkt2L7dHl4/qxp2n9/E+fm/RNv5vxro2bVMyVJ9kqD7JUH2SodqsnF/YFpkANTU1oW5C2LDZbNx5el++ve0k4iOdALy3aCt7K2vbtF3JUH2SofokQ/VJhmqzan4yXC6CzjAMJr/6E6t2mncEuuv0vtxxQC+nEEIIIaxJhst94PF4yM3NtWT3cntns9m46eRe3scvzt7AzNW78eit+3tHMlSfZKg+yVB9kqHarJxf2BaZIrQmDsng/WuOp2tyNAA3vL+MKW9k+3WKIyGEEEKEjgyXi5DatHcfF72RTVm1G4BuKTH878YxpCdEhbhlQgghhDiYDJf7wOPxkJOTY8nu5XDSOy2OX/98Bree0huA7SXVnPbCD6zZVXHE90qG6pMM1ScZqk8yVJuV8wvbIhMgOjo61E0QgN1u494z+zHv3pMB2Fen8dr8TWTnFR1xclnJUH2SofokQ/VJhmqzan4yXC4s5aXZG3hp9kbv45N6p/LGFSOI2z/tkRBCCCFCR4bLfaBpGkuWLEHT5EITK+nVMa7J4x83FfHCrPWHfa1kqD7JUH2SofokQ7VZOb+wLTJtNhvJycnYbHJbQyvplhLjXR7TswMAb/+0lYe/zGXVjvImr5UM1ScZqk8yVJ9kqDYr5yfD5cJSymvcDHtsFgC/PnwG5/7jR/JLzDsZRDjt/PrwGTJ0LoQQQoSIDJf7QNM0srOzLdm9HM4So118dcuJTL99LCmxETz4mwEMy0wCoF7T+fDnbXyZs5Nat0cybAckQ/VJhuqTDNVm5fzCtkvIbrfTpUsX7PawrbMtq6GoBHPS9olDMjjzxQWs31PJM9+tA6DLzGhev2y4ZKg4+RyqTzJUn2SoNivnZ70WBYndbicrK8uSoYhDdYiLaPJ4Z1kNd3yygowumZKhwuRzqD7JUH2SodqsnJ/1WhQkmqaxYMECS3Yvi0PVuBsnmV3259NJjYtkS1EVz0ydKxkqTD6H6pMM1ScZqs3K+YVtkWm32+nVq5clK39xqPL9t50E6BAXyc0n9wLg7dw67v88l01794WqaaIN5HOoPslQfZKh2qycn/VaFCRWPodBHOrm/bedPO+YzgBcOqobmSnmHQ4++3Unk175ka+W7wxZ+8TRkc+h+iRD9UmGarNyftZrUZBomsbcuTLUqooLj+3Ct7edxLMXDgUgyuXg0xtGc8UAFyO7J1Pj9nDH1OXc/OEy6jTr3b9VHJ58DtUnGapPMlSblfML23kydV2nqKiI1NRUS1b/4sgaMkxO6cDfZm/k9fl5AJzSryMv/W44idGuELdQHIl8DtUnGapPMlRbsPNrTb0WtkWmaH+mrSzg9qk5eHSDfunxvPuH4+mUGBXqZgkhhBDthkzG7gO3283MmTNxu91HfrGwpIMzPHtoBl/cfAJp8ZGs31PJpf/8mW9X7qKyVjK2Kvkcqk8yVJ9kqDYr5xe2PZm6rlNWVkZSUpIMDyiquQzzS6o555UfKa9p/MB9dtMJjMhKDkUzRQvkc6g+yVB9kqHagp2f9GT6wG63k5KSIh8ohTWXYWZKDHed3qfJusv/9Qv/W5qPRf6mEvvJ51B9kqH6JEO1WTk/67UoSNxuN9OmTbNk97LwTUsZXnViDz68dhR/OLEH8ZFOatwe/vjpSi56YxG6LoWmVcjnUH2SofokQ7VZOb+wHS43DIPKykri4+Ox2WwB35/wP18z1Dw6L8/ZyMtzN3nXjevbkVcukSvQQ00+h+qTDNUnGaot2PnJcLkPbDYbCQkJ8oFSmK8ZOh127p7Qj4uP6+pdt2BDIX94ZwnF++oC3UzRAvkcqk8yVJ9kqDYr5xe2Rabb7earr76yZPey8E1rM7zvrP7cMK4n15zUgwinnWXbShnx5Gw++HlbgFsqmiOfQ/VJhuqTDNVm5fzCeri8traWqKgoS1b/4sjakuGaXRX8/u3FFFaaPZldkqKZev1oMlNiAtFU0Qz5HKpPMlSfZKi2YOcnw+U+cjqdoW6CaKOjzXBg5wSyHziVK0ZnAbCzrIaxz83j/k9X4pELg4JKPofqkwzVJxmqzar5hW2RqWka06dPt+S9PoVv2pqhy2Hn8XMH8c7VI+kYHwnAJ0vz6f/wd5z10gLmr9/rz+aKw5DPofokQ/VJhmqzcn5hPVyuaRpOp1OGBxTlzwxr6j08/u0aPl68vcn61Y+dSWykNf9CbA/kc6g+yVB9kqHagp1fwIfLDcPgvffeY8yYMU3W5+TkMHr0aLKyshg4cCDff//90Ww+aKxY9YvW8VeG0REOnrlgCPPuPbnJ+kGPzOSRr3LJL6n2y37EoeRzqD7JUH2Sodqsml+ri8wZM2YwdOhQHn/8cUpLS73rKysrmTRpEk8++STbtm3j9ddfZ8qUKezevduvDfYXTdOYNWuWZYMRRxaIDHukxvLHM/s1Wffuom2c8vx8/vJVLvvq5OfFn+RzqD7JUH2SodqsnF+rh8s/++wzoqOjiYmJ4cYbb2TdunUAvPXWW3z33Xd88cUX3tdOnjyZ0047jTvuuOOI2w32cLkQR3L/pyv5ZGn+Iev/etFQJg7JkGF0IYQQYSegw+UXXnghEydOPGT9okWLOPHEE5usGzVqFMuXLz/sdurq6qioqGjyD8Dj8Xi/Hm5Z07Qmy7qut7jsdrubLDfU1PX19ZSXl6Prune9YRjeeaYOXG54TcNyw18LzS17PJ4my8E6poOPo70fk2EYlJeXU19fH5BjeuzcQXx9ywnkPXUWV53QnQZ//HQlgx6ZyWPfrEbXDcmpDcfk8XgoKSnBMIx2c0ztMaeWjsntdlNRUYGmae3mmNpjTi0dk2EYlJaWHvZYVT2m9phTc8sH5xesY/KF364uLygoID09vcm6tLQ0iouLD/v6Z555hsTERO+/zMxMAHJzcwFYu3Yta9euBWDlypVs3LgRMM/73LJlCwCLFy8mP9/sacrOzqagoACABQsWUFRUBMDcuXMpKysDYNasWVRWVgLw3XffsXDhQmpra71XZTUsgzn8P2vWLADKysqYO3cuAEVFRSxYsMB7zNnZ2QDk5+ezePFiALZs2UJOTg4AGzduZOXKlUE5punTp1NbW9vkSrP2fEyaprFw4ULvub/+PqYolwNP0VY2btzIH07sQVxE0xOq3/5pKz3/NJ2HPpjPhq35GIYhObXymPbs2cPChQvRNK3dHFN7zOlIx7Rw4ULy8vLa1TG1x5yaO6aG36WrV69uN8fUHnNq7pga8svLywvKMTUchy+O+ury+fPnNxkuP/3007nyyiu58sorva954403+O677/jqq68OeX9dXR11dY239KuoqCAzM5OSkhKSk5O9lbrD4WiyrGkaNpvNu2y327Hb7c0uu91uHA6Hd7nh6quGZTCr+wOXXS6X92otl8uFrut4PB7vsq7rOJ3OZpcb/rJoWD7cccgxqXlMhgErdlZw0RuLDvmZHt+3I6N6JDOyewoje3RQ5pjaY05yTHJMckxyTHJMgTmmkpISUlNTfRou91uRefHFFzN69Gjuvvtu72ueeOIJduzYwZtvvnnE7QX7nExd1ykrKyMpKQm7PWynC1VaKDNckV/Guf/46bDPdUqI4uc/nRbU9qhKPofqkwzVJxmqLdj5heSOPyNGjPB2+TbIzs4+ZJojq/B4PCxZssT7l4BQTygzHJaZxOy7x5ORGHXIc7sraun+wDQufnMRS7aWBL1tKpHPofokQ/VJhmqzcn5+68ncsWMHQ4YM4bPPPuPUU09l+vTp3HzzzaxevZrY2Ngjbk+uLhcqK6mq58lpa/j8151N1sdEOLjvzH5cMKIrCVGuELVOCCGE8I+Q9GR27dqVqVOncvPNN5OWlsaTTz7JN99841OBGQq6rrN3717v1VVCPVbKMCU2gr9dfAyThnUG4NT+aQzqnEB1vYdHv1nD0EdncdoL8/lhQ2GIW2otVspQHB3JUH2SodqsnF/Y3lZS0zQWLFjAuHHjLHtjedEyK2ZYWFnHkq0lnDWoE7ph8NHi7Tz57VrqPY0f/g6xETxzwRBsNhtjenUgLozn27RihqJ1JEP1SYZqC3Z+ranXwrbIFCJYivbVUafpPDdjHV8t33XI8zeM68m9Z/bD5ZAT7oUQQlhbSIbLVaPrOjt37rRk97LwjSoZpsZF0iUpmr//bjj3nNH3kOffXLCZ0174gZU7yrDI33xBo0qGonmSofokQ7VZOb+w7RfXdZ28vDzS09NlygZFqZjhbaf14bbT+rCrrAan3cbb2Vv5ePF2tpdUM/lVc0qkYV0TOaF3Knee3odIpyPELQ4sFTMUTUmG6pMM1Wbl/GS4XIgQK6ys47r3lrI8v+ywz197Ug9uO60PidFydboQQojQknMyfaDrOvn5+WRmZlqu8he+aW8Zbi+u5o0FeXz0y/ZDnkuKcdEpIYqKGjd/OKkHlxzfjZgIBzab7TBbUkd7yzAcSYbqkwzVFuz85JxMH1j5HAbhm/aWYbcOMTx9/hCWPHQ6qXGRTZ4rq3azbnclu8preXLaWgY9MpPr31/GrNW7mbd+L79sLmZnWQ0A9ZpOrdt6k/IeTnvLMBxJhuqTDNVm5fzCtidTCCszDAPdgFU7y+nZMZaFG4owMJi2soDvcnc3+74Lju3CivwyKmo1pt8+lo7xkc2+VgghhGgt6cn0gcfjYdOmTZa8DZPwTXvO0Gaz4bDbOCYziYQoF2cPzeCcoZ157bJj+ffvj6PzYW5nCfD5rzvJK6yisLKOkU/NZv3uSgrKa6h1eyzZu9meMwwXkqH6JEO1WTm/sC0yDcOgtLQ07KaMaU/CMUObzcZpA9L5+raT6JwYxbi+HZlzz3gemTTwsK8/86UFjHlmLv0fnsHoZ+bwr4Wbyd1ZHuRWNy8cM2xvJEP1SYZqs3J+MlwuRDuxelc5c9bupXNSNOkJkVzx78XNvjYx2sW1J/Vg4tAM/rVwM/FRLpZuLeHG8b2YMKhTk9eu313J1CXbueuMvnL/dSGECHNydbkPPB4PGzdupE+fPjgc7XsuwvZKMmxeZa2bIY/OAmBYZhKXHp/J/Z+t8um9r146nNJqN/07xTOyewrHPD6Lsmo35w/vwou/Pcav7ZQM1ScZqk8yVFuw82tNvRa2k7ED1NTUhLoJoo0kw8OLP6DH0WGDi4/L5P2ft5G7s+KI7731oxzvcnKMi7JqNwBf5OzkppN7kRTtorTaTd/0OL9MoSQZqk8yVJ9kqDar5he2PZlCtHfdH5gGwJQRXfnrlGEU7avjuCdnA2C3gW5ATISDW0/tzXMz1gMQF+mkoW6sqtPQW/jt0L9TPOfsvyBp0959OB02/vLVan47MpObT+6l/ByeQgghDiXD5T7weDysXbuWAQMGyPCAoiTDls1bt5cPf9nOMxcM8U5ltGFPJe8t2so9Z/SjuKqOhGgXqbGRXPGfX6io0fj0pjE47XbsNtheUk1e4T5+2lTMztIalm4rpWhfnU/7vmB4FwZ1SSR3ZzlpCZH0So3j5P4dSYtvelW8ZKg+yVB9kqHagp2fDJcLITilfxqn9E9rsq5vejxPnjcEgOTYCO/6D68dfcj7szrEktUhllP7pwNQU+/hv0vzSU+IQtP1JsPqB/s8Zyef5+w8ZP0Fx3Zh/vpC7jq9D2XVbjonRZFpt94EwkIIIdoubHsyhRBtMyO3gMe+WUNmSgyXjerGorxirh/Xk5ztZdzzvxWt2lZcpJOLj8tk495K7p3Qj+gIB3GRTmIiHCTFRFBYWUeH2AjsdhmCF0KIUJLhch94PB5WrlzJ0KFDZXhAUZKhde2trOXtn7aSGhdJUrSr1UXn4Qztmsjlo7NIiHJycr80FuUVExflZEiXRKJckn+oyOdQfZKh2oKdnwyX+yg6OjrUTRBtJBlaU1p8FPef1d/7eOm2EqYuyefDa0YxuGsiv24rpabeQ0VtPfd/luvTNlfuKOe+T1cesv64rGQuHdWNFfllJMVE8OOmIk7s1YGbT+ntLT49uoFDekEDRj6H6pMM1WbV/MK2J1MIETy1bg+FlXVkpsQ0Wb94SwkXv7kIgBFZySzbVgrAOUMzuOWU3lTXe7j+vaUUV9W3ep9XjM5iZI8Upq8sYO76vVw2qhtXndCdrA6xLb5vc+E+sjrESlEqhBCHIcPlPtA0jZycHIYPH47TGdYdusqSDNW3ZW8Fp/xtIQAvXzKc/y3Np1NCFH+dMsz7mup6jVq3zsNf5bKnvJbfHd+NnO2lXH1iD16du5Evl+9q1T4HdU4gKcZFrVunY1wkO8tqWLWznLhIJyOykvlhQyFnDEznrStGNDsNU63bQ1m1m07N3EM+nMjnUH2SodqCnZ8Ml/vAZrORnJwsc/kpTDJUX6fExiGe1LgI3r9m1CGviYlwEhMB/7j0WO+6i0Z0BWjSK/l/Fw5B0w0GZCRwwWvZ3vWn9OvIiKxkZq3Zw8od5azedfgJ6ffVafywoRCA79fsoceD073PPXzOQI7tloQBLNlSwnuLtrGzrIYLju3CPRP60SXJmkNVwSCfQ/VJhmqzcn5h25MphLCGWz/6lS1FVXx+8wlEOlt30vp/l+Z7z9Oce894enaMAxonogfY+uzZ3uVtxVUs2FBIWbWbeev3kpNfhj9+Aw7vlkSnhCgSo11sKarippN7sbagko7xkZw5KN17Bya3R8dpt1nyPwMhhPCF9GT6QNM0Fi9ezPHHHy/DA4qSDNWnaRqXdq/l+IvH4GxlgQkQG9GYe5fkxt7EKJedWveh829mdYjlijFm7+dtp/XB7dGp03TiIp38vLmYmnoPfTvFs6Wwim9X7mLqknyf2pGzvazJ41+2lHiX7/0fjO6ZQk29h/V7KhmQkcCEgZ3YXlLFut2V9EiNJS7Sya2n9j5ksnoVyOdQfZKh2qycn7VaE0R2u50uXbpgt9tD3RRxlCRD9bU1w5N6pxIf6aR3elyTXtD3/jCKO6fm8NQFQ1p8v8thx+Uw9z26Zwfv+i5J0YzqmcKybaVs3LuPKJedS4/P4sbxPbHbbazeVcHv/7MYgCFdElm1s5xeHWPJK6zybmN4tyS2FlVRWu3m582NRWfO9rImRWnD8nuLtgHgsNu48NguOB129tVqTBySwa6yGsqq68nqEEu3DjGsyC9j0959fLpsB89PGcZ5w7sAYBgGOfllDO6cSIQzOJ8L+RyqTzJUm5Xzk+FyIYTSymvcRLscASmq6jUdA4NIpwPDMJoMc3+/Zg+90+Lo3iEGwwC73ca+Oo3CyjrqNZ2+6XHsLKth+qoCnHY71fUakU4HRVV1bCmsIiU2gg17Kvn1oF7Qo3FKv46kxEYyc/Vu9tVp9EiN5fpxPTl/eBccdhs1bg/xkU5v+3/aVERBeS0XDO9CjdtDbGTY9jcIIVpJri73gaZpZGdnc8IJJ1iue1n4RjJUn2QIv24vpaLGzc0f/kp1veewr0mNiyQu0rz70ZqCCuo1/9yKMz7KSWWtBsAjkwYyqHMiKbERdE2O5oucnSzZUsKgLonec0sPd85sSxnWa3rQelTF0ZPPodqCnZ8UmT7QdZ2CggIyMjIs2cUsjkwyVJ9k2Mjt0fnrzPW8tWAzAK9cMhzdMDi5bxoJ0c5DLhaqqHWzu7yWK/79C3sq6po8d2Dx6E8n9U5lTUEFY/uk0jc9npKqem4Y14M9e/aQlpZOemI0ZdX1xEY6+fDnbTz93TqmjOjKk+cNloudLEw+h2oLdn5SZAohhIJytpdy/mvZpMVHsvih0316z66yGraXVDO6Zwd03cBmM6c0Ka2q558LN/PZrzuaFKE3jO9Jv/R4uiRF8+XynXy82LeLm9rirtP7Uqd5SIuPBGDhxiJO6J3KRcd2pc7jwWW3s7OshkGdE9hRWkOHuAhi9l/UVa/p3kn6x/Tq0Ow+hBDBIUWmDzRNY8GCBYwbN06GBxQlGapPMjxUzvZSOidFk57gvyvNG6Z0evaCIfzu+G5Nnqt1m0P0L36/gdlr93DvhH78lFfEJcd34+yXfwTAbgM9CP9TpCdEegviTglR3D2hL/9euIX1eyoBs0Ae2iWJtQUVdIyP5IrRWdgPuDOTRzfYVVZzyJ2lAO85tXWahw279zGka2LgD0gR8jlUW7DzkyLTB7quU1RURGpqqgwPKEoyVJ9kGBzLtpWybFsJ157Us0lRdiSL8or537J8HjlnECt3lrFxzz6uOqE736/dw12fLOfG8b34evlOhneO5bpT+lFWo/HmD3nMWbeX1y87lnF9O3LCs3Mpr3EH8OhMg7skUFbtZkdpDaf1T8MAzhyUjsNuZ9WOMj78ZTu/HZnJ6l0VLM8v49FJA7liTHfKa9zohsHnv+5A0w2uOanHEedrPfgiMNXJ51Btwc5PikwhhBAhUad52FvReJ/6Z6av5c3955nOumscd/93OVef0IPRvTrwc14xY/ukEul0ULivlnW7K6mp97AorxiH3cb/lu0AICU2gu4dYli/u5LYSCduj05pddsL18P10Ea57Nx+Wh9m5u7mppN7sXHPPqrqPfTvFM+ivGJ+3lKM5jF4/NxBDO2aRMf4SGrdHqJcZmG6elc5u8pqOal3KtERzReru8tr2Vpc1WTqLCFUIEWmD9xuN3PnzuXUU0/F5XIFfH/C/yRD9UmG6jtShlV1Gg99sYoR3VO4YnRWq7Y9I7eAR79ew/9dNJTxfTt61+u6wec5O5m+qoCsDjF4dMM7z2iXpGh2ltV4X+ty2HB7Gv+bG9k9mSVbS1t7mM2KcNoxDIMol4NRPVKYv74QTTeIiXDQIS6CiYMzSI2L5IcNhYzISuaMgem8Nn8T01ftBuCDa0ZxUp/UFq/Er3V7mLduL12TYwIyzC+fQ7UFOz8pMn2g6zplZWUkJSXJ8ICiJEP1SYbqs0qGRfvq2LhnH6N7pjBrzR40j8GIrGRS4yJYvLWECIed47qnAOZwd3mNm7zCfcREOEmJjSAtPpINe/Zx5ksLmmy3e4cYBnZO8BaFnRKiOKlPKt+tKqCqmSmnWmtY10Ryd1XQv1M8TruNgvJaOu+/IYDLbmfx1hIW77+LVGpcJPFRTjrGRfLCxcPw6AYrdpQxrk9H5q3fy2kD0tlVVkOX5GgSog4tOIr31VGr6cRGmFNigXUyFEcn2PlJkSmEEEIchfySai54PZshXRI5f3gXxvXtSGK0i9yd5SRGu5pcVHT7xzl8vWIXAAMzzP+3zhmWwWXHZ3Hvpyv4fs2ekBxDg9MHpNMhNoIFGwvpmx5PtMvBjNVmsexy2Lj/rP5MOS6ThKimU2Qt2VrC1qIqPLpB77Q4b3HekoLyGkqr3AzsLP9/t3dSZPrA7XYza9YsJkyYIMMDipIM1ScZqq89Zqjrhk8XSJVV1/Ofn7byu5GZdE6KbvKcRzcoqaqnY3wka3ZVsLaggn8u3ExaQhQvXjyM5JgIymrczF+/l8VbSjipTyqax6CgvJbkGBfV9R62FldRVu1mxurdJEa7GNk9mTW7KvAYBvklNc206uj0TIC4+ATW7dl3yGT/l43qxnHdk/nngi2sKahgZPdkBnVOpN6jc0xmEjtKa/jngs3Ue3TevHwE3VNj+HlzCZOGdiYxxoVhGOworWFfnca17y5l0rDO3DOhL3abDccB3+f8kmrqNA+90+L9emztXbA/g1Jk+sAwDCorK4mPj29XVwmGE8lQfZKh+iTDwKt1e7DbbE3O2SzaV8fbP22hU2I0j3+zmq7JMfzzyhHsrajDAGau3s3O0hrqNJ0ol4PEaBff5RZQXe/hN4M7sbOshpU7ygPe9oPPiT1Qt5QYLhrRlWkrC9hVVkNlnXkDgUuOz+SiEV2x22zERTrpk95YdG4tquLRb1Zz7jGdOX9414C3XwXB/gxKkSmEEEKEifySahKiXSRG+96LVVnr5qdNRfy0qZj3fzYvmnr2giEkRLso2lfHD+sLKa6qZ+WOskOuwB+RleydID8YTuufRtG+Okqr3WwvqfauH9k9mY7xkXTvEMv3a/ZQVafxu+O70SM1lpp6DxeN6Mq2kmrWFlSQ1SGG3mlxbC2qRjcMenaMZUdpDTnby1i8pZjeaXFcOiqLuEhznsncneXUaTojspL9dhzl1W4cDpt3H6qSItMHbreb6dOnM3HixHYzxBNuJEP1SYbqkwzV5vboTP1lG/qOlVx6/qEZenSDaasKuP3jHMb37ci7fzgeMO80lRIbwTcrduFy2Dm2WzIb91aS1SGGXWW11Gk6d0zNAcwey5tO7kVcpJNjuyUzPbeAzYVVVNa6Kamqp95jsGBDIcD+4fdqivbVB/cbsd/J/ToyolsyL3y/AZsNLjm+Gy67jXOGdaZjXCTbSqrpEBvBrNW7ueakniTGuHji2zX8b2k+H1w7io7xkZRU1TOoc9NZAMqq6znl+fl0jI9k+u1jcTr8d4FOsD+DUmT6wDAMamtriYqKkiEeRUmG6pMM1ScZqu9IGRqGwaK8YgZ3TTzsFestbdfXn4nsTUXERjoZlpmEYRj8+ctcPvxlO8kxLtITohjWNYlRPVNYkV/GJaO6sbeijse/XUPRvjo6J0ajGwZnDEzn9fl5aAd1vUa57DhsNr/NBtCcLknRFO2ro07TGdk9mQ6xkVTVa0wYmM6cdXuZv94spM8ZmsH143oytGsSbo/OT5uKGNY1ieTYiCbbc3t0XC0Uox/9sp156/fy5LmDSIggaJ9BKTJ9YBgGmqbhdDrlF6OiJEP1SYbqkwzVZ8UMa+o9LNpcxPi+aU0uDjqSjXsqsdttZCbHsLloH5sLqzhjYDpOu43vcnfz8+ZiTh+QzstzNnJc9xTe+CEPgDE9O+DRDRKinWwvqaakqp7MlBgSo10YBvywv6fVn1JiI3BrOpV1Gi6HjeHdkslIjCIlNoL8khp+3FTI4M6JRLkcuD06Q7smEhvp5IReqZRW13PD+8sA6Jsexzc3jyEiwiVFZnNkuFy0lmSoPslQfZKh+sI5w+y8IjonRtM9NbbF1734/Qb+Pmfj/tuO2omJcFC0r57RPVPonBRNpNPBLR/9yr5ajcnHdKZe09lcVOU9DaDBgIwE1hZU+P04OkUb/PeW8XRLDfyV+VJk+sCKf7mJ1pEM1ScZqk8yVJ9keGSGYbC2oJK+6XHNnk/ZUE4d+D2sdXuYtrKAQV0SKCivZXyfjtR7dOav34vNZkPXDQZ2TmDDnn1U1JjnqO6trCU20klyTARbi6vYVlzN3HV7AYjbf1vVOk0nJsLBKf3SmLtuL2nxkcy5Z7xfz/VsTmvqNbUvcWqjhg+VUJdkqD7JUH2Sofokw5bZbLYjTjR/uAI9yuXgwhHmVEv9O5nvj7I7OGtwRpPXZXVouSc1O6+ILUVVXDKyG7WaeW6pw24j0umgtKqOvN1lrTqtIFjC9v5RmqYxa9YsNE0LdVPEUZIM1ScZqk8yVJ9kaH0n9ErlslFZ2O02YiKcxEQ4iXQ6AIiLsLMz92dL5he2w+VCCCGEEKJ1WlOvhW1PpmEYVFRUYJEaWxwFyVB9kqH6JEP1SYZqs3J+YVtkaprGwoULLdm9LHwjGapPMlSfZKg+yVBtVs5PhsuFEEIIIYRPZLjcB7quU1JSgq7roW6KOEqSofokQ/VJhuqTDNVm5fzCtsj0eDwsWbIEjyewt5kSgSMZqk8yVJ9kqD7JUG1Wzk+Gy4UQQgghhE9kuNwHuq6zd+9eS3YvC99IhuqTDNUnGapPMlSblfML6yIzNzfXkqEI30iG6pMM1ScZqk8yVJuV85PhciGEEEII4RMZLveBruvs3LnTkpW/8I1kqD7JUH2SofokQ7VZOb+wLjLz8vIsGYrwjWSoPslQfZKh+iRDtVk5PxkuF0IIIYQQPpHhch/ous62bdssWfkL30iG6pMM1ScZqk8yVJuV8wvrItOq5zAI30iG6pMM1ScZqk8yVJuV85PhciGEEEII4RMZLveBx+Nh06ZNlrwNk/CNZKg+yVB9kqH6JEO1WTm/sC0yDcOgtLQUi3TkiqMgGapPMlSfZKg+yVBtVs5PhsuFEEIIIYRPZLjcBx6Ph3Xr1lmye1n4RjJUn2SoPslQfZKh2qycX9gWmQA1NTWhboJoI8lQfZKh+iRD9UmGarNqfjJcLoQQQgghfBKy4fKamhquv/56srKy6Nq1K/fdd58lT0QFs3s5NzfXkt3LwjeSofokQ/VJhuqTDNVm5fz8WmTec8893ntorl69mnnz5vHqq6/6cxdCCCGEEEIBfhsu37dvH+np6eTn55OSkgLA559/zhNPPEFOTs4R3y/D5UIIIYQQ1taaes3pr50uW7aMHj16eAtMgFGjRnm7cB0OR5PX19XVUVdX531cXl4OQGlpKYC329fhcDRZ1jQNm83mXbbb7djt9maX3W43DofDu+x0OrHZbNTW1rJu3ToGDx6MYRg4nea3QtM0XC4XhmF4l3Vdx+PxeJd1XcfpdDa77PF4vNts7jgCcUwNyw3H0d6PyW63s2rVKvr3709UVFS7OKb2mFNLx+R2u8nNzWXo0KHYbLZ2cUztMaeWjsntdrNu3ToGDBiA3W5vF8fUHnNq6ZgAVq1axcCBA4mIiGgXx9Qec2rumABWrlzJoEGDiIiICPgxNdRpvvRR+q3ILCgoID09vcm6tLQ0NE2jvLy8SfEJ8Mwzz/DYY48dsp3u3bv7q0lCCCGEECIAKisrSUxMbPE1fisyNU07pKptqLJtNtshr3/wwQe5++67vY91XaekpIQOHToc9vX+VlFRQWZmJvn5+TI8ryjJUH2SofokQ/VJhmoLdn6GYVBZWUnnzp2P+Fq/FZkpKSkUFRU1WVdYWEhUVNRhK93IyEgiIyObrEtKSvJXc3yWkJAgHyrFSYbqkwzVJxmqTzJUWzDzO1IPZgO/XV1+7LHHsn79eu9YPUB2djajRo3Cbg/rOd+FEEIIIcKO36q/Tp06cdZZZ/GnP/0JTdMoKiriqaee4s477/TXLoQQQgghhCL82sX473//m127dpGRkcFxxx3H9ddfz3nnnefPXfhNZGQkjzzyyCFD9kIdkqH6JEP1SYbqkwzVZuX8LHNbSSGEEEII0X7IyZJCCCGEEMLvpMgUQgghhBB+J0WmEEIIIYTwOykyhRBCCCGE34VlkVlTU8P1119PVlYWXbt25b777vPpHpwieObOncuJJ55I79696dWrF6+88or3ua1bt3LGGWeQlZVF7969+eCDD5q89+OPP2bAgAF07dqVU045hS1btgS7+eIgN910E/379/c+zsnJYfTo0WRlZTFw4EC+//77Jq9/6aWX6N27N126dOH888+nuLg42E0W+y1evJhx48aRlZVF586d+fzzzwHJUBU7d+5k0qRJdOnShZ49e/LEE094n5MMrcswDN577z3GjBnTZH1bMisuLmbKlCl069aNrKwsXnjhhaAcSNi56aabjGuuucZwu91GWVmZcdxxxxkvv/xyqJslDnD77bcb69atMwzDMPLy8owuXboY3333naFpmjF48GDj7bffNgzDMFavXm0kJycbOTk5hmEYRnZ2ttG9e3dj27ZthmEYxlNPPWWMGDEiFIcg9tu+fbsRExNj9OvXzzAMw6ioqDC6dOlifP/994ZhGMb8+fONxMREo6CgwDAMw/jkk0+M4cOHG8XFxYamacaNN95oXHDBBSFrfzhbu3atkZGR4c2qrq7O2LNnj2SokFNPPdW47777DF3XjeLiYmPYsGHG22+/LRla2HfffWcMHjzY6NWrl/f3pmG0/Xfnb37zG+PRRx81dF03du7caWRlZRlff/11QI8l7IrMyspKIyYmxiguLvau++yzz4xjjjkmhK0SR3LXXXcZf/zjH42ZM2cektVtt91m3HnnnYZhGMYll1xivPTSS97n3G63kZKSYixfvjyo7RWNLrzwQuOWW27x/rJ88803jfPOO6/JayZNmuTNbcyYMcaXX37pfa6wsNBwOp1NPrMiOC644ALj6aefPmS9ZKiO5ORkY9WqVd7HDz30kHHLLbdIhhb26aefGtOmTTPmzZvXpMhsS2br1683OnbsaLjdbu/zL7zwwiHb87ewGy5ftmwZPXr0ICUlxbtu1KhR5Obm4vF4Qtgy0ZLCwkISExNZtGgRJ554YpPnRo0axfLlywEOed7pdHLsscd6nxfBNW3aNIqLi7nooou861rKUNM0li5d2uT51NRUunfvzqpVq4LWbgG1tbV8++23XH311Yc8Jxmq46KLLuLVV1+lvr6ebdu28dVXX3HRRRdJhhZ24YUXMnHixEPWtyWzRYsWcfzxx+N0Og95byCFXZFZUFBAenp6k3VpaWlomkZ5eXmIWiVasnjxYr799lsuvfTSZvNrOO/kSM+L4CkuLub222/n9ddfb7K+pYyKiorweDykpqYe9nkRPBs2bCA6Opp58+YxdOhQevbsyQ033EBFRYVkqJCnnnqKGTNmkJycTI8ePTjllFM4+eSTJUMFtSWzUP3fGHZFpqZph1zk09CDabPZQtEk0YKpU6cyefJk3n33XXr06NFsfg3ZHel5ERyGYXDNNddw5513NrngB1rOSNM07/sP97wInsrKSm/vyOLFi1mxYgWFhYXccccdkqEiPB4PEydO5M4776S8vJydO3eyYsUK/v73v0uGCmpLZqH6vzHsisyUlBSKioqarCssLCQqKorExMQQtUoczOPxcPPNN/PYY48xc+ZMJk+eDDSfX6dOnXx6XgTHs88+i9vt5tZbbz3kuZYySk5OxjAMSktLD/u8CJ7U1FTcbjfPPvssUVFRxMfH8+ijj/L1119LhoqYO3cu9fX13HnnnTidTjIyMvjb3/7Gc889JxkqqC2Zher/xrArMo899ljWr1/fJIjs7GxGjRqF3R523w7LuvPOO9m8eTNLly5l2LBh3vUjRowgOzu7yWuzs7O90zwc/Hx9fT3Lli1j9OjRwWm4AODll19m4cKFJCcnk5SUxDnnnMPGjRtJSkpqMcPY2Fj69evX5PmCggL27NnT5OdABF5WVhYRERHU1tZ619ntdqKioiRDRdTX1zc5Bw/A5XJRX18vGSqoLZmNGDGCX375BV3XD3lvQAX0siKLmjx5snHjjTcabrfbKCwsNIYMGWJ88cUXoW6W2K+mpsZwOBzGrl27DnmuqqrKyMjIMN5//33DMAxjyZIlRkZGhpGfn28YhmF8/vnnRvfu3Y38/HxD0zTjz3/+c8CvnhNHduBVkvn5+UZSUpIxZ84cwzAMY9q0aUZWVpaxb98+wzAM429/+5tx3HHHGaWlpUZdXZ3x+9//3jt7gAium2++2bjuuusMt9tt1NbWGhdccIFx3333SYaKKCsrMzp37mx89NFHhmGYs6ucc845xo033igZKuDgq8vbkpmu68awYcOMp59+2vB4PEZeXp7RrVs3Y+nSpQE9hrAsMgsLC43JkycbqampRlZWlvHKK6+EukniAKtXrzZsNpuRlZXV5N+ECRMMwzCMpUuXGsOHDzc6duxoDBkyxJg3b16T9z/33HNGRkaGkZ6ebvz2t781SkpKQnAU4kAH/7KcMWOG0a9fP6Njx47GmDFjjJUrV3qf83g8xj333GN07NjRyMjIMG688UajtrY2FM0Oe5WVlcbll19upKWlGb169TLuu+8+o66uzjAMyVAVq1atMs444wwjKyvL6NGjh3HnnXcaVVVVhmFIhlZ38O9Nw2hbZnl5ecb48eON1NRUo0+fPsZ///vfgB+DzTDkVjdCCCGEEMK/5CREIYQQQgjhd1JkCiGEEEIIv5MiUwghhBBC+J0UmUIIIYQQwu+kyBRCCCGEEH4nRaYQQgghhPA7KTKFEEIIIYTfSZEphBBCCCH8TopMIYQQQgjhd1JkCiGEEEIIv5MiUwghhBBC+N3/A2HITK+DiBBTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 학습결과를 시각화\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(epochs), train_loss_list, label='Train Loss')\n",
    "plt.plot(range(epochs), valid_loss_list, label=\"Valid loss\")\n",
    "plt.ylim(0, 50)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 저장 - 모델 전체를 저장.\n",
    "boston_model_save_path = \"saved_models/boston_model.pth\"\n",
    "torch.save(model, boston_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 저장된 모델 load\n",
    "load_model = torch.load(boston_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 새로운 데이터 추정\n",
    "new_data = torch.tensor(X_test_scaled[:5])\n",
    "model.eval()\n",
    "y_hat = model(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat2 =load_model(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 13]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape, y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.3089],\n",
       "        [29.6214],\n",
       "        [25.1559],\n",
       "        [11.8053],\n",
       "        [18.1612]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.3089],\n",
       "        [29.6214],\n",
       "        [25.1559],\n",
       "        [11.8053],\n",
       "        [18.1612]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.6],\n",
       "       [50. ],\n",
       "       [23. ],\n",
       "       [ 8.3],\n",
       "       [21.2]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[ :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 26421880/26421880 [00:18<00:00, 1404425.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 29515/29515 [00:00<00:00, 109894.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4422102/4422102 [00:11<00:00, 375911.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5148/5148 [00:00<00:00, 5096123.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Classes\\deeplearning\\datasets\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 1. Dataset 생성 - Built-in dataset 이용\n",
    "root_path = r\"C:\\Classes\\deeplearning\\datasets\"\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root=root_path, train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root=root_path, train=False, download=True, transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validatation set 을 trainset으로 부터 생성\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "len(trainset), len(testset), len(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.classes\n",
    "testset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 입력 이미지 확인\n",
    "idx = 1187\n",
    "x, y = testset[idx]\n",
    "plt.imshow(x.squeeze(), cmap=\"Greys\") # gray: 최소값(black) ~ 최대값(white), Greys: 반대\n",
    "plt.title(f\"{testset.classes[y]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader 생성\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(validset, batch_size=128)\n",
    "test_loader = DataLoader(testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 모델 정의\n",
    "class FashionMNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(28*28*1, 1024)  # 첫번째 Layer함수: in-입력데이터 feature수\n",
    "        self.lr2 = nn.Linear(1024, 512)\n",
    "        self.lr3 = nn.Linear(512, 256)\n",
    "        self.lr4 = nn.Linear(256, 128)\n",
    "        self.lr5 = nn.Linear(128, 64)\n",
    "        # output layer 함수 - out: 추론할 값의 개수 (다중분류: y의 class 개수)\n",
    "        self.lr6 = nn.Linear(64, 10) \n",
    "        # activation 함수\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X.shape : (batch크기, channel, height, width)   -입력(1차원)-> Linear()\n",
    "        # X.shape을 1차원으로 변환: (batch크기, channel*height*width)\n",
    "        # torch.flatten(X, start_dim=1)\n",
    "        out = nn.Flatten()(X) # batch축은 유지하고 그 이후 축들을 flatten시킨다. ()\n",
    "        # lr1 ~ lr6 : lr -> relu : Hidden Layer 계산\n",
    "        out = self.relu(self.lr1(out))\n",
    "        out = self.relu(self.lr2(out))\n",
    "        out = self.relu(self.lr3(out))\n",
    "        out = self.relu(self.lr4(out))\n",
    "        out = self.relu(self.lr5(out))\n",
    "        # lr7 : output layer -> Linear의 처리결과를 출력. (softmax-확률로 계산-는 따로 계산)\n",
    "        out = self.lr6(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTModel(\n",
      "  (lr1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (lr2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (lr3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (lr4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (lr5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (lr6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델구조 확인\n",
    "f_model = FashionMNISTModel()\n",
    "print(f_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 1024]               803,840\n",
       "├─ReLU: 1-2                              [128, 1024]               --\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─ReLU: 1-4                              [128, 512]                --\n",
       "├─Linear: 1-5                            [128, 256]                131,328\n",
       "├─ReLU: 1-6                              [128, 256]                --\n",
       "├─Linear: 1-7                            [128, 128]                32,896\n",
       "├─ReLU: 1-8                              [128, 128]                --\n",
       "├─Linear: 1-9                            [128, 64]                 8,256\n",
       "├─ReLU: 1-10                             [128, 64]                 --\n",
       "├─Linear: 1-11                           [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 1,501,770\n",
       "Trainable params: 1,501,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 192.23\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 2.04\n",
       "Params size (MB): 6.01\n",
       "Estimated Total Size (MB): 8.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(f_model, (128, 1,  28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.ones((2, 1, 28, 28), dtype=torch.float32)  # 2개 이미지.\n",
    "y_hat = f_model(sample)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 별 정답 여부값.\n",
    "i = y_hat[0].argmax(dim=-1)\n",
    "testset.classes[i.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0647,  0.0954,  0.1097, -0.1152, -0.0892,  0.0076, -0.1010, -0.1248,\n",
       "        -0.0600,  0.1301], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0953, 0.1119, 0.1135, 0.0906, 0.0930, 0.1025, 0.0919, 0.0898, 0.0958,\n",
       "         0.1158],\n",
       "        [0.0953, 0.1119, 0.1135, 0.0906, 0.0930, 0.1025, 0.0919, 0.0898, 0.0958,\n",
       "         0.1158]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = nn.Softmax(dim=-1)(y_hat)#.sum(dim=-1)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 20\n",
    "##### Train -> model, loss_fn, optimizer\n",
    "##### device로 이동해야 할 것: model, X, y (이 셋은 같은 device에 위치시켜야 한다.)\n",
    "model = FashionMNISTModel().to(device)\n",
    "#  다중분류: CrossEntropyLoss()(pred, y) - pred: softmax => log: log_softmax(), y: one-hot encoding\n",
    "#####CrossEntropyLoss() - 모델추정->softmax를 통과하면 안된다.  정답: one hot encoding 처리하면 안된다.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### 학습 - train loss, validation loss, validation accuray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "- Input layer(첫번째 Layer)의 in_features\n",
    "    - 입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "- Hidden layer 수\n",
    "    - 경험적(art)으로 정한다.\n",
    "    - Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "- output layer의 출력 unit개수(out_features)\n",
    "    -  정답의 개수\n",
    "    - ex\n",
    "        -  집값: 1\n",
    "        -  아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "- 출력 Layer에 적용하는 activation 함수\n",
    "    - 일반적으로 **None**\n",
    "    -  값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        - ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "- loss함수\n",
    "    -  MSELoss\n",
    "- 평가지표\n",
    "    - MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "- output layer의 unit 개수\n",
    "    - 정답 class(고유값)의 개수\n",
    "- 출력 Layer에 적용하는 activation 함수\n",
    "    - Softmax: 클래스별 확률을 출력\n",
    "- loss함수\n",
    "    - **categrocial crossentropy**\n",
    "    - 파이토치 함수\n",
    "        - **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        - **NLLLoss** \n",
    "            - 정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "\t        - 입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        - **LogSoftmax**\n",
    "            - 입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                - NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "              \n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```    \n",
    "\n",
    "## 이진분류 모델\n",
    "- output layer의 unit 개수\n",
    "    - 1개 (positive일 확률)\n",
    "- 출력 Layer에 적용하는 activation 함수\n",
    "    -  Sigmoid(Logistic)\n",
    "- loss 함수\n",
    "    - **Binary crossentropy**\n",
    "    - 파이토치 함수: **BCELoss**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
