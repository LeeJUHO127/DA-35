{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96733614",
   "metadata": {},
   "source": [
    "# DCGan \n",
    "Convolutional Layer 를 이용한 GAN 이미지 생성 모델\n",
    "\n",
    "- 논문: https://arxiv.org/pdf/1511.06434.pdf\n",
    "- 파이 토치 튜토리얼: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607ef26d-dd8a-47ed-9894-b067691d3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d3cdb4-7ef9-469e-9086-d39d3a3d6e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Random Seed 설정\n",
    "seed_value = 0\n",
    "random.seed(seed_value)          # python random 모듈 seed\n",
    "torch.manual_seed(seed_value)  # pytorch random seed\n",
    "np.random.seed(seed_value)     # numpy random seed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ef751-bd91-4794-9aa4-5366b92e5a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T12:24:34.987473Z",
     "iopub.status.busy": "2023-08-17T12:24:34.987233Z",
     "iopub.status.idle": "2023-08-17T12:24:34.990099Z",
     "shell.execute_reply": "2023-08-17T12:24:34.989678Z",
     "shell.execute_reply.started": "2023-08-17T12:24:34.987462Z"
    }
   },
   "source": [
    "# 하이퍼파라미터 변수 정의\n",
    "\n",
    "- **dataroot:** 학습데이터셋 저장 디렉토리 경로\n",
    "- **workers:** `DataLoader`로 데이터를 로드하기 위한 쓰레드 개수.\n",
    "- **batch_size:** 배치 크기. DCGAN **논문에서는 128**의 배치 크기를 사용.\n",
    "- **image_size:** 훈련에 사용되는 이미지의 크기. 여기서는 64 X 64 사용.\n",
    "- **nc:** 입력 이미지의 컬러 채널 수. 컬러일 경우 3.\n",
    "- **nz:** Latent vector 의 길이. Fake 이미지를 만들때 입력할 데이터.\n",
    "- **ngf:** 제너레이터의 레이어들을 통과한 특징 맵크기의 기본값으로 레이어 별로 이 값에 * N한 값을 out features로 설정.\n",
    "- **ndf:** 판별기의 레이어들을 통과한 특징 맵크기의 기본값으로 레이어 별로 이 값에 * N한 값을 out features로 설정.\n",
    "- **num_epochs:** Train 에폭 수입니다. 더 오래 훈련할수록 더 나은 결과를 얻을 수 있지만 시간도 훨씬 더 오래 걸린다.\n",
    "- **lr:** 훈련에 대한 학습률. DCGAN 논문에서 0.0002를 사용.\n",
    "- **beta1:** 아담 옵티마이저를 위한 베타1 하이퍼파라미터. 논문에서 0.5를 사용.\n",
    "- **ngpu:** 사용 가능한 GPU 개수. 0이면 CPU 모드에서 실행되고 0보다 크면 해당 수의 GPU에서 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28025381-7c9a-4ef9-a24b-769b7cea7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 변수 설정\n",
    "\n",
    "# dataroot = r\"코랩 local 경로\"\n",
    "dataroot = r'C:\\Classes\\deeplearning\\datasets'\n",
    "os.makedirs(dataroot, exist_ok=True)\n",
    "\n",
    "workers = os.cpu_count() \n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nc = 3 \n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "num_epochs = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c43406-be64-4f4f-a083-91619daec1f1",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 - celeb-A face dataset\n",
    "- 유명인사들의 얼굴 사진들\n",
    "- torchvision의 built-in dataset으로 받을 수 있다.\n",
    "    - https://pytorch.org/vision/stable/generated/torchvision.datasets.CelebA.html#torchvision.datasets.CelebA\n",
    "- 다음 사이트에서도 다운로드 받을 수 있다.\n",
    "    - http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "    - 다운 받은 뒤 압축을 풀면 디렉토리구조가 다음과 같다.\n",
    "    - 이것을 ImageFolder 를 이용해 Dataset으로 구성할 수 있다.\n",
    "    ```\n",
    "      /path/to/celeba\n",
    "         -> img_align_celeba\n",
    "            -> 188242.jpg\n",
    "            -> 173822.jpg\n",
    "            -> 284702.jpg\n",
    "            -> 537394.jpg\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72a584-9d5c-459c-9eba-90e5ce98a165",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acc2cd-1e6f-4cd1-a44c-982bd2f3c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수: 종횡비가 다를경우 긴쪽을 64 resize하고 짧은 쪽은 종횡비에 맞게 줄인다.\n",
    "transforms.Resize(64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47970c4-17fe-4210-93b4-ec59639e4778",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from gdown) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from requests[socks]->gdown) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\08_deeplearning_pytorch\\env\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64e7862",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileURLRetrievalError",
     "evalue": "Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n\nbut Gdown can't. Please check connections and permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Classes\\DA-35\\08_deeplearning_pytorch\\env\\Lib\\site-packages\\gdown\\download.py:267\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Classes\\DA-35\\08_deeplearning_pytorch\\env\\Lib\\site-packages\\gdown\\download.py:53\u001b[0m, in \u001b[0;36mget_url_from_gdrive_confirmation\u001b[1;34m(contents)\u001b[0m\n\u001b[0;32m     52\u001b[0m         error \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(error)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n",
      "\u001b[1;31mFileURLRetrievalError\u001b[0m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m                                 transforms\u001b[38;5;241m.\u001b[39mResize(image_size),     \n\u001b[0;32m      3\u001b[0m                                 transforms\u001b[38;5;241m.\u001b[39mCenterCrop(image_size),\u001b[38;5;66;03m# 가운데를 잘라내기 크기를 64 x 64\u001b[39;00m\n\u001b[0;32m      4\u001b[0m                                 transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      5\u001b[0m                                 transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)) ])\n\u001b[1;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mworkers)\n",
      "File \u001b[1;32mC:\\Classes\\DA-35\\08_deeplearning_pytorch\\env\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:85\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_transform is specified but target_type is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Classes\\DA-35\\08_deeplearning_pytorch\\env\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:155\u001b[0m, in \u001b[0;36mCelebA.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (file_id, md5, filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list:\n\u001b[1;32m--> 155\u001b[0m     \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_align_celeba.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mC:\\Classes\\DA-35\\08_deeplearning_pytorch\\env\\Lib\\site-packages\\torchvision\\datasets\\utils.py:210\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing downloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand verified \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmd5\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mfile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m \u001b[43mgdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSER_AGENT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Classes\\DA-35\\08_deeplearning_pytorch\\env\\Lib\\site-packages\\gdown\\download.py:278\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    269\u001b[0m         message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    270\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may still be able to access the file from the browser:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m             url_origin,\n\u001b[0;32m    277\u001b[0m         )\n\u001b[1;32m--> 278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[0;32m    280\u001b[0m filename_from_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    281\u001b[0m last_modified_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mFileURLRetrievalError\u001b[0m: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n\nbut Gdown can't. Please check connections and permissions."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(image_size),     \n",
    "                                transforms.CenterCrop(image_size),# 가운데를 잘라내기 크기를 64 x 64\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])\n",
    "\n",
    "dataset = datasets.CelebA(root=dataroot, split=\"all\", target_type=[\"attr\", \"identity\"], download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ae4d3-f00c-4d0f-9976-8e5f965e561b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 다운받은 일부 이미지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f6578-4c69-4f5e-9aa5-2e82ce2cb29f",
   "metadata": {},
   "source": [
    "- vutils.make_grid: https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html\n",
    "- 여러 이미지 Tensor 를 하나로 합친 Tensor를 반환한다.\n",
    "- Parameter \n",
    "    - **tensor (Tensor or list)**: 4D mini-batch Tensor (Batch, Channel, Height, Width) 또는 같은 크기의 이미지 리스트   \n",
    "    - **nrow (int, optional):** 한 행에 표시될 이미지의 개수. 최종 그리드의 형태는 ( Batch / nrow, nrow )가 된다. (Default : 8)   \n",
    "    - **padding (int, optional)**: 이미지 사이 간격 pdding (Default : 2)\n",
    "    - **normalize (bool, optional)**: True 일 경우, image 를 0~1 값으로 변환. (value_range 파라미터의 min, max 값을 기준) (Default : False)   \n",
    "    - **pad_value (float, optional)**: 패딩 되는 픽셀의 값 (Default : 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724e019-477b-4b58-9254-4ae1b24debeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))  # X(이미지들), y(label들)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "# vutils.make_grid() # 이미지들을 하나의 행렬로 합쳐주는 함수.\n",
    "plt.imshow(vutils.make_grid(real_batch[0][:64] \n",
    "                            , padding=2\n",
    "                            , normalize=True).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4117c-f872-438c-8c09-10f6a069bbab",
   "metadata": {},
   "source": [
    "# 모델 정의\n",
    "GAN 모델은 Generator와 Discriminator 두 개 모델을 정의한다.\n",
    "\n",
    "## 모델을 구성하는 Layer의 파라미터 초기화\n",
    "- DCGAN 논문에서 저자에서  모든 모델 가중치를 평균=0, 표준편차=0.02의 정규 분포에서 무작위로 초기화하도록 한다.   \n",
    "- `weights_init()` 함수는 Random값으로 초기화된 모델을 입력으로 받아 위 기준을 충족하도록 모든 convolution, convolution-transpose 및 Batch Normalization 레이어의 파라미터들을 다시 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cacceedf-cd5f-42f2-a3b6-f7d9b85074c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"asdlkjfasldkf\".__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb4ad69-6c6c-486c-b334-0ac835268446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conv2d'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv2d(3, 10, 3) # 객체 생성\n",
    "l.__class__.__name__  #객체.__class__ : class  type객체: 객체의 class정보를 담은 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df04875e-f5f0-4965-afb8-8ef7089df4bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nn.init.normal_(l.weight.data, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef7d390-f6a0-4f12-90e5-de3a319e7027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"alkdfsljksdf\".find(\"안녕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf95133-4959-418b-8db5-8bdbf91c9bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "tensor([-16.2392,   8.2318,  13.9330,  -5.7815, -11.7039])\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "print(tmp)\n",
    "nn.init.normal_(tmp, 0, 10)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfda27ee-bf5b-4b85-a750-c8ec0d1397dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 논문에 따라 레이어의 파라미터들을 초기화하는 함수.\n",
    "# nn.init.normal_(텐서, 평균, 표준편차): 텐서를 평균, 표준편차를 따르는 정규분포의 난수들로 채운다.\n",
    "# nn.init.constant_(텐서, value): 텐서를 value:float 으로 채운다.\n",
    "def weights_init(m:\"Layer\"):\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:  # 이름에 Conv가 있으면 --> Convolution Layer라면.\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02) #파라미터 초기화 (평균 0, 표준편차 0.002의 정규분포 랜덤값)\n",
    "    elif classname.find('BatchNorm') != -1: # BatchNormalization Layer라면\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02) #파라미터 초기화 (평균: 1, 표준편차 0.002  정규분포 랜덤값)\n",
    "        nn.init.constant_(m.bias.data, 0)     # bias = 0 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885051d7-6535-4135-b11e-650095addb6a",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "- Generator는 **Latent space Vector(잠재공간벡터) 를 입력** 받아 training image와 동일한 형태(분포)의 **이미지를 생성** 한다.\n",
    "    - Latent space Vector는 GAN의 입력데이터로 동일한 분포(보통 정규분포)의 random 값으로 구성된다. random 값이 어떻게 구성되느냐에 따라 다른 이미지가 생성된다.\n",
    "- Generator는 Strided Transpose Convolution, Batch Normalization, ReLU 로 이어지는 layer block들로 구성된다.\n",
    "    - **Strided Convolution** pooling layer를 사용하지 않고 stride를 이용해 size를 조정하는 것을 말함.\n",
    "    - **Transpose Convolution** 은 Convolution을 역으로 계산한다. 보통 Upsampling에 사용된다.\n",
    "- Generator의 최종 출력은 \\[-1, 1\\] 범위의 결과를 리턴한다. 그래서 출력 Layer의 activation 함수로 **tanh**를 사용한다.\n",
    "\n",
    "\n",
    "![paper](https://pytorch.org/tutorials/_images/dcgan_generator.png)<br>\n",
    "\\[DCGAN paper의 Generactor 구조\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea1e94",
   "metadata": {},
   "source": [
    "> ### Transpose Convolution Layer 출력 size 공식\n",
    "> - i: input 크기\n",
    "> - k: kernel 크기\n",
    "> - s: stride\n",
    "> - p: padding\n",
    "> \n",
    ">$$\n",
    "r\\_size = k + (i-1)\\times{s} - 2\\times{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd27ca47-b5d2-47ca-8461-940129132bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력: 10 x 10, kernel: 3 x 3, stride: 1, padding: 0\n",
    "3 + (10 - 1)*1 - 2*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b084b39-2c09-44cc-bff6-d2ad742399bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 + (4-1)*2 - 2*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d7dcefc-fa7c-4039-9b00-5e26bbff71a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngf * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca8cf020-1335-4131-8f75-e1cb844f9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 입력 : (batch, 100, 1, 1)  h: 1, w: 1, channel: 100 \n",
    "        # feature map의 size는 늘리고  (Transpose Convolution)\n",
    "        #     channel은 줄이는 방향으로 진행.  (out_channels 를 줄여나간다.)\n",
    "        self.main = nn.Sequential(\n",
    "            #ConvTranspose2d: Transpose Convolution Layer\n",
    "            nn.ConvTranspose2d(in_channels=nz, # 100\n",
    "                               out_channels=ngf * 8, \n",
    "                               kernel_size=4, \n",
    "                               stride=1,\n",
    "                               padding=0, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()  # -1 ~ 1\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20907e-2d6f-4d79-a84f-c54e8bf03cc6",
   "metadata": {},
   "source": [
    "Generator를 생성하고 `weights_init` 함수를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d0270-13b9-4c35-9c3f-6edbd6f99e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.apply(함수)  # 모델을 구성하는 layer들의 함수에 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e4939d7-8afa-48ec-bc9f-a515ffedea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generator 생성\n",
    "netG = Generator().to(device)\n",
    "### 모델을 구성하는 Layer들의 파라미터들을 초기화.\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386563e2-712f-4b18-a320-8d7d548bd136",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "- Discriminator는 trainset의 진짜 이미지와 generator가 생성한 가짜 이미지를 분류하는 역할을 한다. 이미지를 입력받아 이진분류를 해서 진짜이미지 인지 여부의 확률값을 출력한다.\n",
    "- 모델의 구조는 Strided Convolution layer, Batch normalization, LeakyReLU 로 구성된 layer block들을 통과한 뒤 sigmoid activation 함수를 통해 최종 확률값을 출력한다.\n",
    "    - 논문에서 Activation 함수로 ReLU가 아닌 LeakyReLU를 사용한 것이 특징이다.\n",
    "    - 논문에서는 down sampling을 max pooling 이 아니라 convolution layer의 stride를 이용해 줄여 나간다.\n",
    "        - 이유는 pooling layer를 사용할 경우 convolution layer가 pooling 함수를 학습하게 되기 때문이라고 한다. (convolution layer가 입력의 특성을 찾는 것 뿐만 아니라 어떻게 max pooling에 적용해야 할지 까지 학습하게 된다.)\n",
    " \n",
    "![discriminator](figures/gan/discriminator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b42e27cb-c501-4058-bea9-1d72af88efb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b84a59d1-a00b-4a20-aa73-e1969ff89e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (i_size - k_size + 2*padding)/stride + 1  # conv 연산 결과 size\n",
    "(8-4+2)/2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbb6a447-9ea7-4d69-8fb3-2f1a0beff5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 입력: 이미지 - (batch, 3, 64, 64)  # channel:3, h: 64, w:64\n",
    "        ### Feature map 의 size 는 줄이고 (Conv의 stride와 padding을 이용해서 줄인다.)\n",
    "        ###     channel은 늘린다. (ndf(64) * n n: [1,2,4,8])\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=nc,   \n",
    "                out_channels=ndf, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # f(x) = max(x, x*negative_slope)\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),  # 1 x 1 x 1\n",
    "            nn.Sigmoid()  # output: 양성(진짜이미지)일 확률로 변환.\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cb792a0-8205-4e1a-8579-430474a130aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Discriminator 생성\n",
    "netD = Discriminator().to(device)\n",
    "# 모델 파라미터들을 초기화\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa06fc-7057-49fb-9c84-d020a3d349c7",
   "metadata": {},
   "source": [
    "# 학습\n",
    "## Loss 함수와 Optimizers\n",
    "\n",
    "- GAN 모델의 최종 출력은 Real image인지 여부이므로 이진분류 문제이다.\n",
    "- Loss 함수는 Binary Cross Entropy loss (BCELoss) 함수를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bc9ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T16:55:45.692078Z",
     "start_time": "2023-11-01T16:55:45.677656Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2005f55e-ccb7-4bc5-9bce-1d729a727945",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()  # 이진분류 loss : Binary crossentropy\n",
    "\n",
    "# 학습도중 500step마다 생성자가 만든 이미지를 출력.\n",
    "### 생성자에 입력할 Latent vector (입력값을 고정하기 위해서 미리 생성.)\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device) \n",
    "# (64:batch, 100:channel, 1:h, 1:w) => 64장의 이미지를 생성.\n",
    "\n",
    "# y에 사용할 label\n",
    "real_label = 1.  # 진짜 image label\n",
    "fake_label = 0.  # 가짜 image label\n",
    "\n",
    "### optimizer\n",
    "##### 판별자 optimizer\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, \n",
    "                                         betas=(beta1, 0.999))  # beta1: 0.5\n",
    "                                        # (모멘텀계수:moment의 hp, 이동가중평균계수:rmsprop hp)\n",
    "##### 생성자 optimizer\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe6afc-0be6-476b-baef-ad10d3401e61",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36046da0-930a-4a04-99f8-0a2834723d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b6cf87f-21a9-4286-9fa0-1d50efd1174b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1000, 1000, 1000],\n",
       "        [1000, 1000, 1000]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[10,20,30]])\n",
    "a.fill_(1000)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cf3b7-6f53-4898-8787-227171c4bed4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습도중 Generator가 생성한 이미지들을 저장할 리스트. (500 step마다 한번씩 생성.)\n",
    "img_list = [] \n",
    "\n",
    "# Generator/Discriminator 의 step별 loss 를 저장.\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "s_all = time.time()  # 전체 학습시간 계산용\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    s = time.time() # 한 step 학습시간 계산용.\n",
    "    for i, data in enumerate(dataloader, 0):  # i: 로그출력할 시점 확인용. (50 step마다 loss출력)\n",
    "\n",
    "        ####################################################################################\n",
    "        # (1) Update Discriminator(판별자) network Training\n",
    "        ###################################################################################\n",
    "        # 파라미터 gradient 초기화.\n",
    "        netD.zero_grad() \n",
    "        # batch에서 이미지만 조회 -> device로 옮기기.\n",
    "        real_cpu = data[0].to(device)  # data: (X, y)\n",
    "        # batch 값을 조회 -> 현재 step에서 사용할 이미지 개수 (64, 3, 64, 64)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # label 생성 - label의 class: 1.0 (real_label변수값)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "        ##### 진짜 이미지로 판별자를 학습\n",
    "        # 판별자를 dataset에서 가져온 진짜 이미지로 추정 (Image - 1.0)\n",
    "        output = netD(real_cpu).view(-1)  # [64, 1] => [64]\n",
    "        # loss 계산\n",
    "        errD_real = loss_fn(output, label)\n",
    "        # 파라미터들의 gradient  계산 (오차역전파)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # output: 정답이 1인 이미지에대한 판별자의 예측 결과.\n",
    "        # output.mean()  # 전체(예측한 개수) 중에 1의(맞은것) 비율\n",
    "        D_x = output.mean().item()  # 진짜이미 추론한 결과 중 맞은 것의 비율\n",
    "\n",
    "        ###### 가짜 이미지로 판별자 학습\n",
    "        # 생성자를 이용해 가짜 이미지 생성.\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device) # Latent vector 정의\n",
    "        # 가짜이미지 생성 (X)\n",
    "        fake = netG(noise)  # 100, 1, 1 -> 3, 64, 64\n",
    "        # 정답(Y)의 label 생성 -> 0.0\n",
    "        label.fill_(fake_label)  # Tensor.fill_(value) Tensor의 값을 value로 다 변경.\n",
    "\n",
    "        ### 판별자에 fake 이미지를 넣어서 추론\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        ## loss 계산 \n",
    "        errD_fake = loss_fn(output, label)\n",
    "        ## gradient 계산. (real image gradient 계산 결과에 누적. weight.grad)\n",
    "        errD_fake.backward()\n",
    "        \n",
    "        # 판별자가 fake image에 대해 추론한 틀린 것의 비율\n",
    "        ### fake 이미지 추론: 정답-0,  output.mean(): 값이 1(틀린것)인 것의 비율\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # 진짜와 가짜 이미지 추론 결과 loss\n",
    "        errD = errD_real + errD_fake\n",
    "        #  판별자 파라미터 업데이트.\n",
    "        optimizerD.step()\n",
    "\n",
    "        #######################################################\n",
    "        # (2) Update Generator(생성자) network Training\n",
    "        #######################################################\n",
    "        ### 파라미터 gradient값 초기화 (weight.grad=0)\n",
    "        netG.zero_grad()\n",
    "        # label 생성 -> 1.0 (real_image)\n",
    "        label.fill_(real_label) \n",
    "        ## 판별자 학습할 때 생성한 fake 이미지를 넣어서 진짜인지 가짜인지 판별.\n",
    "        output = netD(fake).view(-1) \n",
    "        ### 추론결과: 1(진짜) 이면 생성자 입장에서는 맞은 것. (판별자를 속이는 것이 목적이므로.)\n",
    "        ############ 정답 label: 1 로 설정함(위에서 설정).\n",
    "        ### loss 계산.\n",
    "        errG = loss_fn(output, label)\n",
    "        ### gradient 계산.\n",
    "        errG.backward()\n",
    "        ### 맞은 것의 비율(생성자 입장에서.)\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # 생성자 파라미터 업데이트.\n",
    "        optimizerG.step()\n",
    "\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[{:02d}/{}][{:04d}:/{}]\\tLoss_D: {:.4f}\\tLoss_G: {:.4f}\\tD(x): {:.4f}\\tD(G(z)): {:.4f} / {:.4f}'.format(\n",
    "                epoch+1,\n",
    "                num_epochs, \n",
    "                i,\n",
    "                len(dataloader),\n",
    "                errD.item(), \n",
    "                errG.item(), \n",
    "                D_x, \n",
    "                D_G_z1, \n",
    "                D_G_z2 \n",
    "            ))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (i % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        \n",
    "    e = time.time()\n",
    "    print(f\"{epoch+1} epoch 걸린시간: {e-s}초\")\n",
    "    \n",
    "e_all = time.time()\n",
    "print(f\"총 걸린 시간: {e_all - s_all}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fc789-4cf1-4358-bd80-a7d3c31f1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습결과 시각화\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"Generator\")\n",
    "plt.plot(D_losses,label=\"Discriminator\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a11003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fake_image(idx):\n",
    "    \"\"\"학습도중 저장한 fake 이미지를 출력\"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    img = img_list[idx].permute(1,2,0)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "show_fake_image(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7783bcc-b78a-4059-aab9-f8fa392470d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain set의 이미지와 생성한 이미지 비교\n",
    "# real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(vutils.make_grid(real_batch[0][:64], padding=5, normalize=True).permute(1,2,0))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ddb1c-229a-484a-b2ca-684520e3f9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "465.455px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
