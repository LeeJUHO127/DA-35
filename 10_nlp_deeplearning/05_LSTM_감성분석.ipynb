{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfee4328-7019-4f85-bb71-d7f172024ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229f3339-08b4-4ba6-b27a-9a61c3229e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight: num_embeddings x embedding_dim\n",
    "e_layer = nn.Embedding(\n",
    "    num_embeddings=10,    # embedding할 총 단어수(vocab_size, n_vocab)\n",
    "    embedding_dim=5,        # embedding vector의 차원수. (한개 단어를 몇개 숫자 vector로 표현)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60259a9c-de25-4ec5-827f-8e6c1d88acb7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.2872, -0.9666,  1.0572,  1.8553,  1.4034],\n",
       "        [-0.2536,  0.4192, -2.0518,  0.0535, -1.4707],\n",
       "        [ 0.3092, -0.5023, -1.0968, -1.5061,  0.2261],\n",
       "        [-0.7654, -2.6820,  1.2210, -0.9326, -0.6549],\n",
       "        [ 1.0581,  0.7595,  0.0052,  0.8301, -1.6413],\n",
       "        [ 0.3635,  0.5969, -0.1236, -1.2737, -0.3104],\n",
       "        [-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],\n",
       "        [ 0.0207, -0.5517,  0.8156, -0.9954,  2.0784],\n",
       "        [ 0.3543,  0.1219, -1.8038,  0.9530,  0.1627],\n",
       "        [ 0.9938,  1.4051,  0.1349,  0.3557,  1.7975]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight 조회\n",
    "print(e_layer.weight.shape)\n",
    "e_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf045d2-e493-4e92-a922-7e9604f73f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 index 는 정수여야 한다. \n",
    "input_data = torch.LongTensor([[3, 6, 1]])    # dtype=int64\n",
    "result = e_layer(input_data)\n",
    "result.shape   # [1:문서개수,    3:문서의 토큰수,    5:embedding차원수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46498ebb-93bc-46c2-978b-0c5ab29284f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.2872, -0.9666,  1.0572,  1.8553,  1.4034],\n",
       "        [-0.2536,  0.4192, -2.0518,  0.0535, -1.4707],\n",
       "        [ 0.3092, -0.5023, -1.0968, -1.5061,  0.2261],\n",
       "        [-0.7654, -2.6820,  1.2210, -0.9326, -0.6549],\n",
       "        [ 1.0581,  0.7595,  0.0052,  0.8301, -1.6413],\n",
       "        [ 0.3635,  0.5969, -0.1236, -1.2737, -0.3104],\n",
       "        [-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],\n",
       "        [ 0.0207, -0.5517,  0.8156, -0.9954,  2.0784],\n",
       "        [ 0.3543,  0.1219, -1.8038,  0.9530,  0.1627],\n",
       "        [ 0.9938,  1.4051,  0.1349,  0.3557,  1.7975]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c691a478-46b4-41d8-9819-6fbc43389969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfa7a6-61e3-454a-8643-7df508348a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c3c68-c689-4fda-b86e-72bc29fc7078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b3c97b-9774-4829-bd2b-8fad457247ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting korpora\n",
      "  Using cached Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dataclasses>=0.6 (from korpora)\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from korpora) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from korpora) (4.66.4)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from korpora) (2.32.3)\n",
      "Collecting xlrd>=1.2.0 (from korpora)\n",
      "  Using cached xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from tqdm>=4.46.0->korpora) (0.4.6)\n",
      "Using cached Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: dataclasses, xlrd, korpora\n",
      "Successfully installed dataclasses-0.6 korpora-0.2.0 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성\n",
    "\n",
    "### Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "#### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0534360-f0ee-48ce-bb46-24c004d63272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860ccce7-1ce8-4584-a1e4-5612299c868b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\USER\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\USER\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "### 데이터 로딩 (다운로딩 + 로딩)\n",
    "corpus = Korpora.load(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935e8d9d-e360-4d98-9fc3-8c513ac971c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Korpora.korpus_nsmc.NSMCKorpus"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f517e780-5f0b-4811-aef7-ab807048d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 200000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 다 읽어오기\n",
    "all_text = corpus.get_all_texts()  # (모든 input)\n",
    "type(all_text), len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8323c36-78a4-4951-ad39-02168f6c9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n"
     ]
    }
   ],
   "source": [
    "print(all_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f05b92-063b-41c0-bceb-851d93f7617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 200000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_label = corpus.get_all_labels() # list(모든 label)\n",
    "print(type(all_label), len(all_label))\n",
    "print(all_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd1b474-69b6-4fa5-97a7-04fee8fedb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 150000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train/test 분리해서 로딩\n",
    "tr = corpus.train\n",
    "len(tr.texts), len(tr.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d5be4c-0c84-4588-ac7d-8e1beb66bda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = corpus.test\n",
    "# te.texts[0]\n",
    "len(te.texts), len(te.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb336b35-31f4-42f1-aaa3-73cec731448f",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "1. cleaning (전처리)\n",
    "2. subword 방식 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef86a71-b913-43d3-a3d7-d02336835eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install    jpype1  konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f2ec9ba-47cb-4f63-b55c-374c0a9de7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# cleaning 전처리 - 함수\n",
    "#########################\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import string\n",
    "\n",
    "def text_preprocessing(text, tokenizer):\n",
    "    \"\"\"\n",
    "    1. 영문자를 소문자 변경.\n",
    "    2. 구두점 제거\n",
    "    3. 토큰화 -> 형태소기반으로 토큰화. 어간을 추출해서 토큰화.  \n",
    "    # 소문자변환, 어간으로 토큰화 -> 같은 단어는 같은 표현으로 만든다.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", ' ', text)\n",
    "    text = tokenizer.morphs(text, stem=True) # stem=True: 어간 추출\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b407c93c-4ff2-4313-b84f-27a1a65f849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 movie 정말 재미 있다 꼭 보다 저기 있다'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"이 Movie 정말 재미 있어요....,!!!!!!. 꼭보세요.### 저기 있네요.\"\n",
    "okt = Okt()\n",
    "text_preprocessing(text, okt)\n",
    "# text = text.lower()\n",
    "# text = re.sub(f\"[{string.punctuation}]\", ' ', text)\n",
    "# text = okt.morphs(text, stem=True) # stem=True: 어간 추출\n",
    "# \" \".join(text)  # 리스트의 원소들을 \" \"을 구분자로 하나의 string으로 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaeae48-aed4-48bb-809c-558cd7d68fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# 전처리\n",
    "########################\n",
    "okt = Okt()\n",
    "all_text = [text_preprocessing(text, okt)  for text in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8633763-d349-4676-824a-b966e0bc3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38a063-8a65-43d0-8d37-bf020831579a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132019d1-281d-417e-9202-7ae7982de669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b0a09-e2fa-451e-bbca-12b124f444b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42523b2-5929-435e-903e-eadc037a8ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f5f5b-f9b2-4a37-938a-7eb02c60108f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabab58-1505-4f66-a97c-9e29e7997071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "## 모델\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9db1e0-62ce-4701-883f-714518b7ca75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76105534-b6fe-4df3-bcc6-126eabef99c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723b511-d7fd-499f-81e3-92f1db49032b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10febaf-0c9f-44b5-892d-546f8645f3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65579fe-6a24-4262-a347-fb2531e59fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93db526-8c20-42e9-8192-e2215f6e99cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1b16a-50f9-4e4b-8ea0-a22196ea21cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e68bcb-82b5-4aac-b67b-9e0ceb37390a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd542046-d101-41ee-a571-105a4c68993e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "### 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9af6e-5287-47dc-a198-deb381ba7fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b50888-c3fd-4390-bdbc-23b61e87b910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f1f61-2605-4704-964c-5d96f04f700f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
