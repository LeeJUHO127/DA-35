{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfee4328-7019-4f85-bb71-d7f172024ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229f3339-08b4-4ba6-b27a-9a61c3229e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight: num_embeddings x embedding_dim\n",
    "e_layer = nn.Embedding(\n",
    "    num_embeddings=10,    # embedding할 총 단어수(vocab_size, n_vocab)\n",
    "    embedding_dim=5,        # embedding vector의 차원수. (한개 단어를 몇개 숫자 vector로 표현)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60259a9c-de25-4ec5-827f-8e6c1d88acb7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.2872, -0.9666,  1.0572,  1.8553,  1.4034],\n",
       "        [-0.2536,  0.4192, -2.0518,  0.0535, -1.4707],\n",
       "        [ 0.3092, -0.5023, -1.0968, -1.5061,  0.2261],\n",
       "        [-0.7654, -2.6820,  1.2210, -0.9326, -0.6549],\n",
       "        [ 1.0581,  0.7595,  0.0052,  0.8301, -1.6413],\n",
       "        [ 0.3635,  0.5969, -0.1236, -1.2737, -0.3104],\n",
       "        [-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],\n",
       "        [ 0.0207, -0.5517,  0.8156, -0.9954,  2.0784],\n",
       "        [ 0.3543,  0.1219, -1.8038,  0.9530,  0.1627],\n",
       "        [ 0.9938,  1.4051,  0.1349,  0.3557,  1.7975]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight 조회\n",
    "print(e_layer.weight.shape)\n",
    "e_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf045d2-e493-4e92-a922-7e9604f73f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 index 는 정수여야 한다. \n",
    "input_data = torch.LongTensor([[3, 6, 1]])    # dtype=int64\n",
    "result = e_layer(input_data)\n",
    "result.shape   # [1:문서개수,    3:문서의 토큰수,    5:embedding차원수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46498ebb-93bc-46c2-978b-0c5ab29284f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.2872, -0.9666,  1.0572,  1.8553,  1.4034],\n",
       "        [-0.2536,  0.4192, -2.0518,  0.0535, -1.4707],\n",
       "        [ 0.3092, -0.5023, -1.0968, -1.5061,  0.2261],\n",
       "        [-0.7654, -2.6820,  1.2210, -0.9326, -0.6549],\n",
       "        [ 1.0581,  0.7595,  0.0052,  0.8301, -1.6413],\n",
       "        [ 0.3635,  0.5969, -0.1236, -1.2737, -0.3104],\n",
       "        [-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],\n",
       "        [ 0.0207, -0.5517,  0.8156, -0.9954,  2.0784],\n",
       "        [ 0.3543,  0.1219, -1.8038,  0.9530,  0.1627],\n",
       "        [ 0.9938,  1.4051,  0.1349,  0.3557,  1.7975]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c691a478-46b4-41d8-9819-6fbc43389969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfa7a6-61e3-454a-8643-7df508348a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1.4983,  1.6032,  0.9305,  1.3301,  0.0593],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c3c68-c689-4fda-b86e-72bc29fc7078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b3c97b-9774-4829-bd2b-8fad457247ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting korpora\n",
      "  Using cached Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dataclasses>=0.6 (from korpora)\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from korpora) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from korpora) (4.66.4)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from korpora) (2.32.3)\n",
      "Collecting xlrd>=1.2.0 (from korpora)\n",
      "  Using cached xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.20.0->korpora) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from tqdm>=4.46.0->korpora) (0.4.6)\n",
      "Using cached Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: dataclasses, xlrd, korpora\n",
      "Successfully installed dataclasses-0.6 korpora-0.2.0 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성\n",
    "\n",
    "### Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "#### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0534360-f0ee-48ce-bb46-24c004d63272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860ccce7-1ce8-4584-a1e4-5612299c868b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\USER\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\USER\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "### 데이터 로딩 (다운로딩 + 로딩)\n",
    "corpus = Korpora.load(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935e8d9d-e360-4d98-9fc3-8c513ac971c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Korpora.korpus_nsmc.NSMCKorpus"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f517e780-5f0b-4811-aef7-ab807048d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 200000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 다 읽어오기\n",
    "all_text = corpus.get_all_texts()  # (모든 input)\n",
    "type(all_text), len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8323c36-78a4-4951-ad39-02168f6c9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n"
     ]
    }
   ],
   "source": [
    "print(all_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f05b92-063b-41c0-bceb-851d93f7617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 200000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_label = corpus.get_all_labels() # list(모든 label)\n",
    "print(type(all_label), len(all_label))\n",
    "print(all_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd1b474-69b6-4fa5-97a7-04fee8fedb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 150000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train/test 분리해서 로딩\n",
    "tr = corpus.train\n",
    "len(tr.texts), len(tr.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d5be4c-0c84-4588-ac7d-8e1beb66bda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = corpus.test\n",
    "# te.texts[0]\n",
    "len(te.texts), len(te.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb336b35-31f4-42f1-aaa3-73cec731448f",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "1. cleaning (전처리)\n",
    "2. subword 방식 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef86a71-b913-43d3-a3d7-d02336835eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install    jpype1  konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f2ec9ba-47cb-4f63-b55c-374c0a9de7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# cleaning 전처리 - 함수\n",
    "#########################\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import string\n",
    "\n",
    "def text_preprocessing(text, tokenizer):\n",
    "    \"\"\"\n",
    "    1. 영문자를 소문자 변경.\n",
    "    2. 구두점 제거\n",
    "    3. 토큰화 -> 형태소기반으로 토큰화. 어간을 추출해서 토큰화.  \n",
    "    # 소문자변환, 어간으로 토큰화 -> 같은 단어는 같은 표현으로 만든다.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", ' ', text)\n",
    "    text = tokenizer.morphs(text, stem=True) # stem=True: 어간 추출\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b407c93c-4ff2-4313-b84f-27a1a65f849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 movie 정말 재미 있다 꼭 보다 저기 있다'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"이 Movie 정말 재미 있어요....,!!!!!!. 꼭보세요.### 저기 있네요.\"\n",
    "okt = Okt()\n",
    "text_preprocessing(text, okt)\n",
    "# text = text.lower()\n",
    "# text = re.sub(f\"[{string.punctuation}]\", ' ', text)\n",
    "# text = okt.morphs(text, stem=True) # stem=True: 어간 추출\n",
    "# \" \".join(text)  # 리스트의 원소들을 \" \"을 구분자로 하나의 string으로 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5aaeae48-aed4-48bb-809c-558cd7d68fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# 전처리\n",
    "########################\n",
    "okt = Okt()\n",
    "all_text = [text_preprocessing(text, okt)  for text in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8633763-d349-4676-824a-b966e0bc3c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아 더빙 진짜 짜증나다 목소리',\n",
       " '흠 포스터 보고 초딩 영화 줄 오버 연기 조차 가볍다 않다',\n",
       " '너 무재 밓었 다그 래서 보다 추천 한 다',\n",
       " '교도소 이야기 구먼 솔직하다 재미 는 없다 평점 조정',\n",
       " '사이 몬페 그 의 익살스럽다 연기 가 돋보이다 영화 스파이더맨 에서 늙다 보이다 하다 커스틴 던스트 가 너무나도 이쁘다 보이다']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0eda01-4d3b-4a13-914b-905452373d9d",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "569fc87e-4379-4c20-a71f-301d2eb1f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93165968-a577-4c57-b269-ab3828681b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처한 text를 파일로 저장.\n",
    "corpus_path = \"datasets/nsmc_morphs.txt\"\n",
    "with open(corpus_path, 'wt', encoding=\"utf-8\") as fw:\n",
    "    fw.write('\\n'.join(all_text))  \n",
    "# 리스트안에 모든 원소 문장들을 하나의 string 만들어서 저장.  각 문장 구분자로 enter 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "132019d1-281d-417e-9202-7ae7982de669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface tokenizer lib 사용.\n",
    "## subword 방식 (BPE)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000  # 최대 단어수 \n",
    "min_frequency = 5   # 어휘사전에 포함할 때 최소빈도수 조건.\n",
    "\n",
    "tk = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\")  # unknown 토큰 - 모르는 토큰을 표시할 토큰.\n",
    ")\n",
    "tk.pre_tokenizer = Whitespace()  # 공백을 기준으로 사전 토큰화.\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency, \n",
    "    special_tokens=['[PAD]' , '[UNK]'], # 우리가 등록하는 토큰들. 특정 의미를 가지는 토큰들을 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "499b0a09-e2fa-451e-bbca-12b124f444b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "tk.train([corpus_path], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b42523b2-5929-435e-903e-eadc037a8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 저장\n",
    "tk.save(\"models/msmc_bpe.json\")\n",
    "\n",
    "# load_tk = Tokenizer.from_file(경로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d27f5f5b-f9b2-4a37-938a-7eb02c60108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "김상중\n",
      "None\n",
      "총 토큰 개수: 22921\n"
     ]
    }
   ],
   "source": [
    "##### tokenizer 조회 \n",
    "# 문자열 token -> 정수 index  (id)   없으면 None\n",
    "print(tk.token_to_id('김상중'))\n",
    "# 문자열 token <- 정수 index  (id)\n",
    "print(tk.id_to_token(16959))\n",
    "print(tk.id_to_token(30000))  # 없으면 None\n",
    "print(\"총 토큰 개수:\", tk.get_vocab_size())\n",
    "# 전체 token 조회\n",
    "# tk.get_vocab()  # dict-  token : id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cabab58-1505-4f66-a97c-9e29e7997071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걍 인피니트 가 짱 이다 진짜 짱 이다 ♥\n",
      "<class 'tokenizers.Encoding'>\n",
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['걍', '인피니트', '가', '짱', '이다', '진짜', '짱', '이다', '♥']\n",
      "[540, 8863, 506, 2408, 3081, 3096, 2408, 3081, 119]\n"
     ]
    }
   ],
   "source": [
    "### 문서 -> token화 \n",
    "print(all_text[10])\n",
    "result = tk.encode(all_text[10])\n",
    "print(type(result))\n",
    "print(result)\n",
    "print(result.tokens)  # 토큰화-token\n",
    "print(result.ids)       # 토큰화 - id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'걍 인피니트 가 짱 이다 진짜 짱 이다 ♥'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.decode([540, 8863, 506, 2408, 3081, 3096, 2408, 3081, 119])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27009724-f096-48d8-b2a6-c751ab87d646",
   "metadata": {},
   "source": [
    "# pytoch Dataset, DataLoader 생성\n",
    "\n",
    "## Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a726d87-cd00-4e6b-9a10-1db9639cfc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6824c33-b404-4cc8-98d0-d49da15e9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text, label, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            text: 전체 댓글들 (input)\n",
    "            label: 전체 정답(긍부정 여부, output)\n",
    "            max_length: text의 최대 토큰수 (모든 문서의 token수를 max_length에 맞춘다.)\n",
    "            tokenizer: 토크나이저\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        text(댓글) -> 모든 text의 토큰개수를 max_length 에 맞춘다.\n",
    "        max_length 보다 토큰수가 많은 경우 뒤에를 짤라낸다.\n",
    "        max_length 보다 적은 경우 모자라는 만큼 뒤에 [PAD] 토큰을 추가.\n",
    "        \"\"\"\n",
    "        \n",
    "        text = self.text[idx]  # 문장  - \"이 영화 재미있네요\"\n",
    "        encode = self.tokenizer.encode(text)  # 문자열 기반으로 토큰화\n",
    "        text_tokens_id = encode.ids  # [10, 2, 5, 300]\n",
    "        \n",
    "        PAD_TOKEN_ID = self.tokenizer.token_to_id('[PAD]')\n",
    "        seq_length = len(text_tokens_id)  # idx 번째 문서의 토큰 개수\n",
    "        result = None\n",
    "        if seq_length >= self.max_length:\n",
    "            result = text_tokens_id[:self.max_length]\n",
    "        else: # max_length보다 토큰수가 적은 경우 -> 뒤에 [PAD] 추가.\n",
    "            result = text_tokens_id + ([PAD_TOKEN_ID] * (self.max_length - seq_length))\n",
    "       \n",
    "        label = self.label[idx]\n",
    "        # return input[idx],  output[idx]  ==> torch.Tensor\n",
    "        return (torch.tensor(result, dtype=torch.int64), # token id(index) => 정수(int64)\n",
    "                  torch.tensor([label], dtype=torch.float32))# y: float32\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 전체 데이터 개수를 반환. len(ds객체)\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1ea9f09-2d13-4c11-9f96-b70add501176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Dataset 생성\n",
    "#########################\n",
    "okt = Okt()\n",
    "train_texts = [text_preprocessing(txt, okt) for txt in corpus.train.texts]\n",
    "test_texts = [text_preprocessing(txt, okt) for txt in corpus.test.texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5251f5d1-045b-42ed-b8f5-82097cd2e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "train_set = NSMCDataset(\n",
    "    train_texts,            # 댓글들(x)\n",
    "    corpus.train.labels, # 정답(y)\n",
    "    max_length = MAX_LENGTH, # 각 문서의 총 토큰개수.\n",
    "    tokenizer=tk\n",
    ")\n",
    "\n",
    "test_set = NSMCDataset(test_texts, corpus.test.labels, \n",
    "                                max_length=MAX_LENGTH, tokenizer=tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "28970ceb-ab8e-49f6-aa43-ab590f557f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5eb1fe11-8d7e-4f5d-ab48-5cd3b8df2e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40]) torch.Size([1])\n",
      "tensor([3105, 2203,    4,   16, 3076,  506, 3102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]) tensor([1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'최고 의 2 d 영화 가 아니다'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 확인\n",
    "idx = 10000\n",
    "x, y = train_set[idx]\n",
    "print(x.shape, y.shape)\n",
    "print(x, y)\n",
    "tk.decode(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46fee389-6686-426f-a42d-4bed23cdd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# DataLoader\n",
    "################################\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ca9af511-95e2-4c34-84d6-beed6b74aa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 782)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step수\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1e566974-595f-4894-b9dc-efc42da45bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 40]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape\n",
    "# x shape: [64, 40]  # 64-batch_size(한번 입력되는 문서 개수),  \n",
    "#                          40: seq_length (한개 문서의 토큰개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7eb4b2-42db-4b5a-aa04-7b75658acf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "## 모델\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b239a69e-2e25-4881-9375-f826f2b9fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NSMCModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, \n",
    "                         hidden_size, num_layers, dropout_rate=0):\n",
    "        \"\"\"\n",
    "        Embedding, LSTM, Linear 세개의 Layer를 생성.\n",
    "        Parameter\n",
    "            vocab_size: 어휘사전에 등록된 총 단어(토큰) 개수\n",
    "            embedding_dim: embedding vector의 차원\n",
    "            hidden_size: LSTM의 hidden state 개수\n",
    "            num_layers: LSTM의 layer를 몇개 쌓을 것인지. \n",
    "            dropout_rate: LSTM의 dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0  # PAD embedding은 학습하지 않는다.(update안함.)\n",
    "            # 모자라는 token을 채우는 토큰->padding: [PAD]의 토큰값을 지정\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,  # 입력 feature개수 -> nn.Embedding을 통과한 개별 토큰의 원소수\n",
    "            hidden_size=hidden_size,     # Unit(Node)의 개수.\n",
    "            num_layers=num_layers, \n",
    "            bidirectional=True,             #양방향.(입력된 문서의 내용을 양방향에서 특성을 추출)\n",
    "            dropout=dropout_rate\n",
    "        )# batch_first=False  입력/출력 : (seq, batch, features)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # LSTM->droput->Linear\n",
    "        \n",
    "        # classifer(Linear) 입력: many to one: one-문맥정보 (입력 문서의 정보-마지막 timestep 처리결과)\n",
    "        #                      LSTM (output(o), hidden(x), cellstate(x))\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=hidden_size * 2, # * 2 (양방향)  만약 단방향 * 1\n",
    "            out_features=1    # 이진분류 => positive일 확률\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() # Linear 출력결과(1개) 를 0 ~ 1 확률로 변환.\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 입력 X shape:   [batch_size,   seq_length:문장토큰수] \n",
    "        embeddings = self.embedding(X)\n",
    "        # embeddings shape : [batch_size, seq_length, embedding_dim]\n",
    "        # LSTM input shape을 변경: batch_first=False [seq_length, batch_size, embedding_dim]\n",
    "        embeddings = embeddings.permute(1, 0, 2) # 원소들의 index를 변경.\n",
    "        output, _ = self.lstm(embeddings) \n",
    "        # output shape:   [seq_length:40, batch_size, hidden_size * 2] # 마지막 sequence hidden\n",
    "        last_output = output[-1, :, :]  # [batch_size, hidden*2]\n",
    "        last_output = self.dropout(last_output)\n",
    "        last_output = self.classifier(last_output)\n",
    "        last_output = self.sigmoid(last_output)\n",
    "        return last_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef02fa59-dbd7-42d9-9f72-06747e51534b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22921"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59d1c5a0-23b9-4dc3-8915-6e14d88785c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSMCModel(\n",
      "  (embedding): Embedding(22921, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 32, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = NSMCModel(\n",
    "    vocab_size=tk.get_vocab_size(), \n",
    "    embedding_dim=100,\n",
    "    hidden_size=32, \n",
    "    num_layers=2,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "485544c2-c63b-45a4-8eec-610ecc590f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 40]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f188ee93-0c7f-49e8-8a6d-caf922daa651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4ae24ebc-3645-4dd0-a2be-4e7ac1277c9b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4829],\n",
       "        [0.4868],\n",
       "        [0.4831],\n",
       "        [0.4824],\n",
       "        [0.4890]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08de9ab1-fc9b-4785-a894-0478934cf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_fn(model, dataloader, loss_fn, optimizer, device, interval=100):\n",
    "    \"\"\"\n",
    "    1 epoch 학습 하는 함수.\n",
    "    interval: 몇 step에 한번씩 로그를 남길지 간격지정.\n",
    "    \"\"\"\n",
    "    # 1. 모델 train 모드로 변환.\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    # 2. 학습\n",
    "    for step, (X, y) in enumerate(dataloader):\n",
    "        # 1. X, y 를 device 로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 2. 추론\n",
    "        pred = model(X)\n",
    "        # 3. loss 계산\n",
    "        loss = loss_fn(pred, y)  # 추론값, 정답\n",
    "        loss_list.append(loss.item())\n",
    "        # 3. 학습\n",
    "        ## grad 계산\n",
    "        loss.backward()\n",
    "        ## 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        ## 파라미터 gradient 초기화 \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"\\t{step} step Train Loss: {np.mean(loss_list)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "78fe5975-713b-407a-9ab1-88611fef5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 평가 함수\n",
    "def test_fn(model, dataloader, loss_fn, device):\n",
    "    # 1. 모델 mode  변경\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for X, y in dataloader:\n",
    "        # 2. X, y를 device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 추론 - 평가\n",
    "        with torch.no_grad():\n",
    "            pred_proba = model(X)  # pos 확률\n",
    "            pred_label = (pred_proba > 0.5).type(torch.int32)\n",
    "            loss = loss_fn(pred_proba, y)\n",
    "            loss_list.append(loss.item())\n",
    "            # 정확도 \n",
    "            acc_list.extend(torch.eq(pred_label, y).to(\"cpu\").tolist())     # Tensor -> List\n",
    "    print(f\"\\t>>>>>validation loss: {np.mean(loss_list)}, validation accuracy: {np.mean(acc_list)}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96b62e34-00aa-442b-823e-c0059e76102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "58693722-fe8d-4b09-88ba-b59832eeb308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========0/3===========\n",
      "\t0 step Train Loss: 0.4393729567527771\n",
      "\t500 step Train Loss: 0.401001913551085\n",
      "\t1000 step Train Loss: 0.4004125414373396\n",
      "\t1500 step Train Loss: 0.39587123365421284\n",
      "\t2000 step Train Loss: 0.39212915837496654\n",
      "\t>>>>>validation loss: 0.3762972017421442, validation accuracy: 0.83152\n",
      "=========1/3===========\n",
      "\t0 step Train Loss: 0.36396801471710205\n",
      "\t500 step Train Loss: 0.3424482988026328\n",
      "\t1000 step Train Loss: 0.34713382694449696\n",
      "\t1500 step Train Loss: 0.3502136333674927\n",
      "\t2000 step Train Loss: 0.35284143080120384\n",
      "\t>>>>>validation loss: 0.3755488446377732, validation accuracy: 0.83554\n",
      "=========2/3===========\n",
      "\t0 step Train Loss: 0.3660000264644623\n",
      "\t500 step Train Loss: 0.3422320996989271\n",
      "\t1000 step Train Loss: 0.33985113539359907\n",
      "\t1500 step Train Loss: 0.34003690503622036\n",
      "\t2000 step Train Loss: 0.3409044269932204\n",
      "\t>>>>>validation loss: 0.38057345765478473, validation accuracy: 0.83644\n",
      "학습에 걸린시간:  347.86237835884094\n"
     ]
    }
   ],
   "source": [
    "## 학습\n",
    "EPOCHS = 3\n",
    "import time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "s = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"========={epoch}/{EPOCHS}===========\")\n",
    "    train_fn(model, train_loader, loss_fn, optimizer, device, 500)\n",
    "    test_fn(model, test_loader, loss_fn, device)\n",
    "e = time.time()\n",
    "print(\"학습에 걸린시간: \", (e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "### 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3bc9af6e-5287-47dc-a198-deb381ba7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"models/nsmc_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13e498-8b1d-4b6d-add6-b702636074ce",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f187645c-904a-4bc1-a01c-040cea395517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 전처리\n",
    "## text cleaning\n",
    "# 토큰화 + padding 추가.\n",
    "def pad_tokens(tokens, max_length=40):\n",
    "    PAD_TOKEN = tk.token_to_id('[PAD]')\n",
    "    seq_length = len(tokens)\n",
    "    result = None\n",
    "    if seq_length >= max_length:\n",
    "        result = tokens[:max_length]\n",
    "    else:\n",
    "        result = tokens + ([PAD_TOKEN] * (max_length - seq_length))\n",
    "    return result\n",
    "\n",
    "def text_to_input_tensor(text):\n",
    "    #1. cleaning\n",
    "    text = text_preprocessing(text, Okt())\n",
    "    #2. token화 + padding\n",
    "    encode = tk.encode(text)\n",
    "    text_token = pad_tokens(encode.ids)\n",
    "    return torch.LongTensor(text_token)  # 정수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "899f1f61-2605-4704-964c-5d96f04f700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "댓글: 돈이 아깝다\n"
     ]
    }
   ],
   "source": [
    "txt = input(\"댓글:\")\n",
    "input_data = text_to_input_tensor(txt).unsqueeze(0)  # [40] -> [1, 40]\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    yhat = model(input_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2ce951d5-bbad-487b-8174-0eccbc843659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012134862132370472"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4d57944d-8c08-452b-b764-431a44883062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'부정'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"긍정\" if yhat.item() > 0.5 else \"부정\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0ee25-a817-493e-b547-5e9c7063863e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
