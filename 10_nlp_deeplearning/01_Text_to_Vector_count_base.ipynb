{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a241325d-ff0d-4a85-a151-fa3eca3953e9",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing)\n",
    "\n",
    "## 자연어\n",
    "- 사람이 일상적으로 사용하는 언어를 말한다.\n",
    "    - 어떤 목적을 가지고 사람이 만든 것이 아니라 **자연적으로 만들어진 언어**를 말한다.\n",
    "- **인공언어**\n",
    "    - 특정 목적을 위해 사람이 인위적으로 만든 언어로 자연어의 대척점에 있는 언어 개념.\n",
    "    - 예: 프로그래밍 언어\n",
    "\n",
    "## 자연어 처리 (NLP)\n",
    "- 자연어 처리(Natural Language Process)는 컴퓨터가 인간의 언어를 이해하고 분석하는 분야를 말한다.\n",
    "- 자연어 처리도 오래된 분야인데 딥러닝이 적용되면서 획기적인 발전을 이루었다.     \n",
    "\n",
    "\n",
    "## NLP 응용 분야\n",
    "- **텍스트 분류(Text classification)**\n",
    "   - 입력받은 문장을 분류하는 문제로 대표적으로 감성분석(sementic analysis)가 있다.\n",
    "   - 감성분석\n",
    "       -  입력받은 텍스트가 어떤 감정의 글인지를 분류한다. 일반적으로 긍정, 중립, 부정적 인지를 분류한다.\n",
    "- **Image captioning**\n",
    "   - 입력받은 이미지를 설명하는 문장을 생성하는 문제.\n",
    "- **개체명인식(Named Entity Recognition)**\n",
    "   - 문장의 각 단어가 어떤 종류(의미) 인지를 찾는 문제\n",
    "   - 미국에 사는 톰은 스무살입니다. ==> 미국: 위치, 톰: 이름, 스므살: 나이\n",
    "- **품사태깅(Pos tagging)**\n",
    "   - 문장의 각 토큰의 품사를 찾는 문제\n",
    "   - 미국에 사는 톰은 스무살입니다. ==> 미국: 대명사, 예: 조사,  톰: 대명사, 은: 조사, 스무: 명사, 살: 명사, 이다: 동사\n",
    "- **Chatbot**\n",
    "   - 입력받은 문장에 대한 답을 하는 시스템.\n",
    "   - Encoder는 질문을 받아 처리하고 Decoder는 답변을 생성하는 seq2seq 구조를 사용한다.\n",
    "- **Machine translation**\n",
    "   - 번역 시스템\n",
    "   - Encoder는 번역 대상문장을 입력받아 처리하고 Decoder는 번역 문장을 생성하는 seq2seq 구조를 사용한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3c5d03a-dee7-44ab-9272-a27246dfe58e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Text to vector\n",
    "\n",
    "-   자연어 처리(Natural Language Process)는 컴퓨터가 인간의 언어를 이해하고 분석하는 분야를 말한다.\n",
    "-   자연어 처리를 위해서는 자연어 문장(Text)를 컴퓨터가 이해할 수있는 숫자 집합(Vector)으로 변환해야 한다.\n",
    "-   머신러닝을 이용해 문제를 해결하는 경우도 마찬가지로 모델이 추론할 대상 Text를 Vector로 변환해야 한다.\n",
    "    -   Text를 Vector 로 변환하는 작업은 Feature extractor가 담당한다. Feature extractor가 변환한 Feature vector가 Text vector가 된다.\n",
    "    -   Feature extractor를 구현할 때 어떻게 하면 단순히 숫자로 변환하는 것이 아닌 **text의 의미를 잘 표현할 수있는 vector(숫자 집합)로 변환**할 지 다양한 연구가 되어왔다.\n",
    "\n",
    "![nlp_text_to_vector_outline](figures/nlp_text_to_vector_outline.png)\n",
    "\n",
    "## Word Representation\n",
    "\n",
    "-   단어를 컴퓨터가 이해할 수 있는 값으로 변환하는 작업을 word representation 이라고 한다.\n",
    "-   Text를 **NLP 모델이 잘 처리할 수 있도록** 단어 단위로 나눠서 **그 단어의 의미를 잘 표현할 수있는 숫자 집합** 으로 변환하는 방법을 말한다.\n",
    "\n",
    "### Word Representation의 방법들\n",
    "\n",
    "1. **Count-Based Word Representation**\n",
    "    - 단어가 나오는 횟수(출연 빈도수) 기반 표현방식\n",
    "    - Bag of word, TF-IDF\n",
    "2. **Word Embedding**\n",
    "    - 단어를 고정된 Vector로 표현한다. 고정됬다는 것은 문맥에 상관없이 같은 단어는 같은 값으로 표현한다.\n",
    "    - Word2Vec, Fastext, Glove\n",
    "3. **Contextualized Word Embedding**\n",
    "    - 문맥을 고려한 Word Embedding 방식. 단어가 문맥에 따라 의미가 바뀌는 동적 Vector 표현방식이다.\n",
    "    - ELMo, BERT, GPT2, RoBERTa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66796e-dc71-403b-a119-d7024720e2f0",
   "metadata": {},
   "source": [
    "# Count-Based Word Representation\n",
    "\n",
    "1. [One-Hot Encoding](#One-Hot-Encoding)\n",
    "2. [Bag Of Words(BoW)](<#Bag-Of-Words(BoW)>)\n",
    "    - DTM(Document Term Matrix)/TDM(Term Document Matrix)\n",
    "    - TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209171a4-6362-4db1-817e-e9316f7889d7",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "-   One-Hot Encoding을 이용해 단어를 표현하는 방법.\n",
    "    -   전체 vocabulary size 크기에 vector로 구성하며 각 index는 vocabulary를 구성하는 단어들을 의미한다. Value는 표현하는 단어는 1 나머지는 0으로 구성한다.\n",
    "    -   보통 Word Embedding 모델의 입력으로 사용된다.\n",
    "\n",
    "### 구현방법\n",
    "\n",
    "1. 토큰화(Tokenization)\n",
    "    - 문장을 최소단위(token)로 나눈다.\n",
    "    - 토큰화는 모든 Embedding의 시작이다.\n",
    "    - ex) `\"나는 축구를 좋아합니다. 친구는 야구를 좋아합니다.\"`\n",
    "2. Vocabulary(vocab - 어휘/단어 사전) 구성\n",
    "    - Token - index 형식으로 구성한 어휘 사전을 만든다.\n",
    "    - `['.', '나는', '야구를', '좋아합니다', '축구를', '친구는']`\n",
    "3. Vocab을 이용해 각 token들을 one-hot encoding 처리한다.  \n",
    "   ![ohe](figures/onehotvector.png)\n",
    "4. 문장을 one-hot vector를 이용해 행렬로 구성한다.\n",
    "\n",
    "-   ex) 나는 야구를 좋아합니다.\n",
    "\n",
    "```\n",
    "[\n",
    "  [0, 1, 0, 0, 0, 0],   -- 나는\n",
    "  [0, 0, 1, 0, 0, 0],   -- 야구를\n",
    "  [0, 0, 0, 1, 0, 0]    -- 좋아합니다.\n",
    "]\n",
    "```\n",
    "\n",
    "> ### 어휘 사전(단어 사전, vocaburay, vocab)\n",
    ">\n",
    "> -   사용할 모든 단어(토큰)들을 모아놓은 집합.\n",
    "> -   보통 단어-정수index 쌍의 구조로 저장한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33f9c-0b0d-4f79-945a-304a136bf158",
   "metadata": {},
   "source": [
    "### One-Hot Encoding의 문제\n",
    "\n",
    "1. 단어 수가 늘어날 수록 vector의 차원도 늘어난다.\n",
    "2. 단어의 의미를 표현할 수없다. 그래서 단어 또는 문장의 유사도와 같이 의미를 알아야 하는 분야에서는 사용할 수없다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef9d87-75a8-4c18-a013-76acf2b5d134",
   "metadata": {},
   "source": [
    "## Bag Of Words(BoW)\n",
    "\n",
    "-   **단어의 출현 빈도에만 집중해 단어를 표현**(word representation) 하는 방법으로 단어의 순서는 전혀 고려하지 않는다.\n",
    "    -   **많이 나온 단어가 중요한 단어이다.**\n",
    "\n",
    "### DTM/TDM\n",
    "\n",
    "-   문서안에서 문서를 구성하는 단어들이 몇 번 나왔는지를 표현하는 행렬로 Vector화 한다.\n",
    "-   행: 문서, 열: 단어 - DTM\n",
    "-   행: 단어, 열: 문서 - TDM\n",
    "-   Value: 개수\n",
    "\n",
    "#### 구현방법\n",
    "\n",
    "1. 어휘 사전(vocabulary) 생성\n",
    "    - 모든 단어에 고유 정수 index를 부여\n",
    "2. 문장을 embeddig 할 때 각 단어 index에 등장 횟수를 기록한다.\n",
    "\n",
    "#### scikit-learn CountVectorizer 이용\n",
    "\n",
    "-   빈도수 기반 Vector화 지원 Text 전처리 클래스\n",
    "-   **주요 생성자 매개변수**\n",
    "    -   stop_word :stopword 지정\n",
    "        -   str: \"english\" - 자체 제공 불용어는 제공됨\n",
    "        -   list: stopword 리스트\n",
    "    -   max_df: 특정 횟수 이상나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "        -   int(횟수), float(비율)\n",
    "    -   min_df: 특정 횟수 이하로 나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "    -   max_features: 최대 token 수\n",
    "        -   빈도수가 높은 순서대로 정렬 후 지정한 max_features 개수만큼만 사용한다.\n",
    "    -   ngram_range: n_gram 범위 지정\n",
    "        -   n_gram:\n",
    "        -   튜플 (범위 최소값, 범위 최대값)\n",
    "        -   1, 2) : 토큰화된 단어를 1개씩 그리고 순서대로 2개씩 묶어서 Feature를 추출\n",
    "    -   tokenizer: 토큰화 처리 함수\n",
    "-   **메소드/ 속성**\n",
    "    -   fit(X)\n",
    "        -   학습\n",
    "        -   매개변수: raw document - 문장을 원소로 가지는 1차원 배열형태(list, ndarray)\n",
    "        -   **Train(훈련) 데이터셋 으로 학습한다. Test 데이터셋은 Train 셋으로 학습한 CountVectorizer를 이용해 변환만 한다.**\n",
    "    -   transform(X)\n",
    "        -   DTM 변환\n",
    "    -   fit_transform(X)\n",
    "        -   학습/변환 한번에 처리\n",
    "    -   vocabulary\\_: 어휘 사전\n",
    "\n",
    "> #### n-gram\n",
    ">\n",
    "> -   N 개의 단어(token)을 묶어서 하나의 토큰으로 처리하는 방식을 n-gram이라고 한다. (n은 몇개 토큰을 하나의 단위로 묶을지 개수)\n",
    ">     -   uni-gram (n=1), bi-gram (n=2), tri-gram (n=3), 4개부터는 n-gram으로 표기(4-gram, 5-gram, ..)\n",
    ">     -   Embedding이나 언어모델을 만들때 적용하는 기법이다.\n",
    "> -   BoW에 n-gram을 적용하면 n개의 단어를 하나의 tokne으로 처리한다.\n",
    "> -   언어모델에 n-gram을 적용하면 이전/이후 n 개의 단어를 이용해 다음 단어를 유추한다.\n",
    "> -   n-gram의 문제\n",
    ">     -   n이 너무 크면 희소성(출현 빈도가 낮아진다)의 문제가 발생한다.\n",
    ">         -   `나는 어제 밥을 먹으러 식당에 가려다가 마음을 바꾸었다.` 이런 token이 전체 corpus에 얼마나 있을까?\n",
    ">     -   n이 너무 작으면 단어간의 관계성이 표현이 안되는 문제가 발생한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e90e2-607e-4840-a188-60608b4717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"나는 야구를 좋아합니다. 나는 축구를 좋아하지 않습니다.\", \n",
    "        \"친구는 야구와 축구를 좋아합니다. 야구보다 축구를 더 좋아합니다.\", \n",
    "        \"나는 축구를 더 좋아합니다. 영국 축구 리그보다 이탈리아 축구 리그를 더 좋아합니다.\", \n",
    "        \"나는 농구를 좋아하지 않습니다. 배구도 좋아하지 않습니다.\", \n",
    "        \"나는 액션영화를 좋아합니다.\", \n",
    "        \"어제 비빕밥을 먹었습니다.\"]\n",
    "\n",
    "stop_words = list('은는이가을를도.,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b224d-59e3-4931-a06e-b5d8a4f9a302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30772ba-8455-4b2f-a0da-249f1cf173c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef93f4-de57-41b8-b364-6cfc2ecf21e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ea5d8-0b33-4b10-9f1a-fad3b2e11678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1550e02-8b1d-4b4f-9826-c1e61d8cf087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f3c27c1-4f94-483a-a002-c447e494ab88",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "-   개별 문서에 많이 나오는 단어가 높은 값을 가지도록 하되 동시에 여러 문서에 자주 나오는 단어에는 페널티를 주는 방식\n",
    "-   어떤 문서에 특정 단어가 많이 나오면 그 단어는 해당 문서를 설명하는 중요한 단어일 수 있다. 그러나 그 단어가 다른 문서에도 많이 나온다면 언어 특성이나 주제상 많이 사용되는 단어 일 수 있다.\n",
    "-   각 문서의 길이가 길고 문서개수가 많은 경우 Count 방식 보다 TF-IDF 방식이 더 좋은 예측 성능을 내는 경우가 많다.\n",
    "\n",
    "#### 구현방법\n",
    "\n",
    "-   TF (Term Frequency) 정의: 해당 단어가 **해당 문서에** 몇번 나오는지를 나타내는 지표\n",
    "-   DF (Document Frequency) 정의: 해당 단어가 **몇개의 문서에** 나오는지를 나타내는 지표\n",
    "-   IDF (Inverse Document Frequency) 정의: DF에 역수로 $\\cfrac{\\text{전체 문서수}}{\\text{해당 단어가 나오는 문서수}}$\n",
    "-   TF-IDF 정의: $TF * \\left(\\log \\cfrac{\\text{전체 문서수}}{\\text{해당 단어가 나오는 문서수}} \\right)$\n",
    "    -   log는 전체 문서의 수가 많으면 값의 단위가 너무 커지므로 log를 취한다.\n",
    "    -   scikit-learn의 경우 분모가 0이면 Exception이 발생하기 때문에 **분모에 1을 더하고** $\\log(0)$도 계산이 안되므로 **분자에도 1을 더했으며** 그 계산 결과에 **1을 더하여 계산**함.\n",
    "        -   $TF * \\left(\\log \\cfrac{\\text{전체 문서수 + 1}}{\\text{해당 단어가 나오는 문서수 + 1}} + 1\\right)$\n",
    "\n",
    "#### scikit-learn TfidfVectorizer 이용\n",
    "\n",
    "-   TF-IDF 기반 텍스트 벡터화 전처리 클래스\n",
    "\n",
    "#### 주요 생성자 매개변수\n",
    "\n",
    "-   stop_word :stopword 지정\n",
    "    -   str: \"english\" - 영문 불용어는 제공됨\n",
    "    -   list: stopword 리스트\n",
    "-   max_df: 특정 횟수 이상나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "    -   int(횟수), float(비율)\n",
    "-   min_df: 특정 횟수 이하로 나오는 것은 무시하도록 설정(무시할 횟수/비율 지정)\n",
    "-   max_features: 최대 token 수\n",
    "    -   빈도수가 높은 순서대로 정렬 후 지정한 max_features 개수만큼만 사용한다.\n",
    "-   ngram_range: n_gram 범위 지정\n",
    "    -   n_gram:\n",
    "    -   튜플 (범위 최소값, 범위 최대값)\n",
    "    -   (1, 2) : 토큰화된 단어를 1개씩 그리고 순서대로 2개씩 묶어서 Feature를 추출\n",
    "-   tokenizer: 토큰화 처리 함수\n",
    "\n",
    "#### 메소드/Attribute\n",
    "\n",
    "-   fit(X)\n",
    "    -   학습\n",
    "    -   매개변수: 문장을 가진 1차원 배열형태(list, ndarray)\n",
    "    -   **Train(훈련)+Test(테스트) 데이터셋 으로 학습한다.**\n",
    "-   transform(X)\n",
    "    -   DTM 변환\n",
    "-   fit_transform(X)\n",
    "    -   학습/변환 한번에 처리\n",
    "-   vocabulary\\_: 어휘 사전\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd911ddf-7d9a-49bf-801a-0c0c604553b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb07335-9a21-4072-a99a-aa278c1a0c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bfbcd-091b-468c-9e01-32908c8fb9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f23f17-d109-4aa0-9975-9067e828224a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a655da-c86c-4c7e-94e0-1e53f3374138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
