{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2b3c4-8539-466d-ad52-da953f97498f",
   "metadata": {},
   "source": [
    "# Naver 영화댓글 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf4e484-b0a5-4852-b9f4-0b2908014f61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.1)\n",
      "Requirement already satisfied: sympy in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: requests in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.12.0)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "   ---------------------------------------- 0.0/309.4 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 122.9/309.4 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 309.4/309.4 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.31.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b44cbd-2c06-40c2-8beb-982bf47e3b70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.32.1->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.32.1->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "   ---------------------------------------- 0.0/542.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 542.1/542.1 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 370.8/370.8 kB ? eta 0:00:00\n",
      "Downloading pyarrow-16.1.0-cp311-cp311-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 3.4/25.9 MB 71.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.5/25.9 MB 79.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.7/25.9 MB 93.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.9/25.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.9/25.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.5/25.9 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.1/25.9 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.9/25.9 MB 50.4 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB ? eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.19.2 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.3.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce81c9e7-13a9-491d-a67e-370ba0e72894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate -U\n",
    "# 설치후 커널 재시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca8d80-d72b-4c50-8225-16a06f17c731",
   "metadata": {},
   "source": [
    "# Huggingface Dataset 패키지\n",
    "- 허깅페이스 허브에 공유된 데이터셋을  다운로드해서 전처리 및 관리할 수있도록 돕는 라이브러리. \n",
    "- 많은 공개데이터셋을 동일한 인터페이스로 사용할 수있다.\n",
    "- 설치\n",
    "    - `pip install datasets`\n",
    "- https://huggingface.co/datasets\n",
    "- https://github.com/huggingface/datasets\n",
    "      \n",
    "## Huggingface Dataset loading\n",
    "- datasets 로딩\n",
    "    - `load_data('dataset name')`\n",
    "        - huggingface datasets에 등록된 Dataset 이름 넣어 Loading한다.\n",
    "          \n",
    "![img](figures/huggingface_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bf57c5-8fb9-4da6-b74d-6db47842e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439126e6-4bd6-4941-b792-c43f8caec4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e541255b-7700-45fc-89e7-cd9a316567da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9331c66f404c31ac8a26649e4e6837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3261233f5abb4de19f9c7d395ab4b954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c8de4b4fd94e8192f123f3de70b9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf1dc236a1e4b708fbaa19812fdbc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ea49c5d9af464fbbf09d9598d8d49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3a97441b184e05b5048a7f47ef6aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "nsmc = load_dataset(\"e9t/nsmc\")\n",
    "print(type(nsmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d1fd9e-601f-4a27-afe4-a3d58492084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cfff92a-ed13-4db3-b924-4efb7bc76c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9976970', '3819312', '10265843', '9045019', '6483659']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc['train'][\"document\"][:5]  # list\n",
    "nsmc['train']['label'][:5]\n",
    "nsmc['train']['id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b97d8089-3dff-4ed8-9637-88bd8ad0d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f173235-9500-4219-afd7-c3e2460ec719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eda4387e03e4c9cbb6311ce3345e283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee0aafee01b4489b1b0f312af4bd631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word', 'sentiment'],\n",
       "        num_rows: 2118\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lex = load_dataset('senti-lex/senti_lex', \"ko\")\n",
    "sent_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cb02884-485e-4664-8503-7cfc151d379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_lex['train']['word']\n",
    "# sent_lex['train']['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e5e7e8-3d44-4b0b-88f9-8ed9632730a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터셋 구성. train: 150,000   test: 50,000\n",
    "train_X = nsmc['train']['document']\n",
    "train_y = nsmc['train']['label']\n",
    "test_X = nsmc['test']['document']\n",
    "test_y = nsmc['test']['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87222688-f85f-4b9b-b60e-7e0f0d3cb478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 데이터 일부만 sampling \n",
    "# Dataset\n",
    "a = nsmc['train'].shuffle().select(range(1000))    # 0 ~ 9999 번째 값만 sub sampling\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae86b0-062f-43cd-bd66-3e6b0a500a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd956109-6f8f-4412-a159-4c9c4cdb0ef8",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 loading\n",
    "\n",
    "- 모델 별 Model 클래스를 이용하거나 Auto class를 이용해 모델, 전처리기(tokenizer, ImageProcessor 등)을 로딩한다.\n",
    "    - Huggingface에 저장된 model name을 입력해서 pretrained 모델을 loading 한다.\n",
    "    - fine tuning 한 경우 모델 저장 디렉토리 경로를 넣어 pretrained 모델을 loading한다.\n",
    "- AutoModel은 model name을 주면 그 모델이 학습한 base 모델에 맞는 객체를 생성해서 반환한다.\n",
    "    - Auto Model은 task 별로 다양한 클래스들이 있다.\n",
    "        - 클래스 이름 형식: AutoModelFor{Task형식}\n",
    "        - ex) `AutoModelForObjectDetection`, `AutoModelForSequenceClassification`\n",
    "    - https://huggingface.co/docs/transformers/model_doc/auto\n",
    "    - 전처리기(tokenzier)는 사용하려는 모델이 사용한 전처리기를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c02f7b81-3eb6-4d4b-9031-d38010e13c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"beomi/kcbert-base\"  #backbone 이름.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# num_labels는 분류할 class의 개수.\n",
    "## Backbone network는 미리학습된 beomi/kcbert-base 모델을 사용.\n",
    "## Estimator network는 학습안된 layer를 추가해서 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66b22541-3ad5-4d18-ad53-9dbdbe6682c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "BertForSequenceClassification                                --\n",
       "├─BertModel: 1-1                                             --\n",
       "│    └─BertEmbeddings: 2-1                                   --\n",
       "│    │    └─Embedding: 3-1                                   23,040,000\n",
       "│    │    └─Embedding: 3-2                                   230,400\n",
       "│    │    └─Embedding: 3-3                                   1,536\n",
       "│    │    └─LayerNorm: 3-4                                   1,536\n",
       "│    │    └─Dropout: 3-5                                     --\n",
       "│    └─BertEncoder: 2-2                                      --\n",
       "│    │    └─ModuleList: 3-6                                  85,054,464\n",
       "│    └─BertPooler: 2-3                                       --\n",
       "│    │    └─Linear: 3-7                                      590,592\n",
       "│    │    └─Tanh: 3-8                                        --\n",
       "├─Dropout: 1-2                                               --\n",
       "├─Linear: 1-3                                                1,538\n",
       "=====================================================================================\n",
       "Total params: 108,920,066\n",
       "Trainable params: 108,920,066\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eb52259-f139-4db7-9314-c3496e52ab31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(300, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33799b2d-c76f-4c24-8704-7ad1c05c5300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ab74ab-3349-49fa-9626-3f2f28e52754",
   "metadata": {},
   "source": [
    "## pytorch Dataset 생성\n",
    "모델 입력으로 다음 4개 항목을 dictionary로 묶어서 제공하도록 구현한다.\n",
    "1. input_ids: 입력 text 토큰을 id로 변환한 값\n",
    "2. token_type_ids: 문자쌍 구분시 사용. 단일 문장: 0, 문자쌍-첫문장: 0, 두 번째 문장: 1\n",
    "3. attention_mask: 실제 토큰값과 패딩구분값\n",
    "4. labels: 정답 class index\n",
    "\n",
    "1 ~ 3은 위의 train_encoding, test_encoding으로 만듬. labels은 train_data/test_data의 label 키 값 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0f4a766-6ad7-40fc-a061-79b8cedf2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (댓글) -> 토큰화\n",
    "train_encoding = tokenizer(\n",
    "    train_X, \n",
    "    return_tensors='pt', # 토큰화 처리결과들을 torch.Tensor 로 반환.\n",
    "    padding=True,      # 패딩 방식 - 제일 토큰수가 많은 문장에 맞춘다.\n",
    ")\n",
    "test_encoding = tokenizer(\n",
    "    test_X, \n",
    "    return_tensors='pt',\n",
    "    padding=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bdbfcee-1c09-4895-9b73-18bf1562944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoding.keys()\n",
    "type(train_encoding['input_ids'])\n",
    "train_encoding['input_ids'].shape  # [150000:문장수, 142:토큰수]\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "228123a6-282d-44b8-84df-a0d2d4540136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': 4, 'attention_mask': 40, 'token_type_ids': 400}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\n",
    "    \"input_ids\":[1, 2, 3, 4, 5],\n",
    "    \"attention_mask\":[10, 20, 30, 40, 50],\n",
    "    \"token_type_ids\":[100, 200, 300, 400, 500]\n",
    "    \n",
    "}\n",
    "idx = 3\n",
    "{key: value[3]  for key, value in a.items()}\n",
    "# {\"input_ids\":4, \"attention_maks\":40, \"token_type_ids\":400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01a2b900-a89c-4573-ae52-21a621bcdaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Dataset 정의 #################\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            encodings: tokenizer로 encoding 된 댓글.\n",
    "            labels: 정답 라벨들\n",
    "        \"\"\"\n",
    "        self.encodings = encodings # DatasetDict\n",
    "        self.labels = labels   # List\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        index 번째의 학습/검증/테스트 데이터를 반환.\n",
    "        BERT 모델 입력 형식에 맞춰서 반환. bert_model(input_ids, token_type_ids, attention_mask)\n",
    "        Parameter\n",
    "            index: int - 반환할 데이터의 index\n",
    "        Return\n",
    "            dictionary - input_ids, token_type_ids, attention_mask, label 을 딕셔너리에 묶어서 반환.\n",
    "        \"\"\"\n",
    "        data = {key: value[index]  for key, value in self.encodings.items()}  # label 뺀 dict \n",
    "        # data 에 label 추가\n",
    "        data[\"labels\"] = torch.tensor(self.labels[index], dtype=torch.long)     # label 추가.\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d79841df-ab33-419b-a065-bf11020d8ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = NSMCDataset(train_encoding, train_y)\n",
    "test_set = NSMCDataset(test_encoding, test_y)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61d54c1a-513e-4e9e-a399-d47e01c41665",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  2170,   832,  5045,    17,    17,  7992, 29734,  4040, 10720,\n",
       "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train_set[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f905f-3f53-480b-8e70-1e76f8479555",
   "metadata": {},
   "source": [
    "# 학습\n",
    "- Transformers는 model 학습을 위해 TrainingArguments, Trainer 클래스를 제공한다.\n",
    "- TrainingArguments Trainer를 위한 설정을 하는 클래스\n",
    "- TrainingArguments, Trainer를 이용하면 training option, logging, gradient accumulation, mixed precision등을 쉽게 설정해 학습, 평가를 모두 진행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ad4307a-609a-49f7-b7e2-215500eb60a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117187.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * 150000 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82a80239-306a-4574-ae6b-5603ae161a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 총 step:  epoch수 * 데이터수 / batch_size\n",
    "\n",
    "##  학습(fine tuning) 관련된 설정.\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"models/nsmc\", # 학습 도중 모델이 저장될 디렉토리.\n",
    "    logging_dir = \"logging/nsmc\", # 학습 도중 생성되는 기록(log)를 저장할 디렉토리.\n",
    "    num_train_epochs=1,         # 학습 에폭수\n",
    "    per_device_train_batch_size=128, # batch size (학습)\n",
    "    per_device_eval_batch_size=128, # batch size (검증, 평가)\n",
    "    logging_steps=50,    # 몇 step에 한번씩 로그를 저장할지. \n",
    "    save_steps=50,        # 몇 step에 한번씩 모델을 저장할지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5378b3e7-803b-41dd-9456-1d03288c7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8373a-12ce-4b45-899c-6fbfe9281801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c0b4dec-3f5a-4030-a838-b96bf3e077c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3a53785c994289bb5e5e26eeb0ecd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "# acc_fn = load_metric('accuracy')\n",
    "acc_fn = load_metric('f1')\n",
    "acc_fn.compute(references=[0, 1, 1], predictions=[1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5bea4539-23e8-4b18-9ce5-b0da53c2e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(pred):\n",
    "    \"\"\"\n",
    "    모델 학습하는 도중에 예측값과 정답을 받아서 평가점수(accuracy)를 계산. (callback 함수)\n",
    "    Parameter\n",
    "        pred: EvalPrediction - 예측값, 정답들을 묶어서 제공.\n",
    "    Return\n",
    "        dict : key-평가지표이름, value: 평가점수\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(dim=-1) # 모델 추론값([0일확률, 1일확률])에서 class 추출\n",
    "\n",
    "    metrics1 = load_metric('accuracy')\n",
    "    metrics2 = load_metric('f1')\n",
    "    acc = metrics1(references=labels, predictions=preds)\n",
    "    f1 = metrics2(references=labels, predictions=preds)\n",
    "    return {\"accuracy\":acc, \"f1 score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "820fbae6-cfe6-4702-9360-071b721cc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "## Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model, # 학습할 대상 모델\n",
    "    args=args,       # TrainingArguments\n",
    "    train_dataset=train_set, # 학습 데이터셋    trainer.train()\n",
    "    eval_dataset=test_set,   # 검증 데이터셋    trainer.evaluate()\n",
    "    compute_metrics=compute_metric       #  평가 함수를 등록.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed29fc23-b191-4cbb-9938-415bb6efb2ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1172 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000276F94F8950>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Classes\\DA-35\\10_nlp_deeplearning\\env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1501, in enumerate\n",
      "    def enumerate():\n",
      "    \n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a986d4f-41aa-4506-baf6-88e4be5e2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate() # 평가 (eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e0d2ede-78d3-4d80-b72d-1921a3f93f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/my_nsmc\\\\tokenizer_config.json',\n",
       " 'models/my_nsmc\\\\special_tokens_map.json',\n",
       " 'models/my_nsmc\\\\vocab.txt',\n",
       " 'models/my_nsmc\\\\added_tokens.json',\n",
       " 'models/my_nsmc\\\\tokenizer.json')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 모델 저장 -> fine tuning한 모델, tokenizer\n",
    "# model.save_pretrained(\"저장할 디렉토리\")\n",
    "model.save_pretrained(\"models/my_nsmc\")\n",
    "tokenizer.save_pretrained(\"models/my_nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7ea65-606c-4572-8e6f-a253cefadb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6e490a-4d3e-454d-a7dd-15b052cee797",
   "metadata": {},
   "source": [
    "## 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23911e38-7e34-416d-9137-b9ce7c7895d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# 파인튜닝한 모델/토크나이저 디렉토리 경로\n",
    "path = \"models/text_cls_model/\"\n",
    "load_tokenzer = AutoTokenizer.from_pretrained(path)\n",
    "load_model = AutoModelForSequenceClassification.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74f7113d-d452-4016-b689-11f6f587e94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast,\n",
       " transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(load_tokenzer), type(load_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0f780-a07d-45d4-af5b-43af910adc5c",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1abb813-88ca-41b3-8be6-7ff47eef5624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대상(댓글) 문자열 -> list ->tokenizer 토큰화 -> model 추론 \n",
    "sentence = [\"이걸 영화라고 만든 거냐?\", \n",
    "            \"아무 기대 없이 봤는데 재미있네.\", \n",
    "            \"연기 죽이네.\", \"그냥 OTT로 볼껄. 극장표값이 아깝다.\", \"또 보고 싶다. 너무 재미있었다.\"]\n",
    "sent_encoding = load_tokenzer(\n",
    "    sentence, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "sent_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81194e5f-9197-46b7-9074-c54ca04ebdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = load_model(**sent_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e3d50cca-be78-4fb9-bb8c-9a3a0ff02b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6738, -2.2030],\n",
       "        [-2.2332,  1.8189],\n",
       "        [ 0.3708, -0.2615],\n",
       "        [ 2.8151, -2.2008],\n",
       "        [-2.5266,  2.0443]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c30886cd-2758-4805-8539-51124031accf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9924, 0.0076],\n",
       "        [0.0171, 0.9829],\n",
       "        [0.6530, 0.3470],\n",
       "        [0.9934, 0.0066],\n",
       "        [0.0102, 0.9898]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "419b34a7-5de4-4e78-b8ec-bc9c682fe9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이걸 영화라고 만든 거냐? - 부정적 댓글 - 99.24%\n",
      "아무 기대 없이 봤는데 재미있네. - 긍정적 댓글 - 98.29%\n",
      "연기 죽이네. - 부정적 댓글 - 65.30%\n",
      "그냥 OTT로 볼껄. 극장표값이 아깝다. - 부정적 댓글 - 99.34%\n",
      "또 보고 싶다. 너무 재미있었다. - 긍정적 댓글 - 98.98%\n"
     ]
    }
   ],
   "source": [
    "for comment, label in zip(sentence, output.logits.softmax(dim=-1)):\n",
    "    label_str = \"긍정적 댓글\" if label.argmax(dim=-1).item() == 1 else \"부정적 댓글\"\n",
    "    label_pred = label.max().item()\n",
    "    print(f\"{comment} - {label_str} - {label_pred * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1eb6d-643e-4e10-86f1-3ee3657b47f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7f10b-f61f-4f3d-abb0-29c00fdc679a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
