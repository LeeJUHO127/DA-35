{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d23187-2b5d-4ef4-aed4-1792210d2c54",
   "metadata": {},
   "source": [
    "# [Huggingface ](https://huggingface.co)\n",
    "\n",
    "Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다. 2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\n",
    "\n",
    "## 주요 서비스\n",
    "1. **Transformers 라이브러리**: 다양한 NLP 작업을 위한 사전 학습된 모델(Pretrained Model)들을 제공하는 라이브러리로, BERT, GPT-3, T5 등의 모델과 huggingface 생태계의 다양한 서비스들을 이용할 수있는 파이썬 기반의 오픈소스 라이브러리.\n",
    "3. **Huggingface Model Hub**: 사용자들이 자신이 훈련한 모델들을 공유 할 수 있는 git 기반 플랫폼. \n",
    "2. **Datasets 라이브러리**: 다양한 NLP 데이터셋을 손쉽게 활용할 수 있도록 하는 라이브러리로, 데이터 전처리 및 관리에 유용하다.\n",
    "4. **API 서비스**: 모델 배포를 위한 API 서비스를 제공하여, 사용자가 쉽게 모델을 배포하고 활용할 수 있도록 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fd763-772e-4ed8-99f9-ce8115756cb7",
   "metadata": {},
   "source": [
    "## Transformers 라이브러리\n",
    "\n",
    "Hugging Face의 Transformers는 자연어 처리(NLP) 분야에서 가장 널리 사용되는 라이브러리 중 하나로, 다양한 사전 학습된 모델을 제공하여 연구자들과 개발자들이 손쉽게 NLP 애플리케이션을 구축할 수 있도록 돕는다. 이 라이브러리는 언어 모델링, 텍스트 생성, 번역, 감성 분석, 질문 응답 시스템과 같은 다양한 NLP 작업을 수행할 수 있도록 설계되었다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "1. **광범위한 모델 지원**: BERT, GPT-3, RoBERTa, T5 등 다양한 최첨단 모델들을 지원한다. 이러한 모델들은 텍스트 분류, 감성 분석, 질문 답변, 텍스트 생성 등 다양한 NLP 작업에 활용될 수 있다.\n",
    "2. **파이토치(PyTorch)와 텐서플로우(TensorFlow) 호환**: Transformers 라이브러리는 두 가지 주요 딥러닝 프레임워크와 호환되어, 사용자가 자신의 선호에 따라 선택하여 사용할 수 있다. \n",
    "3. **사용자 친화적인 인터페이스**: 간단하고 직관적인 API를 통해 복잡한 모델을 쉽게 불러오고 사용할 수 있다. 이를 통해 빠르게 프로토타입을 만들고 실험할 수 있다.\n",
    "4. **모델 허브**: HuggingFace Model Hub를 통해 공유되는 수많은 모델들을 Transformers 라이브러리를 이용해 다운받아서 추론까지 쉽게 사용할 수있다.\n",
    "5. **자동화된 파이프라인**: 파이프라인(pipeline) API를 사용하면 몇 줄의 코드로 다양한 NLP 작업을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers 를 이용해 Backbone 사용\n",
    "\n",
    "## Transformers 설치\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3817d6ef-7239-4738-89c1-1d6a64d8a8ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.2/9.1 MB 25.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/9.1 MB 44.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/9.1 MB 44.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.3/9.1 MB 33.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 41.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 38.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 287.3/287.3 kB 18.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, transformers\n",
      "Successfully installed safetensors-0.4.3 transformers-4.41.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37d4e3f-73bc-4e24-bb82-3735927e2e1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.45)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.1)\n",
      "Requirement already satisfied: colorama in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Using cached ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Using cached widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee06f46-e5e8-43d9-93e7-9aaf7f2cf8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩한다.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장된다.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용한다.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "\n",
    "1. **AutoModel**\n",
    "   - 주어진 모델 이름에 맞는 사전 학습된 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델을 로드한다.\n",
    "\n",
    "2. **AutoTokenizer**\n",
    "   - 해당 모델에 적합한 토크나이저를 자동으로 로드한다.\n",
    "   - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드한다.\n",
    "\n",
    "3. **AutoConfig**\n",
    "   - 모델의 설정(config)을 자동으로 로드한다. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델을 생성할 수있다.\n",
    "   - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "4. **AutoModelForSequenceClassification**\n",
    "   - 시퀀스(Text) 분류 작업을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "5. **AutoModelForQuestionAnswering**\n",
    "   - 질문-응답 작업을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "6. **AutoModelForTokenClassification**\n",
    "   - 토큰 분류 작업(예: 개체명 인식)을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395e6380-16bc-48e8-b050-9aae6856043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ffab593-17fa-4676-922d-336397744e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer))\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")  # 학습된 weight를 가지는 모델을 loading\n",
    "print(type(model))\n",
    "conf = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(conf))\n",
    "model2 = AutoModel.from_config(conf)   # initial weight를 가지는 모델을 생성. (모델 구조 정보만 가지고 모델을 생성.)\n",
    "print(type(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6119331-d8ed-4325-b52b-6df88f5460cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe70881-f8e1-4eff-b49e-ccc0a8200dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n"
     ]
    }
   ],
   "source": [
    "gpt_model = AutoModel.from_pretrained('openai-community/gpt2')\n",
    "print(type(gpt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa3b5c9-e7ef-43c0-8ee2-445522340168",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d89d1bd-b152-4185-b0ce-27388f41beaf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30d066-0389-425d-bd43-5b22c078fa95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 한글 텍스트 학습 모델\n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저 모델 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc7e8c2-dc3d-4f39-84a7-39e6bbc38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")  # BertTokenizer: WordPiece 방식 토크나이저\n",
    "model = AutoModel.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5087a1-04bb-4b36-912f-c1c5bdd77101",
   "metadata": {},
   "source": [
    "### 입력값 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55779119-cee3-446d-a8a4-5217a35226df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 3개\n",
    "sentences = [\"안녕하세요. 반갑습니다.\",  \"KcELECTRA가 릴리즈 되었습니다!\", \"KcBERT를 Google Colab에서 TPU를 통해 학습할 수 있는 튜토리얼을 제공합니다! 아래 버튼을 눌러보세요.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b46b85a-8123-4108-a087-435d8c1d9218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  batch_encode_plus 와 동일 \n",
    "### __call__함수 호출 해서 여러문장들을 한번에 토큰화.\n",
    "features = tokenizer(\n",
    "    sentences,         # 대상 \n",
    "    max_length=10,  # 최대 토큰수.\n",
    "    padding='max_length',  #`True` ==`'longest'`, `'max_length'`, `False` ==`'do_not_pad'`(default) # 토큰수가 max_length 보다 작을때 처리방식\n",
    "    return_tensors=\"pt\", # np: 넘파이, tf: Tensorflow , pt: pytorch, 생략(list). -> 토큰 id 리스트 반환타입.\n",
    "    truncation=True, # max_length 보다 토큰수가 많은 경우 어떻게 처리할지 방법지정-> True 나머지를 뒤의 토큰들을 버린다.\n",
    ")\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39df4394-252f-4738-9cdb-d6cd250659f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 19017,  8482,    17,  1483,  4981,  8046,    17,     3,     0],\n",
       "        [    2,    44,  4773,  4451,  4450,  4451,  4881,  4850,  4992,     3],\n",
       "        [    2,    44,  4773,  4756,  4451,  4992,  4850,  4180,    40,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7602279-5823-4825-af3c-434303eccb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc7c8a6c-3a62-4bde-9e0b-b6f661cee605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 15830,     5,     3,     0,     0,     0,     0,     0,     0],\n",
      "        [    2,  9909,  2483,  2688,  4755,  8186,    32,     3,     0,     0]])\n",
      "--------------------\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "--------------------\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# input_ids: 워드피스 토큰화 -> token index(id)로 변환한 결과\n",
    "pprint(features['input_ids']) # 2: 문장시작(CLS), 3: 문장 끝(SEP), 0: 패딩(PAD)  표현하는 special token,  (max_length=10으로 했으므로 token idx가 10개로 구성됨.)\n",
    "print(\"-\"*20)\n",
    "# attention mask: 일반 토큰인지 패딩인지 mask 행렬. padding 아닌 token => 1, padding인 token => 0 \n",
    "pprint(features['attention_mask'])\n",
    "print(\"-\"*20)\n",
    "# token_type_ids: segment 정보 (token pair일때 첫번째는 0, 두번째는 1로 채워진다. -> 이 둘을 하나의 tokens로 묶어서 반환하는데 그 구분-https://hyen4110.tistory.com/89 확인) 한개 문장일 경우 모두 0이된다.\n",
    "pprint(features['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80f357f6-73ce-4a1d-9fce-84424b642d86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한문서 토큰화\n",
    "# tokenizer.encode(\"안녕하세요. 저는 홍길동입니다.\")  # -> [2, 19017, 8482, 3]\n",
    "# 디코드 -> 토큰 ID리스를 문장으로 변환\n",
    "# tokenizer.decode([2, 19017, 8482, 3]) # -> [CLS] 안녕하세요 [SEP]\n",
    "result = tokenizer.batch_encode_plus(sentences) # 한번에 여러개 ==> __call__() 와 동일.\n",
    "print(type(result))\n",
    "result['input_ids']  # list - 문장 -> 토큰 id 리스트\n",
    "result['token_type_ids'] # pair 일때 0번, 1번 어떤 문장의 토큰인지\n",
    "result['attention_mask']  # 토큰인지 패딩인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09aaa025-17c9-46ad-b708-f2729cb2a95e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'내일또 봐'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()  # 어휘사전\n",
    "tokenizer.vocab_size, len(tokenizer)   # 총 어휘수\n",
    "tokenizer.convert_ids_to_tokens(2)    # token_id => 문자열토큰\n",
    "tokenizer.convert_tokens_to_ids(\"내일\")\n",
    "tokenizer.convert_tokens_to_string([\"내일\", \"##또\", \"봐\"])  # 토큰 리스트 -> 문장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f1c111-09d8-42f4-b2e6-1467b02dc671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', '[CLS]')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[3], tokenizer.ids_to_tokens[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 문장수준 Vector(Contextualized Word Embedding)생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f0eaf52-43ed-458e-97b8-4835b183ef70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ecc77abc-ee30-4769-96da-c7e837435b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\classes\\da-35\\10_nlp_deeplearning\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.__call__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "240b332b-0827-4388-a394-ede6cd1ee213",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**features) \n",
    "# 'input_ids', 'token_type_ids', 'attention_mask' 입력 ===> torch.Tensor 타입."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4134a6e-757c-4026-8b29-7d25ba455c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "054bb72f-f5bd-49cf-a98a-e32b3222f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393e7b6-0b57-498e-a4c2-27782aa1f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'last_hidden_state' : rnn의 outputs 같이 모든 토큰에 대한 출력 feature\n",
    "# 'pooler_output' : rnn의 hidden_state 같이 마지막 token에 대한 처리값. 입력 문장, 텍스트에 대한 단일 vector 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0f2a839-9fa3-4c22-b3a5-21bc2f7e6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)  # feature값\n",
    "# [3:입력문장수, 10:토큰수-max_length설정한 것, 768: vector] -> 10개 토큰이 768개 features로 변환된 것. \n",
    "##### 이것은 token별 vector가 나오므로 **개체명 인식 task** 와 같이 단어별로 수행해야 하는 task에 활용할 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd9a6873-816e-4cfd-9a2a-e04907208cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.pooler_output.shape)\n",
    "# [3: 입력문장수,  768: 특징값의개수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6ba4a94-96d2-43c9-94d3-44f01d050f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a206af7-a0c1-4025-804d-8012b9eae71c",
   "metadata": {},
   "source": [
    "### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 단일 vector 표현을 말한다.\n",
    "    - BERT는 \\[CLS\\] 토큰(문장시작)의 출력 임베딩을 사용해서 생성한다.\n",
    "        - \\[CLS\\] 토큰은 입력 시퀀스의 맨 앞에 추가되며, 문장의 전체적인 의미를 얻기 위해 설계된 특수 토큰임.\n",
    "    - pooler의 동작과정\n",
    "        1. \\[CLS\\] 토큰의 Embedding 추출: Text sequence가 BERT 모델을 통과하면 각 토큰에 대한 임베딩이 생성된다.(last_hidden_state). 이 중 첫번째 토큰인 \\[CLS\\] 토큰의 임베딩을 선택한다.\n",
    "        2. Embedding Pooling Layer로 1의 임베딩 전달: Pooling Layer는 Pooler를 생성하는 Fully Connected Layer(선형변환)와 Activation 함수(주로 Tanh를 사용한다.)를 통과시킨다.\n",
    "        3. pooler output: 2의 과정을 통과한 결과가 최종 **pooler_output**이 된다. 이 값이 **전체 입력 시퀀스에 대한 요약정보로 사용된다.**\n",
    "    - **pooler output 이용**   \n",
    "       -  이 값은 문장을 입력받아 처리하는 task(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "756d830c-6bd5-47d1-b961-03724a12d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.pooler_output.shape)\n",
    "### CLS(문장시작-2) 토큰이 변환된 Vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
