{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d23187-2b5d-4ef4-aed4-1792210d2c54",
   "metadata": {},
   "source": [
    "# [Huggingface ](https://huggingface.co)\n",
    "\n",
    "Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다. 2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\n",
    "\n",
    "## 주요 서비스\n",
    "1. **Transformers 라이브러리**: 다양한 NLP 작업을 위한 사전 학습된 모델(Pretrained Model)들을 제공하는 라이브러리로, BERT, GPT-3, T5 등의 모델과 huggingface 생태계의 다양한 서비스들을 이용할 수있는 파이썬 기반의 오픈소스 라이브러리.\n",
    "3. **Huggingface Model Hub**: 사용자들이 자신이 훈련한 모델들을 공유 할 수 있는 git 기반 플랫폼. \n",
    "2. **Datasets 라이브러리**: 다양한 NLP 데이터셋을 손쉽게 활용할 수 있도록 하는 라이브러리로, 데이터 전처리 및 관리에 유용하다.\n",
    "4. **API 서비스**: 모델 배포를 위한 API 서비스를 제공하여, 사용자가 쉽게 모델을 배포하고 활용할 수 있도록 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fd763-772e-4ed8-99f9-ce8115756cb7",
   "metadata": {},
   "source": [
    "## Transformers 라이브러리\n",
    "\n",
    "Hugging Face의 Transformers는 자연어 처리(NLP) 분야에서 가장 널리 사용되는 라이브러리 중 하나로, 다양한 사전 학습된 모델을 제공하여 연구자들과 개발자들이 손쉽게 NLP 애플리케이션을 구축할 수 있도록 돕는다. 이 라이브러리는 언어 모델링, 텍스트 생성, 번역, 감성 분석, 질문 응답 시스템과 같은 다양한 NLP 작업을 수행할 수 있도록 설계되었다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "1. **광범위한 모델 지원**: BERT, GPT-3, RoBERTa, T5 등 다양한 최첨단 모델들을 지원한다. 이러한 모델들은 텍스트 분류, 감성 분석, 질문 답변, 텍스트 생성 등 다양한 NLP 작업에 활용될 수 있다.\n",
    "2. **파이토치(PyTorch)와 텐서플로우(TensorFlow) 호환**: Transformers 라이브러리는 두 가지 주요 딥러닝 프레임워크와 호환되어, 사용자가 자신의 선호에 따라 선택하여 사용할 수 있다. \n",
    "3. **사용자 친화적인 인터페이스**: 간단하고 직관적인 API를 통해 복잡한 모델을 쉽게 불러오고 사용할 수 있다. 이를 통해 빠르게 프로토타입을 만들고 실험할 수 있다.\n",
    "4. **모델 허브**: HuggingFace Model Hub를 통해 공유되는 수많은 모델들을 Transformers 라이브러리를 이용해 다운받아서 추론까지 쉽게 사용할 수있다.\n",
    "5. **자동화된 파이프라인**: 파이프라인(pipeline) API를 사용하면 몇 줄의 코드로 다양한 NLP 작업을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers 를 이용해 Backbone 사용\n",
    "\n",
    "## Transformers 설치\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817d6ef-7239-4738-89c1-1d6a64d8a8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩한다.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장된다.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용한다.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "\n",
    "1. **AutoModel**\n",
    "   - 주어진 모델 이름에 맞는 사전 학습된 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델을 로드한다.\n",
    "\n",
    "2. **AutoTokenizer**\n",
    "   - 해당 모델에 적합한 토크나이저를 자동으로 로드한다.\n",
    "   - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드한다.\n",
    "\n",
    "3. **AutoConfig**\n",
    "   - 모델의 설정(config)을 자동으로 로드한다. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델을 생성할 수있다.\n",
    "   - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "4. **AutoModelForSequenceClassification**\n",
    "   - 시퀀스(Text) 분류 작업을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "5. **AutoModelForQuestionAnswering**\n",
    "   - 질문-응답 작업을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "6. **AutoModelForTokenClassification**\n",
    "   - 토큰 분류 작업(예: 개체명 인식)을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffab593-17fa-4676-922d-336397744e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgmyh\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer))\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(model))\n",
    "conf = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(conf))\n",
    "model2 = AutoModel.from_config(conf)\n",
    "print(type(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe70881-f8e1-4eff-b49e-ccc0a8200dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556a051d064d4b7d8ccd89b6e222dfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgmyh\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kgmyh\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26e0eacd714480bbbeba6a6934c298e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n"
     ]
    }
   ],
   "source": [
    "gpt_model = AutoModel.from_pretrained('openai-community/gpt2')\n",
    "print(type(gpt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 한글 텍스트 학습 모델\n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저 모델 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc7e8c2-dc3d-4f39-84a7-39e6bbc38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")  # BertTokenizer: WordPiece 방식 토크나이저\n",
    "model = AutoModel.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5087a1-04bb-4b36-912f-c1c5bdd77101",
   "metadata": {},
   "source": [
    "### 입력값 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b46b85a-8123-4108-a087-435d8c1d9218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2, 15830,     5,     3,     0,     0,     0,     0,     0,     0],\n",
       "        [    2,  9909,  2483,  2688,  4755,  8186,    32,     3,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  batch_encode_plus 와 동일 \n",
    "### call함수 호출\n",
    "features = tokenizer(\n",
    "    sentences,\n",
    "    max_length=10,\n",
    "    padding='max_length',  #`True` or `'longest'`, `'max_length'`, `False` or `'do_not_pad'`(default)\n",
    "    return_tensors=\"pt\", # np: 넘파이, tf: TF, pt: pytorch\n",
    ")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc7c8a6c-3a62-4bde-9e0b-b6f661cee605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 15830,     5,     3,     0,     0,     0,     0,     0,     0],\n",
      "        [    2,  9909,  2483,  2688,  4755,  8186,    32,     3,     0,     0]])\n",
      "--------------------\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "--------------------\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# input_ids: 워드피스 토큰화 -> token index로 변환한 결과\n",
    "pprint(features['input_ids']) # 2: 문장시작(CLS), 3: 문장 끝(SEP), 0: 패딩(PAD)  표현하는 special token,  (max_length=10으로 했으므로 token idx가 10개로 구성됨.)\n",
    "print(\"-\"*20)\n",
    "# attention mask: 일반 토큰인지 패딩인지 mask 행렬. padding 아닌 token => 1, padding인 token => 0 \n",
    "pprint(features['attention_mask'])\n",
    "print(\"-\"*20)\n",
    "# token_type_ids: segment 정보 (token pair일때 첫번째는 0, 두번째는 1로 채워진다. -> 이 둘을 하나의 tokens로 묶어서 반환하는데 그 구분-https://hyen4110.tistory.com/89 확인) 한개 문장일 경우 모두 0이된다.\n",
    "pprint(features['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f357f6-73ce-4a1d-9fce-84424b642d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"안녕하세요. 저는 홍길동입니다.\")  # -> [2, 19017, 8482, 3]\n",
    "tokenizer.decode([2, 19017, 8482, 3]) # -> [CLS] 안녕하세요 [SEP]\n",
    "# result = tokenizer.batch_encode_plus(sentences)# 한번에 여러개\n",
    "print(type(result))\n",
    "result['input_ids']  # list - 문장 -> 토큰 id 리스트\n",
    "result['token_type_ids'] # pair 일때 0번, 1번 어떤 문장의 토큰인지\n",
    "result['attention_mask']  # 토큰인지 패딩인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aaa025-17c9-46ad-b708-f2729cb2a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.get_vocab()\n",
    "# tokenizer.vocab_size\n",
    "# tokenizer.convert_ids_to_tokens(2)\n",
    "# tokenizer.convert_tokens_to_ids(\"내일\")\n",
    "# tokenizer.convert_tokens_to_string([\"내일\", \"또\", \"봐\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f1c111-09d8-42f4-b2e6-1467b02dc671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', '[CLS]')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[3], tokenizer.ids_to_tokens[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 문장수준 Vector생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f0eaf52-43ed-458e-97b8-4835b183ef70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "240b332b-0827-4388-a394-ede6cd1ee213",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**features) # 'input_ids', 'token_type_ids', 'attention_mask' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "054bb72f-f5bd-49cf-a98a-e32b3222f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393e7b6-0b57-498e-a4c2-27782aa1f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'last_hidden_state' : rnn의 outputs 같이 모든 토큰에 대한 출력 feature\n",
    "# 'pooler_output' : rnn의 hidden_state 같이 마지막 token에 대한 처리값. 입력 문장, 텍스트에 대한 단일 vector 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0f2a839-9fa3-4c22-b3a5-21bc2f7e6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)  # feature값\n",
    "# [3:입력문장수, 10:토큰수-max_length설정한 것, 768: vector] -> 10개 토큰이 768개 features로 변환된 것. \n",
    "##### 이것은 token별 vector가 나오므로 **개체명 인식 task** 와 같이 단어별로 수행해야 하는 task에 활용할 수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a206af7-a0c1-4025-804d-8012b9eae71c",
   "metadata": {},
   "source": [
    "### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 단일 vector 표현을 말한다.\n",
    "    - BERT는 \\[CLS\\] 토큰(문장시작)의 출력 임베딩을 사용해서 생성한다.\n",
    "        - \\[CLS\\] 토큰은 입력 시퀀스의 맨 앞에 추가되며, 문장의 전체적인 의미를 얻기 위해 설계된 특수 토큰임.\n",
    "    - pooler의 동작과정\n",
    "        1. \\[CLS\\] 토큰의 Embedding 추출: Text sequence가 BERT 모델을 통과하면 각 토큰에 대한 임베딩이 생성된다.(last_hidden_state). 이 중 첫번째 토큰인 \\[CLS\\] 토큰의 임베딩을 선택한다.\n",
    "        2. Embedding Pooling Layer로 1의 임베딩 전달: Pooling Layer는 Pooler를 생성하는 Fully Connected Layer(선형변환)와 Activation 함수(주로 Tanh를 사용한다.)를 통과시킨다.\n",
    "        3. pooler output: 2의 과정을 통과한 결과가 최종 **pooler_output**이 된다. 이 값이 **전체 입력 시퀀스에 대한 요약정보로 사용된다.**\n",
    "    - **pooler output 이용**   \n",
    "       -  이 값은 문장을 입력받아 처리하는 task(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "756d830c-6bd5-47d1-b961-03724a12d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.pooler_output.shape)\n",
    "### CLS(문장시작-2) 토큰이 변환된 Vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
