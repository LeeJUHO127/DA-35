{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de11e1a-c190-4432-9fcd-2a604abcd715",
   "metadata": {},
   "source": [
    "# Transformer \n",
    "\n",
    "- [Attention Is All You Need 2017](https://arxiv.org/pdf/1706.03762) 논문에서 제시한 모델로 RNN모듈을 사용하지 않고 **Attention 모듈로만 구성한 Encoder-Decoder 구조의 seq2seq 모델**이다.\n",
    "    - RNN → RNN + attention → Transformer\n",
    "- Transformer 이후 자연어 관련 pretrained 모델들은 대부분 Transformer의 Encoder나 Decoder를 변형하거나 단순히 층을 깊게 쌓는 형태로 구현하여 성능을 올렸다.\n",
    "    - Google의 BERT(Bidirectional Encoder Representations from Transformers)는 Transformer의 Encoder를 이용해 구현했다.\n",
    "    - OpenAI의 GPT(Generative Pre-trained Transformer)는 Transformer의 Decoder를 이용해 구현했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b40ae2-ac88-4572-bda4-0752f5eb14a6",
   "metadata": {},
   "source": [
    "## Seq2Seq의 문제와 Transformer\n",
    "- Seq2Seq 의 문제\n",
    "    - 장기기억 소실의 문제.(long term dependency problem)\n",
    "        - seq가 길어지면 초기 timestep의 정보가 점점 소실된다.\n",
    "    - 병렬처리를 못해 연산 효율이 떨어진다.\n",
    "        - time step 별로 순서대로 처리해야 하기 때문에 병렬처리가 안된다.\n",
    "- **장기기억 소실문제는** seq2seq에 **Attention mechanism**을 적용해 해결할 수 있다.\n",
    "  \n",
    "![img](figures/transformer_advantage.gif)\n",
    "- Transformer는  encoder와 decoder에서 RNN을 제거하고 **Attention 모듈로 변경하여 병렬 처리가 가능**하도록 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d1b48-938f-4ab4-935e-3af1c0928ac4",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "- Encoder - Decoder  구조\n",
    "- Encoder와 Decoder가 **self-attention 모듈**로 구성된다. 그래서 RNN을 쓰지 않고 입력 sequence와 출력 sequence를 처리한다.\n",
    "- **Positional Encoding**\n",
    "    - RNN을 사용하지 않기 때문에 입력 sequence의 토큰들에 순서정보가 없다. 그 순서정보를 Embedding에 추가하는 역할.\n",
    "    - 문장을 구성하는 각 토큰들의 embedding vector에 그 토큰 순서 정보를 가지는 positional encoding 값을 더해준다.\n",
    "    - Positional Encoding 값은 sine/cosin 주기함수를 이용해 계산한다.\n",
    "\n",
    "![img](figures/transformer_architecture2.png)\n",
    "\n",
    "\\[transformer의 구조 1\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713c7bb-49ec-4980-839f-3bb8c5e562cb",
   "metadata": {},
   "source": [
    "### ![img](figures/transformer_architecture1.png)\n",
    "\n",
    "\\[Encoder와 Decoder의 구조\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5d460-ad2f-460f-a617-a328bbc6d150",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "- Attention을 자기 자신한테 적용한다는 의미\n",
    "    - seq2seq 의 attention은 decoder의 sequence가 encoder의 hidden state에 attention을 취한다.\n",
    "    - self attention은 attention을 자기 자신에게 취한다. \n",
    "- Self-attention은 한 문장에서 하나의 단어가 다른 단어들과 어떤 연관성을 가지는 지를 찾는다.\n",
    "\n",
    "![img](figures/transformer_self-attention.png)\n",
    "\n",
    "예를 들어 \"The animal didn't cross the street because it was too tired.\" 라는 문서에서 `it`을 encoding 한려고 한다. `it`을 이 문서의 문맥에 맞게 embedding하기 위해서는 이 문장내에서 어떤 의미로 쓰였는지 알아야 한다. 그것을 알기 위해서는 `it` 이 가리키는 것이 어떤 것인지 알아야 한다. 그것을 다른 문서에서 찾는 것이 아니라 `it` 있는(대상 단어가 있는) 문서에서 다른 단어들과의 연관성에서 찾는다. 그래서 **의미를 파악하려는 문서와 그 의미를 찾는 문서가 동일하기 때문에 self-attention**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5498c014-90d9-48cc-9a3c-3e2cd1a99970",
   "metadata": {},
   "source": [
    "### Self Attetion 과정\n",
    "- 같은 문장내에서 단어들 간의 관계를 고려하는 것.\n",
    "  \n",
    "![self-attention](figures/transformer_self_attention.png)\n",
    "\n",
    "- **수식**\n",
    "\\begin{align}\n",
    "&\\text{Attention(Q,K,V)} = \\text{softmax}(\\cfrac{QK^T}{\\sqrt{d_k})})V \\\\\n",
    "&\\small \\text{Q: Query, K: Key, V: Value,}\\;d_{k}: \\text{emmbeding 차원}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b08d5e-9741-480f-8f06-2ed77a2cbc6d",
   "metadata": {},
   "source": [
    "1. Query, Key, Value 생성\n",
    "    - **Query**: Embedding vector를 구할 대상 (ex: it)\n",
    "    - **Key**: Query와 연관성있는 단어들을 계산할 대상 (ex: 위 그림의 왼쪽 문서 토큰들)\n",
    "    - **Value**: Query와 Key를 이용해 찾은 attention weight를 적용해 Attention value를 찾을 대상. \n",
    "    - Self attention은 Query, Key, Value 모두 입력 Sequence(X)로 부터 생성한다.\n",
    "        - X에 각각다른 Weight를 내적하여 만든다.\n",
    "       - $Q=X\\cdot W_q$ , $K=X\\cdot W_k$ , $V=X\\cdot W_v$\n",
    "![img](figures/transformer_query_key_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aae5c0-949f-48a5-bed6-43166ec6c0c2",
   "metadata": {},
   "source": [
    "2. **Attention Score**\n",
    "    - Query 와 Key를 내적(dot product)하여 유사도를 계산한다. embedding vector의 차원의 제곱근으로 나눠서 정규화한다.\n",
    "    - Scaled Dot Product Attention\n",
    "   $$\n",
    "   \\text{Attention Score} = \\cfrac{Q\\cdot K^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "![img](figures/transformer_query_key_matmulpng.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef626d8-437a-4f44-baf0-30dadf786a4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "4. **Attention Weight(Distribution)**\n",
    "   - 위에서 계산된 Attention score에 softmax를 적용해 0 ~ 1 사이 비율로 바꾼다.\n",
    "     \n",
    "   $$\n",
    "    \\text{Attention Weight} = softmax(\\text{Attention Score})\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451330a8-fada-4326-8072-e8e42e799fd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "5. **Attention value**\n",
    "   - 최종 attention 연산의 결과로 **Attention weight를 Value에 내적**해서 Attention Value를 만든다.\n",
    "   - Attention은 Attention value값을 **입력 sequence의 context vector**가 된다.\n",
    "   $$\n",
    "   \\text{Attention Value} = \\text{Attention Weight}\\cdot\\text{Value}\n",
    "   $$ \n",
    "![img](figures/transformer_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae8ca3-2797-42ce-929f-f525cfcc2ee1",
   "metadata": {},
   "source": [
    "# Multi-Head attention\n",
    "- Multi-Head Attention 모듈에서는 여러 개의 Self-Attention Layer을 사용해 입력 sequence 에 대한 여러 feature들을 추출한다.\n",
    "- 여러 Self attention layer들이 출력한 결과들을 합쳐서(concatenate) Linear Layer를 통과시켜 shape을 맞춘 뒤 최종 output context vector를 만든다.\n",
    "\n",
    "![img](figures/transformer_multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a6601-e563-467f-883b-c19526eaca9d",
   "metadata": {},
   "source": [
    "# Masked self Attention\n",
    "- 입력 sequence 가 순서대로 입력되어 처리 되는 경우 i번째 입력 단어의 경우 그 이후의 단어는 모르는 상태이다. 그러나 attention은 입력된 모든 토큰을 한번에 처리해버리기 때문에 attention score를 구할 수있게 된다. 이것은 주어진 토큰(i번째)이 미래시점의 입력토큰(i+1 이후 번째)과의 유사도를 계산한 것이 된다. 이 문제를 해결하기 위해 i번째 토큰에 대한 attention score는 i번째 까지의 토큰들과 계산하도록 한다.\n",
    "- Attention Score를 Softmax 계산하기 전에 적용한다.\n",
    "- Attention  계산시 선택적으로 mask를 사용하면 Masked self attention이 된다. (Masked Multi-Head Attention)\n",
    "\n",
    "![img](figures/transformer_masked_self_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ad3251-5c3d-400e-a40d-0bee9f6db8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1.0, -torch.inf, -torch.inf, -torch.inf])\n",
    "a.softmax(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
