{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "source": [
    "# Attention mechanism \n",
    "\n",
    "- Seq2Seq 모델의 문제점\n",
    "    - Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 **하나의 고정된 context vector**에 압축하여 Decoder로 전달 한다. Decoder는 이 context vector를 이용해서 출력 시퀀스를 만든다.\n",
    "    - 하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 정보 손실이 발생한다.\n",
    "    - Decoder에서 출력 시퀀스를 생성할 때 동일한 context vector를 기반으로 한다. 그러나 각 생성 토큰마다 입력 시퀀스에서 참조해야 할 중요도가 다를 수 있다.\n",
    "\n",
    "- **Attention Mechanism 아이디어**\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder의 입력 문장(context vector)을 다시 참고 하자는 것. 이때 전체 입력 문장의 단어들을 동일한 비율로 참고하는 것이 아니라, Decoder가 해당 시점(time step)에서 예측해야할 단어와 연관이 있는 입력 부분을 좀 더 집중(attention)해서 참고 할 수 있도록 하자는 것이 기본 아이디어이다.\n",
    "\n",
    "- 다양한 Attention 종류들이 있다.\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다 Encoder의 입력 문장의 어느 부분에 더 집중(attention) 할지를 어떻게 계산하느냐에 따라 다양한 attention 방식이 있다.\n",
    "    -  `dot attention - Luong`, `scaled dot attention - Vaswani`, `general  attention - Luong`, `concat  attention - Bahdanau` 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fe5670-af83-4f9d-9198-c9ebc044ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('datasets/ChatbotData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdf3b9a-1404-4442-9b4c-55e2099eb0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 11823, 23646)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_texts = list(df['Q'])\n",
    "answer_texts = list(df['A'])\n",
    "all_texts = list(question_texts + answer_texts)\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c2869f-9d7c-4be4-ac64-b2b42b8ff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000\n",
    "\n",
    "min_frequency = 1\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token='[UNK]')\n",
    ")\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = vocab_size, \n",
    "    min_frequency = min_frequency,\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"],\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096e023b-5ca2-4f9a-b9e8-98a8cef2c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26034\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "e = tokenizer.encode(\"안녕하세요. 즐거운 하루되세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71371d9-4440-4307-835e-3515b319dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '.', '즐거운', '하루', '되', '세요']\n",
      "[5091, 9, 2283, 1435, 323, 1245]\n"
     ]
    }
   ],
   "source": [
    "print(e.tokens)\n",
    "print(e.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493cef9e-2c2d-4f37-ae0d-30642855e72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 . 즐거운 하루 되 세요'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([5091, 9, 2283, 1435, 323, 1245])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e67477f-96a6-40a4-8f36-9efd0694b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "os.makedirs('models/tokenizers', exist_ok=True)\n",
    "tokenizer_path = 'models/tokenizers/chatbot_bpe.json'\n",
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=3, max_length=25):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=3: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=25:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_length = max_length  # 문장 구성 토큰의 최대개수.\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]') # 문장 시작 토큰\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]') # 문장 끝 토큰\n",
    "        \n",
    "        self.question_squences = [] #[[200, 300, 250, ..], ....]\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            # q, a: string \n",
    "            q_tokens = self.__process_sequence(q) # str -> [200, 300, 250, ...]\n",
    "            a_tokens = self.__process_sequence(a)\n",
    "            # 토큰수가 min_length 초과인 질문-답변만 list에 추가.\n",
    "            if len(q_tokens) > min_length and len(a_tokens) > min_length:\n",
    "                self.question_squences.append(q_tokens)\n",
    "                self.answer_sequences.append(a_tokens)\n",
    "            \n",
    "\n",
    "    def __add_special_tokens(self, token_sequence): #[20, 300, 25]\n",
    "        \"\"\"\n",
    "        max_length 보다 token 수가 많은 경우 max_length-1에 맞춰 뒤를 잘라낸다. (-1: EOS 추가하기 위해.)\n",
    "        token_id_list 에 [EOS] 토큰 추가. \n",
    "        \"\"\"\n",
    "\n",
    "        token_sequence = token_sequence[:self.max_length-1]\n",
    "        token_sequence.append(self.EOS)\n",
    "        \n",
    "        return token_sequence\n",
    "    \n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 token_id 리스트(list)로 변환 후 반환\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_squences)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        # self.question_squences[index] -> [token수] => [token수, 1]\n",
    "        return torch.LongTensor(self.question_squences[index]).unsqueeze(1), torch.LongTensor(self.answer_sequences[index]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d68420-66c6-4864-82bc-c4d85c691a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25\n",
    "MIN_LENGTH = 3\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34539632-354d-4785-a4f0-810e2a44d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3752b6-300c-46d7-a227-024dc1f7e24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[13306],\n",
       "         [  369],\n",
       "         [    4],\n",
       "         [    3]]),\n",
       " tensor([[4998],\n",
       "         [ 379],\n",
       "         [3868],\n",
       "         [   9],\n",
       "         [   3]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ff66f-2f24-4f8b-a864-ce97978c4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장씩 학습 -> DataLoader는 생성하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cde8b86-320a-4127-a925-9ad3960eba5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13306],\n",
       "        [  369],\n",
       "        [    4],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af63e-d915-4832-840e-44a07a53cdad",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq 모델과 동일 한 구조\n",
    "    - 이전 코드(seq2seq)와 비교해서 forward()에서 입력 처리는 token 하나씩 하나씩 처리한다. \n",
    "\n",
    "![encoder](figures/attn_encoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs # 총단어개수 \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Embedding Layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        #GRU Layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=False      # Encoder도 토큰 하나씩 순서대로 입력받아 처리. 단방향.\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        한개 질문의 token 한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        batch=1       seq_len=1\n",
    "        Parameter\n",
    "            x: 한개 토큰. shape-[1] ex) [3000]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size] \n",
    "                                                              (layer수(1) * 단방향(1), batch(1), hidden_size)\n",
    "        Return\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        x = self.embedding(x).unsqueeze(0)  # (1, embedding_dim) -> (1**, 1, embedding_dim)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        \"\"\"\n",
    "        처음 입력할 hidden_state. \n",
    "        값: 0\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f0e1f7-e5a4-4da8-8ce5-dcc8171d7ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13306],\n",
       "        [  369],\n",
       "        [    4],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "210f793c-e870-414e-beba-f270f1656d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13306])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce6f89f-b8d1-4f28-baa5-244c11e56b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 1, 100])\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "embedding_dim = 100\n",
    "hidden_size = 256\n",
    "\n",
    "# Encoder 처리 로직\n",
    "el = nn.Embedding(vocab_size, embedding_dim)\n",
    "gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "print(a[0].shape)\n",
    "x = el(a[0])  # 입력: 한개 문장의 한개 토큰씩 입력.\n",
    "print(x.shape)  # 1개, 100 Embedding vector\n",
    "x = x.unsqueeze(0) # gru input shape에 맞추기 위해서. (seq_len, batch_size, input_size)\n",
    "print(x.shape)   \n",
    "# print(x)\n",
    "init_hidden = torch.zeros(1, 1, hidden_size) # ==> encoder.init_hidden()\n",
    "o1, h1 = gru(x, init_hidden)\n",
    "print(o1.shape, h1.shape)\n",
    "\n",
    "#  두번째 timestep grup\n",
    "o2, h2 = gru(x, h1) # a[1] 토큰 embedding 값, 첫번째 timestep의 hidden state 처리결과."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112940e-3f9a-42de-89df-e5b686a7856f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af5359-f8c8-4d9c-b6ec-b229d4465180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52754234-031c-48b1-adbe-120cd607b9fb",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "![seq2seq attention outline](figures/attn_seq2seq_attention_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n",
    "  \n",
    "![attention decoder](figures/attn_decoder-network_graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "source": [
    "### Attetion Weight\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n",
    "  \n",
    "![Attention Weight](figures/attn_attention_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "source": [
    "### Attention Value\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector. \n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n",
    "\n",
    "![attention value](figures/attn_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어(현재 timestep의 input)** 와 **Encoder output에 attention이 적용된 값** 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 Linear Layer함수+ReLU를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n",
    "\n",
    "![rnn](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f57bb1-13c8-43ba-a4b8-58362fc9ca49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, \n",
    "                  dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        \n",
    "        ## Attention Score를 만드는 Linear Layer. 이 결과를 Softmax를 통과시키면 Attention weight(distribution)\n",
    "        self.attn = nn.Linear(\n",
    "            in_features=hidden_size+embedding_dim, # 이전 timestep hidden_state + 현재입력 embedding vector\n",
    "            out_features=max_length, # encoder hidden_state들에 곱해질 값을 출력. hidden_state의 개수==MAX_LENGTH\n",
    "        )\n",
    "        ## Attention Value + embedding vector를 입력으로 받아서 GRU 입력 크기로 줄여주는 역할.\n",
    "        self.attn_combine = nn.Linear(\n",
    "            hidden_size + embedding_dim, #attention value 와 embedding vector concat 한 크기.\n",
    "            hidden_size,   # GRU input_size\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_size,   # 입력\n",
    "            hidden_size=hidden_size # hidden state개수(Node/Unit/Neuron 수)\n",
    "        )\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=hidden_size, \n",
    "            out_features=num_vocabs  # 각 단어가 생성할 단어일 확률 -> 총 단어의 개수.\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰 id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: 다음 단어일 단어별 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                attention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        # Embedding\n",
    "        ## input shape: [1] - 한문장의 단어 1개 (batch-1, seq_len-1)\n",
    "        ## 출력 shape:  [1, embedding_dim]  -> seq_length dim을 추가. [1, 1, embedding_dim]\n",
    "        ######################################\n",
    "        embedding = self.embedding(x).unsqueeze(0)\n",
    "\n",
    "        ################################################\n",
    "        # Attention Weight \n",
    "        ##  embedding dropout 적용\n",
    "        ##  hidden_state(이전 step) 와 embedding vector를 합치기.\n",
    "        ##  attn(Linear) 통과시켜서 size를 줄이기.  ==> Attention Score\n",
    "        ##  softmax(Attention Score) ==> Attention weight\n",
    "        ################################################\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        attn_in = torch.concat((embedding[0], hidden[0]), dim=1) #dim=1  -----> 방향으로 합친다.\n",
    "        # (1, embedding_dim)  (1, hidden_size)  = (1, em + hs)\n",
    "        attention_score = self.attn(attn_in)  # (1, max_length)\n",
    "        attention_weights = nn.Softmax(dim=-1)(attention_score) #attention_score.sofmax(dim=-1)\n",
    "\n",
    "        ################################################\n",
    "        # Attention Value (Context Vector)계산\n",
    "        ## attention_weight x encoder의 output들\n",
    "        ################################################\n",
    "\n",
    "        attention_value = torch.bmm(\n",
    "            attention_weights.unsqueeze(0), # (1, max_length)              => (1, 1, max_legnth)\n",
    "            encoder_outputs.unsqueeze(0)   # (max_length, hidden_size) =>(1, max_length, hidden_size)\n",
    "        )  # attention_value shape:   (1, 1, hidden_size)\n",
    "        # bmm() - batch_matrix_multiply (batch 행렬곱) => 1, 2 축 행렬끼리 행렬곱\n",
    "\n",
    "        ################################################################\n",
    "        # GRU에 입력할 값을 생성 -> GRU\n",
    "        ## attention_value 와 decoder 입력 embedding vector를 concat\n",
    "        ## concat 한 것을 GRU 입력 크기(hidden_size)  에 맞도록 Linear를 이용해 조절.\n",
    "        ## ReLU로 비선형 처리\n",
    "        ## GRU에 입력 (윗단계에서 처리한 값,  이전 timestep의 hidden state)\n",
    "        ################################################################\n",
    "        attn_combine_in = torch.concat(\n",
    "            [attention_value[0], embedding[0]],  # [1, hidden_size] + [1, embedding_dim] => [1, h_s + e_d]\n",
    "            dim=-1\n",
    "        )\n",
    "        gru_in = self.attn_combine(attn_combine_in) # 출력: [1, hidden_size]\n",
    "        gru_in = gru_in.unsqueeze(0)  # [1, 1, hidden_size]\n",
    "        gru_in = nn.ReLU()(gru_in)\n",
    "        output, hidden = self.gru(gru_in)  \n",
    "        ################################################################\n",
    "        # output을 classifier에 입력해서 단어 추론(분류)\n",
    "        ################################################################\n",
    "        last_output = self.classifier(output[0])   #output: [1, 1, hidden] -> [1, hidden] -Linear-> [1, num_vocabs]\n",
    "\n",
    "        # return: last_output[0] [1, num_vocab] => [num_vocab]\n",
    "        #          hidden: gru의 hidden state (1, 1, hidden_size)\n",
    "        #          attention_weight:  [1, max_length]\n",
    "        return last_output[0], hidden, attention_weights\n",
    "\n",
    "    def initHidden(self, device):\n",
    "        # Decoder 첫번 timestep에 넣어줄 이전 timestep의 hidden state 값. (value: 0)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "74978379-d6e3-4edd-bcf0-bc495f8be00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "hidden_size=32\n",
    "embedding_dim = 100\n",
    "dropout_p = 0.1\n",
    "max_length = 25\n",
    "\n",
    "decoder = AttentionDecoder(\n",
    "    num_vocabs=vocab_size, hidden_size=hidden_size, embedding_dim=embedding_dim,\n",
    "    dropout_p=dropout_p, max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8be86aa0-6edd-47e0-a9f6-0e19c7c7c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4998]) torch.Size([25, 32]) torch.Size([1, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "x = b[0]\n",
    "encoder_outputs = torch.randn(max_length, hidden_size)\n",
    "hidden = decoder.initHidden(device)\n",
    "print(x, encoder_outputs.shape, hidden.shape)\n",
    "\n",
    "o, h, aw = decoder(x, hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cbf998c2-df47-40cd-9aef-ffcd4ea278f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 32]), torch.Size([1, 25]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape\n",
    "o.max(dim=0)\n",
    "tokenizer.id_to_token(23597)\n",
    "h.shape, aw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f005b-c60f-4631-a235-f974fd63ea1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d869b-2956-4219-aa0a-91da04a00b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab7c43d-3691-4723-8051-7bc0a8a4a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5eb486a3-09bb-4d05-ad2d-149b355f6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8761"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "75080eb6-da0e-4c86-998e-2dd8405e5a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_TOKEN = dataset.tokenizer.token_to_id(\"[SOS]\")\n",
    "EOS_TOKEN = dataset.tokenizer.token_to_id(\"[EOS]\")\n",
    "SOS_TOKEN, EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "166baaa8-adcc-49a3-b0c7-10417584d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개 question-answer set 학습\n",
    "def train_fn(input_tensor, target_tensor, encoder, decoder, \n",
    "              encoder_optimizer, decoder_optimizer, \n",
    "              loss_fn, device, max_length=MAX_LENGTH, teacher_forcing_ratio=0.5):\n",
    "    # input_tensor, target_tensor : 1개 질문-답 쌍.\n",
    "    \n",
    "    # 옵티마이저 초기화(gradient값 초기화.)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #################\n",
    "    # Encoder 학습\n",
    "    #################\n",
    "    encoder_hidden = encoder.init_hidden(device)  # 첫 timestep에 입력할 hidden state값 조회.\n",
    "    # 입/출력 문장(질문-답변) 의 토큰(단어) 수 조회\n",
    "    input_length = input_tensor.shape[0]\n",
    "    output_length = target_tensor.shape[0]   \n",
    "\n",
    "    # encoder의 hidden state들을 저장할 변수.  hidden_size가 단어 개수만 나옴.\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0.0\n",
    "    ########################\n",
    "    # 질문 문장을 구성하는 단어(토큰)들을  하나씩 Encoder에 입력해서 hidden_state(특성)를 출력\n",
    "    #########################\n",
    "    for ei in range(input_length):  #[1, 30, 20, 780, 60...]\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], # 질문 문장의 ei 번째 토큰_id\n",
    "                                                                  encoder_hidden\n",
    "                                                                  )\n",
    "        # encoder_output을 encoder_outputs에 추가\n",
    "        encoder_outputs[ei] = encoder_output\n",
    "\n",
    "    #########################\n",
    "    # Decoder 처리\n",
    "    ############################\n",
    "\n",
    "    # Decoder 초기 입력 토큰: [SOS]\n",
    "    decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "    # Decoder 첫 timestep에 입력할 hidden state \n",
    "    decoder_hidden = encoder_hidden # Encoder의 마지막 timestep의 hidden state\n",
    "\n",
    "    is_teacher_forcing  = True if random.random() < teacher_forcing_ratio else False  # Teacher Forcing을 적용할 지 여부.\n",
    "    # True:  decoder 입력으로 정답 토큰.  False:  이전 timestep에서 decoder 생성한 토큰을 입력\n",
    "    ### 한 토큰씩 처리\n",
    "    for di in range(output_length):\n",
    "        # decoder_output: 전체 단어의 다음단어일 확률값 : shape - [총단어수]\n",
    "        # decoder_hidden: gru가 출력한 hidden_state: shape - [1, 1, hidden_size]\n",
    "        # attention_weight:  [1, max_length]\n",
    "        decoder_output, decoder_hidden, attention_weight = decoder(decoder_input,   # x_t\n",
    "                                                                                         decoder_hidden, # h_{t-1}\n",
    "                                                                                         encoder_outputs)# encoder의 모든 hidden\n",
    "        ## Loss 계산\n",
    "        loss = loss + loss_fn(\n",
    "            decoder_output.unsqueeze(0), # 모델이 예측한 값-단어별 확률 [총단어수] -> [1, 총단어수]\n",
    "            target_tensor[di]\n",
    "        )\n",
    "        ## decoder _input 만들기 (다음 timestep의 입력) is_teacher_forcing: True-정답, False: 추론\n",
    "        if is_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            # 모델추론 토큰   \n",
    "            output_token = decoder_output.argmax(dim=-1).unsqueeze(0)\n",
    "            decoder_input = output_token.detach() # detach(): 역전파할때 grad 계산 대상에 빼겠다.\n",
    "        \n",
    "        if decoder_input == EOS_TOKEN: # 다음에 입력할 값이 문장의 끝 이라면 멈춤.\n",
    "            break\n",
    "\n",
    "    ######### Encoder/Decoder 순전파 처리 #############\n",
    "    # 학습\n",
    "    \n",
    "    # loss 기반으로 gradient계산\n",
    "    loss.backward()\n",
    "    # 파라미터 update\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    ## loss 평균 리턴. (loss = decoder 생성 문자별 loss가 누적)\n",
    "    return loss.item()/output_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6824a9fb-1592-44f4-8a74-6c5098bd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 학습을 처리하는 함수.\n",
    "def train_iterations(encoder, decoder, n_iters, dataset, device, log_interval=1000, learning_rate=0.001):\n",
    "    # n_iters:  몇 step 학습할지 int. (1step당 1 문장쌍.)\n",
    "    # encoder/decoder를 train 모드로 변경\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    print_loss = 0.0   # log_interval step 당 한번씩 출력할 loss. 출력후 초기화(0)\n",
    "    # 옵티마이저\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 학습시킬 데이터 조회 -> random하게 n_iters 개수만큼 추출.\n",
    "    total_data = len(dataset)\n",
    "    train_pairs = [dataset[random.randint(0, total_data-1)]  for i in range(n_iters)]\n",
    "    \n",
    "    ## loss 함수 정의\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    s = time.time()  # n_iters 만큼 반복하면서 train\n",
    "    for idx in range(n_iters):\n",
    "        train_pair = train_pairs[idx]  # 질문-답변 한쌍을 조회.\n",
    "        \n",
    "        input_tensor, target_tensor = train_pair # 튜플대입        \n",
    "        input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # train_fn() 호출\n",
    "        loss = train_fn(input_tensor, target_tensor, encoder, decoder, \n",
    "                           encoder_optimizer, decoder_optimizer, loss_fn, device)\n",
    "        print_loss += loss\n",
    "\n",
    "        ##### log_interval에 한번씩 train 로그를 출력\n",
    "        if (idx+1) % log_interval == 0:\n",
    "            print_loss_avg = print_loss / log_interval\n",
    "            print_loss = 0.0\n",
    "            print(f\"Step: {idx+1} - 진행: {idx / n_iters * 100}%, Loss: {print_loss_avg:.4f}\")\n",
    "        \n",
    "    e = time.time()\n",
    "    print(f\"총걸린시간: {e-s}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "892bb6f0-f9cd-433a-84b2-c8130f7d1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOCABS = dataset.vocab_size\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDING_DIM = 256\n",
    "DROPOUT_P = 0.1\n",
    "# N_ITERATION = 100000\n",
    "N_ITERATION = 10\n",
    "\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  num_layers=1)\n",
    "\n",
    "decoder = AttentionDecoder(num_vocabs=NUM_VOCABS, \n",
    "                           hidden_size=HIDDEN_SIZE, \n",
    "                           embedding_dim=EMBEDDING_DIM, \n",
    "                           dropout_p=DROPOUT_P, \n",
    "                           max_length=MAX_LENGTH)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6fde0e0f-ae81-45ff-b685-866d94abe63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3 - 진행: 20.0%, Loss: 10.1194\n",
      "Step: 6 - 진행: 50.0%, Loss: 9.8212\n",
      "Step: 9 - 진행: 80.0%, Loss: 9.4678\n",
      "총걸린시간: 1.9593849182128906초\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_iterations(encoder, decoder, N_ITERATION, dataset, device, log_interval=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f416fa1-0a38-4a60-9c02-41261fab6cb0",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f2e04-2603-4e3a-a4cb-2b6a4afd5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models/chatbot_attn_tokenizer', exist_ok=True)\n",
    "encoder_save_path = 'models/chatbot_attn_tokenizer/encoder.pt'\n",
    "decoder_save_path = 'models/chatbot_attn_tokenizer/decoder.pt'\n",
    "torch.save(encoder, encoder_save_path)\n",
    "torch.save(decoder, decoder_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d34-7308-493d-95a4-eef87c361d33",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9ed2ff49-9ceb-488f-9547-b687684187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, dataset, device, max_length=MAX_LENGTH):\n",
    "    ##  한개의 질문에 대해    질문, 정답 답변, 모델이 추정한 답변을 출력\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        # Encoder 처음 timestep에 입력할 hidden state\n",
    "        encoder_hidden = encoder.init_hidden(device=device)\n",
    "        \n",
    "        # Encoder 가 출력한 모든 hidden state값들을 저장할 tensor\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        ## decoder\n",
    "        decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []  # decoder가 추론한 단어(토큰)들을 저장할 리스트.\n",
    "        decoder_attentions = torch.zeros(max_length, max_length) # attention weight를 저장할 텐서\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, \n",
    "                                                                                    decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_attentions[di] = decoder_attention.data  \n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)  \n",
    "            # 단어별 확률 -> 가장 큰값. topv: 확률, topi: index (단어 토큰값)\n",
    "\n",
    "            if topi.item() == dataset.EOS:  # 리스트에 추가하고 다음 단어 예측을 중단.\n",
    "                decoded_words.append('[EOS]')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset.tokenizer.id_to_token(topi.item()))\n",
    "                # token_id -> token(단어:str), 리스트에 추가.\n",
    "            decoder_input = topi.squeeze().detach()  # 다음 timestep의 input\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, dataset, device, n=10):\n",
    "    \n",
    "    for i in range(n):\n",
    "    \n",
    "        x, y = random.choice(dataset)\n",
    "        \n",
    "        print('질문(정답):', dataset.tokenizer.decode(x.flatten().tolist()))\n",
    "        print('답변(정답):', dataset.tokenizer.decode(y.flatten().tolist()))\n",
    "    \n",
    "        output_words, attentions = evaluate(encoder, decoder, x.to(device), dataset, device)\n",
    "        output_sentence = ' '.join(output_words[:-1])   # [EOS] 빼기\n",
    "        \n",
    "        print('답변(예측):', output_sentence)\n",
    "        print('----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d8dc3a82-94c7-4537-81f9-82e673d9d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a-b-안녕'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-\".join([\"a\", \"b\", \"안녕\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98932528-1986-4a77-aba3-c91b905a0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = torch.load(encoder_save_path)\n",
    "# decoder = torch.load(decoder_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6081348f-d9cd-463c-ac48-e7cd06ecfb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문(정답): 왕따된 거 같아\n",
      "답변(정답): 선생님께 도움을 청해보세요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 카톡하고 싶고 자꾸 생각나고 그러면 좋아하는 건가 ?\n",
      "답변(정답): 아마 그럴 거예요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 오늘은 제 미련한 인생을 주절거려보렵니다 .\n",
      "답변(정답): 제가 다 들어드릴게요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 바람 강제이별 너무 힘들어\n",
      "답변(정답): 오히려 잘된거라 생각하세요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 좋아하는 사람이 거짓말을 하면 어떡하죠 .\n",
      "답변(정답): 속아주는 것도 사랑이죠 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 최소 썸인 듯 .\n",
      "답변(정답): 고민 하고 있으면 썸의 가능성이 높겠네요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 내가 다른 무슨 말을 하겠어\n",
      "답변(정답): 하고 싶은 말 다하세요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 여자친구가 나 사랑하는 건지 확신이 없어\n",
      "답변(정답): 당신이 그녀를 사랑하면 됐지 꼭 그녀가 당신을 사랑하는지 확신이 있어야 하는 건 아니랍니다 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 싸우면 연락 안 받는 사람 어떻게 해 ?\n",
      "답변(정답): 혼자 풀릴 때까지 놔둬야하는데 기다리는게 힘들 거예요 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 일주일간 괜찮았는데 .\n",
      "답변(정답): 갑자기 생각나기도 한답니다 .\n",
      "답변(예측): \n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder, decoder, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "15063fb2-6915-430a-9481-2e39dd64e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Attention Weight시각화를 위한 함수\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    " # plt.switch_backend('agg')\n",
    "\n",
    "plt.rcParams['font.family'] = 'malgun gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap=\"gray\")\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(range(len(input_sentence)))\n",
    "    ax.set_xticklabels(input_sentence, rotation=90)\n",
    "\n",
    "    ax.set_yticks(range(len(output_words)))\n",
    "    ax.set_yticklabels(output_words)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_show_attention(encoder, decoder, input_sentence, dataset, device):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence.to(device), dataset, device)\n",
    "    # 질문 문장 구성\n",
    "    input_sentence = ' '.join([tokenizer.id_to_token(t.item()) for t in dataset[idx][0].flatten()])\n",
    "    # 모델이 추정해서 전달한 토큰들을 문장으로 구성.\n",
    "    output_words = ' '.join(output_words)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output_words)\n",
    "    show_attention(input_sentence.split(), output_words.split(), attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6414c754-56a0-4b15-bf90-6b0e8ddd43a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3721],\n",
       "        [ 1905],\n",
       "        [12719],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f0cbbe07-77d4-44d1-bc61-c8fe823768bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 여름 빨리 지나갔으면 [EOS]\n",
      "output = [EOS]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 2100\n",
    "evaluate_and_show_attention(encoder, decoder, dataset[idx][0], dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290c137-825c-4725-8525-3cf7d8e9e677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
