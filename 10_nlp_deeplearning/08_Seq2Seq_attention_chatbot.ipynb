{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "source": [
    "# Attention mechanism \n",
    "\n",
    "- Seq2Seq 모델의 문제점\n",
    "    - Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 **하나의 고정된 context vector**에 압축하여 Decoder로 전달 한다. Decoder는 이 context vector를 이용해서 출력 시퀀스를 만든다.\n",
    "    - 하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 정보 손실이 발생한다.\n",
    "    - Decoder에서 출력 시퀀스를 생성할 때 동일한 context vector를 기반으로 한다. 그러나 각 생성 토큰마다 입력 시퀀스에서 참조해야 할 중요도가 다를 수 있다.\n",
    "\n",
    "- **Attention Mechanism 아이디어**\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder의 입력 문장(context vector)을 다시 참고한자는 것. 이때 전체 입력 문장의 단어들을 동일한 비율로 참고하는 것이 아니라, Decoder가 해당 시점(time step)에서 예측해야할 단어와 연관이 있는 입력 부분을 좀 더 집중(attention)해서 참고 할 수 있도록 하자는 것이 기본 아이디어이다.\n",
    "\n",
    "- 다양한 Attention 종류들이 있다.\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다 Encoder의 입력 문장의 어느 부분에 더 집중(attention) 할지를 어떻게 계산하느냐에 따라 다양한 attention 방식이 있다.\n",
    "    -  `dot attention - Luong`, `scaled dot attention - Vaswani`, `general  attention - Luong`, `concat  attention - Bahdanau` 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe5670-af83-4f9d-9198-c9ebc044ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('datasets/ChatbotData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf3b9a-1404-4442-9b4c-55e2099eb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_texts = list(df['Q'])\n",
    "answer_texts = list(df['A'])\n",
    "all_texts = list(question_texts + answer_texts)\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2869f-9d7c-4be4-ac64-b2b42b8ff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000\n",
    "\n",
    "min_frequency = 1\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token='[UNK]')\n",
    ")\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = vocab_size, \n",
    "    min_frequency = min_frequency,\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"],\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e023b-5ca2-4f9a-b9e8-98a8cef2c5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67477f-96a6-40a4-8f36-9efd0694b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "os.makedirs('models/tokenizers', exist_ok=True)\n",
    "tokenizer_path = 'models/tokenizers/chatbot_bpe.json'\n",
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=3, max_length=25):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=3: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=25:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_length = max_length  # 문장 구성 토큰의 최대개수.\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]')\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]')\n",
    "        \n",
    "        self.question_squences = []\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            q_tokens = self.__process_sequence(q)\n",
    "            a_tokens = self.__process_sequence(a)\n",
    "            if len(q_tokens) > min_length and len(a_tokens) > min_length:\n",
    "                self.question_squences.append(q_tokens)\n",
    "                self.answer_sequences.append(a_tokens)\n",
    "            \n",
    "\n",
    "    def __add_special_tokens(self, token_sequence):\n",
    "        \"\"\"\n",
    "        max_length 보다 token 수가 많은 경우 max_length에 맞춰 뒤를 잘라낸다.\n",
    "        token_id_list 에 [EOS] 토큰 추가. \n",
    "        \"\"\"\n",
    "\n",
    "        token_sequence = token_sequence[:self.max_length-1]\n",
    "        token_sequence.append(self.EOS)\n",
    "        \n",
    "        return token_sequence\n",
    "    \n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 token_id 리스트(list)로 변환 후 반환\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_squences)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        return torch.LongTensor(self.question_squences[index]).unsqueeze(1), torch.LongTensor(self.answer_sequences[index]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d68420-66c6-4864-82bc-c4d85c691a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25\n",
    "MIN_LENGTH = 3\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f9de1-2bf2-4ecb-99e7-7d725768086c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af63e-d915-4832-840e-44a07a53cdad",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq 모델과 동일 한 구조\n",
    "    - 이전 코드(seq2seq)와 비교해서 forward()에서 입력 처리는 token 하나씩 하나씩 처리한다. \n",
    "\n",
    "![encoder](figures/attn_encoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        질문의 token한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        Parameter\n",
    "            x: 한개 토큰. shape-[1]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size]\n",
    "        Return\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        \"\"\"\n",
    "        처음 입력할 hidden_state. \n",
    "        값: 0\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af5359-f8c8-4d9c-b6ec-b229d4465180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52754234-031c-48b1-adbe-120cd607b9fb",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "![seq2seq attention outline](figures/attn_seq2seq_attention_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n",
    "  \n",
    "![attention decoder](figures/attn_decoder-network_graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "source": [
    "### Attetion Weight\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n",
    "  \n",
    "![Attention Weight](figures/attn_attention_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "source": [
    "### Attention Value\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector. \n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n",
    "\n",
    "![attention value](figures/attn_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어(현재 timestep의 input)** 와 **Encoder output에 attention이 적용된 값** 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 Linear Layer함수+ReLU를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n",
    "\n",
    "![rnn](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n",
    "\n",
    "![img](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f57bb1-13c8-43ba-a4b8-58362fc9ca49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰 id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: 예측한 단어별 다음 단어일 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                atttention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def initHidden(self, device):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7c43d-3691-4723-8051-7bc0a8a4a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75080eb6-da0e-4c86-998e-2dd8405e5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = dataset.tokenizer.token_to_id(\"[SOS]\")\n",
    "EOS_TOKEN = dataset.tokenizer.token_to_id(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184448-56e5-4c3c-8d6c-3f8f93d45076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개 question-answer set 학습\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, \n",
    "          loss_fn, device, max_length=MAX_LENGTH, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824a9fb-1592-44f4-8a74-6c5098bd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(encoder, decoder, n_iters, dataset, device, log_interval=1000, learning_rate=0.001):\n",
    "    \n",
    "    pass\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bb6f0-f9cd-433a-84b2-c8130f7d1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOCABS = dataset.vocab_size\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDING_DIM = 256\n",
    "DROPOUT_P = 0.1\n",
    "N_ITERATION = 100000\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  num_layers=1)\n",
    "\n",
    "decoder = AttentionDecoder(num_vocabs=NUM_VOCABS, \n",
    "                           hidden_size=HIDDEN_SIZE, \n",
    "                           embedding_dim=EMBEDDING_DIM, \n",
    "                           dropout_p=DROPOUT_P, \n",
    "                           max_length=MAX_LENGTH)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde0e0f-ae81-45ff-b685-866d94abe63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(encoder, decoder, N_ITERATION, dataset, device, log_interval=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f416fa1-0a38-4a60-9c02-41261fab6cb0",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f2e04-2603-4e3a-a4cb-2b6a4afd5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models/chatbot_attn_tokenizer', exist_ok=True)\n",
    "encoder_save_path = 'models/chatbot_attn_tokenizer/encoder.pt'\n",
    "decoder_save_path = 'models/chatbot_attn_tokenizer/decoder.pt'\n",
    "torch.save(encoder, encoder_save_path)\n",
    "torch.save(decoder, decoder_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d34-7308-493d-95a4-eef87c361d33",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2ff49-9ceb-488f-9547-b687684187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, dataset, device, max_length=MAX_LENGTH):\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "        \n",
    "        encoder_hidden = encoder.init_hidden(device=device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "\n",
    "        decoder_input = torch.tensor([[0]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "            if topi.item() == dataset.EOS:\n",
    "                decoded_words.append('[EOS]')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset.tokenizer.id_to_token(topi.item()))\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, dataset, device, n=10):\n",
    "    \n",
    "    for i in range(n):\n",
    "    \n",
    "        x, y = random.choice(dataset)\n",
    "        \n",
    "        print('질문(정답):', dataset.tokenizer.decode(x.flatten().tolist()))\n",
    "        print('답변(정답):', dataset.tokenizer.decode(y.flatten().tolist()))\n",
    "    \n",
    "        output_words, attentions = evaluate(encoder, decoder, x.to(device), dataset, device)\n",
    "        output_sentence = ' '.join(output_words[:-1])   # [EOS] 빼기\n",
    "        \n",
    "        print('답변(예측):', output_sentence)\n",
    "        print('----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98932528-1986-4a77-aba3-c91b905a0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load(encoder_save_path)\n",
    "decoder = torch.load(decoder_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081348f-d9cd-463c-ac48-e7cd06ecfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_randomly(encoder, decoder, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15063fb2-6915-430a-9481-2e39dd64e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Attention 시각화를 위한 함수\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# plt.switch_backend('agg')\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap=\"gray\")\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "\n",
    "    ax.set_xticks(range(len(input_sentence)))\n",
    "    ax.set_xticklabels(input_sentence, rotation=90)\n",
    "\n",
    "    ax.set_yticks(range(len(output_words)))\n",
    "    ax.set_yticklabels(output_words)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_show_attention(encoder, decoder, input_sentence, dataset, device):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence.to(device), dataset, device)\n",
    "    input_sentence = ' '.join([tokenizer.id_to_token(t.item()) for t in dataset[idx][0].flatten()])\n",
    "    output_words = ' '.join(output_words)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output_words)\n",
    "    show_attention(input_sentence.split(), output_words.split(), attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbbe07-77d4-44d1-bc61-c8fe823768bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2100\n",
    "evaluate_and_show_attention(encoder, decoder, dataset[idx][0], dataset, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
