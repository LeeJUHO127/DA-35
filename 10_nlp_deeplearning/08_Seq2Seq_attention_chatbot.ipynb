{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "source": [
    "# Attention mechanism \n",
    "\n",
    "- Seq2Seq 모델의 문제점\n",
    "    - Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 **하나의 고정된 context vector**에 압축하여 Decoder로 전달 한다. Decoder는 이 context vector를 이용해서 출력 시퀀스를 만든다.\n",
    "    - 하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 정보 손실이 발생한다.\n",
    "    - Decoder에서 출력 시퀀스를 생성할 때 동일한 context vector를 기반으로 한다. 그러나 각 생성 토큰마다 입력 시퀀스에서 참조해야 할 중요도가 다를 수 있다.\n",
    "\n",
    "- **Attention Mechanism 아이디어**\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder의 입력 문장(context vector)을 다시 참고 하자는 것. 이때 전체 입력 문장의 단어들을 동일한 비율로 참고하는 것이 아니라, Decoder가 해당 시점(time step)에서 예측해야할 단어와 연관이 있는 입력 부분을 좀 더 집중(attention)해서 참고 할 수 있도록 하자는 것이 기본 아이디어이다.\n",
    "\n",
    "- 다양한 Attention 종류들이 있다.\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다 Encoder의 입력 문장의 어느 부분에 더 집중(attention) 할지를 어떻게 계산하느냐에 따라 다양한 attention 방식이 있다.\n",
    "    -  `dot attention - Luong`, `scaled dot attention - Vaswani`, `general  attention - Luong`, `concat  attention - Bahdanau` 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fe5670-af83-4f9d-9198-c9ebc044ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('datasets/ChatbotData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdf3b9a-1404-4442-9b4c-55e2099eb0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 11823, 23646)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_texts = list(df['Q'])\n",
    "answer_texts = list(df['A'])\n",
    "all_texts = list(question_texts + answer_texts)\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c2869f-9d7c-4be4-ac64-b2b42b8ff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000\n",
    "\n",
    "min_frequency = 1\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token='[UNK]')\n",
    ")\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = vocab_size, \n",
    "    min_frequency = min_frequency,\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"],\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096e023b-5ca2-4f9a-b9e8-98a8cef2c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26034\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "e = tokenizer.encode(\"안녕하세요. 즐거운 하루되세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71371d9-4440-4307-835e-3515b319dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '.', '즐거운', '하루', '되', '세요']\n",
      "[5091, 9, 2283, 1435, 323, 1245]\n"
     ]
    }
   ],
   "source": [
    "print(e.tokens)\n",
    "print(e.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493cef9e-2c2d-4f37-ae0d-30642855e72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 . 즐거운 하루 되 세요'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([5091, 9, 2283, 1435, 323, 1245])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e67477f-96a6-40a4-8f36-9efd0694b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "os.makedirs('models/tokenizers', exist_ok=True)\n",
    "tokenizer_path = 'models/tokenizers/chatbot_bpe.json'\n",
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=3, max_length=25):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=3: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=25:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_length = max_length  # 문장 구성 토큰의 최대개수.\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]') # 문장 시작 토큰\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]') # 문장 끝 토큰\n",
    "        \n",
    "        self.question_squences = [] #[[200, 300, 250, ..], ....]\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            # q, a: string \n",
    "            q_tokens = self.__process_sequence(q) # str -> [200, 300, 250, ...]\n",
    "            a_tokens = self.__process_sequence(a)\n",
    "            # 토큰수가 min_length 초과인 질문-답변만 list에 추가.\n",
    "            if len(q_tokens) > min_length and len(a_tokens) > min_length:\n",
    "                self.question_squences.append(q_tokens)\n",
    "                self.answer_sequences.append(a_tokens)\n",
    "            \n",
    "\n",
    "    def __add_special_tokens(self, token_sequence): #[20, 300, 25]\n",
    "        \"\"\"\n",
    "        max_length 보다 token 수가 많은 경우 max_length-1에 맞춰 뒤를 잘라낸다. (-1: EOS 추가하기 위해.)\n",
    "        token_id_list 에 [EOS] 토큰 추가. \n",
    "        \"\"\"\n",
    "\n",
    "        token_sequence = token_sequence[:self.max_length-1]\n",
    "        token_sequence.append(self.EOS)\n",
    "        \n",
    "        return token_sequence\n",
    "    \n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 token_id 리스트(list)로 변환 후 반환\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_squences)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        # self.question_squences[index] -> [token수] => [token수, 1]\n",
    "        return torch.LongTensor(self.question_squences[index]).unsqueeze(1), torch.LongTensor(self.answer_sequences[index]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d68420-66c6-4864-82bc-c4d85c691a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25\n",
    "MIN_LENGTH = 3\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34539632-354d-4785-a4f0-810e2a44d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3752b6-300c-46d7-a227-024dc1f7e24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[13306],\n",
       "         [  369],\n",
       "         [    4],\n",
       "         [    3]]),\n",
       " tensor([[4998],\n",
       "         [ 379],\n",
       "         [3868],\n",
       "         [   9],\n",
       "         [   3]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ff66f-2f24-4f8b-a864-ce97978c4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장씩 학습 -> DataLoader는 생성하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cde8b86-320a-4127-a925-9ad3960eba5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13306],\n",
       "        [  369],\n",
       "        [    4],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af63e-d915-4832-840e-44a07a53cdad",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq 모델과 동일 한 구조\n",
    "    - 이전 코드(seq2seq)와 비교해서 forward()에서 입력 처리는 token 하나씩 하나씩 처리한다. \n",
    "\n",
    "![encoder](figures/attn_encoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs # 총단어개수 \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Embedding Layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        #GRU Layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=False      # Encoder도 토큰 하나씩 순서대로 입력받아 처리. 단방향.\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        한개 질문의 token 한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        batch=1       seq_len=1\n",
    "        Parameter\n",
    "            x: 한개 토큰. shape-[1] ex) [3000]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size] \n",
    "                                                              (layer수(1) * 단방향(1), batch(1), hidden_size)\n",
    "        Return\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        x = self.embedding(x).unsqueeze(0)  # (1, embedding_dim) -> (1**, 1, embedding_dim)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        \"\"\"\n",
    "        처음 입력할 hidden_state. \n",
    "        값: 0\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f0e1f7-e5a4-4da8-8ce5-dcc8171d7ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13306],\n",
       "        [  369],\n",
       "        [    4],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "210f793c-e870-414e-beba-f270f1656d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13306])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce6f89f-b8d1-4f28-baa5-244c11e56b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 1, 100])\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "embedding_dim = 100\n",
    "hidden_size = 256\n",
    "\n",
    "# Encoder 처리 로직\n",
    "el = nn.Embedding(vocab_size, embedding_dim)\n",
    "gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "print(a[0].shape)\n",
    "x = el(a[0])  # 입력: 한개 문장의 한개 토큰씩 입력.\n",
    "print(x.shape)  # 1개, 100 Embedding vector\n",
    "x = x.unsqueeze(0) # gru input shape에 맞추기 위해서. (seq_len, batch_size, input_size)\n",
    "print(x.shape)   \n",
    "# print(x)\n",
    "init_hidden = torch.zeros(1, 1, hidden_size) # ==> encoder.init_hidden()\n",
    "o1, h1 = gru(x, init_hidden)\n",
    "print(o1.shape, h1.shape)\n",
    "\n",
    "#  두번째 timestep grup\n",
    "o2, h2 = gru(x, h1) # a[1] 토큰 embedding 값, 첫번째 timestep의 hidden state 처리결과."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112940e-3f9a-42de-89df-e5b686a7856f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af5359-f8c8-4d9c-b6ec-b229d4465180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52754234-031c-48b1-adbe-120cd607b9fb",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "![seq2seq attention outline](figures/attn_seq2seq_attention_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n",
    "  \n",
    "![attention decoder](figures/attn_decoder-network_graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "source": [
    "### Attetion Weight\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n",
    "  \n",
    "![Attention Weight](figures/attn_attention_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "source": [
    "### Attention Value\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector. \n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n",
    "\n",
    "![attention value](figures/attn_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어(현재 timestep의 input)** 와 **Encoder output에 attention이 적용된 값** 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 Linear Layer함수+ReLU를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n",
    "\n",
    "![rnn](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f57bb1-13c8-43ba-a4b8-58362fc9ca49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, \n",
    "                  dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        \n",
    "        ## Attention Score를 만드는 Linear Layer. 이 결과를 Softmax를 통과시키면 Attention weight(distribution)\n",
    "        self.attn = nn.Linear(\n",
    "            in_features=hidden_size+embedding_dim, # 이전 timestep hidden_state + 현재입력 embedding vector\n",
    "            out_features=max_length, # encoder hidden_state들에 곱해질 값을 출력. hidden_state의 개수==MAX_LENGTH\n",
    "        )\n",
    "        ## Attention Value + embedding vector를 입력으로 받아서 GRU 입력 크기로 줄여주는 역할.\n",
    "        self.attn_combine = nn.Linear(\n",
    "            hidden_size + embedding_dim, #attention value 와 embedding vector concat 한 크기.\n",
    "            hidden_size,   # GRU input_size\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_size,   # 입력\n",
    "            hidden_size=hidden_size # hidden state개수(Node/Unit/Neuron 수)\n",
    "        )\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=hidden_size, \n",
    "            out_features=num_vocabs  # 각 단어가 생성할 단어일 확률 -> 총 단어의 개수.\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰 id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tuple: (output, hidden, attention_weight)\n",
    "                output: 다음 단어일 단어별 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                attention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        # Embedding\n",
    "        ## input shape: [1] - 한문장의 단어 1개 (batch-1, seq_len-1)\n",
    "        ## 출력 shape:  [1, embedding_dim]  -> seq_length dim을 추가. [1, 1, embedding_dim]\n",
    "        ######################################\n",
    "        embedding = self.embedding(x).unsqueeze(0)\n",
    "\n",
    "        ################################################\n",
    "        # Attention Weight \n",
    "        ##  embedding dropout 적용\n",
    "        ##  hidden_state(이전 step) 와 embedding vector를 합치기.\n",
    "        ##  attn(Linear) 통과시켜서 size를 줄이기.  ==> Attention Score\n",
    "        ##  softmax(Attention Score) ==> Attention weight\n",
    "        ################################################\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        attn_in = torch.concat((embedding[0], hidden[0]), dim=1) #dim=1  -----> 방향으로 합친다.\n",
    "        # (1, embedding_dim)  (1, hidden_size)  = (1, em + hs)\n",
    "        attention_score = self.attn(attn_in)  # (1, max_length)\n",
    "        attention_weights = nn.Softmax(dim=-1)(attention_score) #attention_score.sofmax(dim=-1)\n",
    "\n",
    "        ################################################\n",
    "        # Attention Value (Context Vector)계산\n",
    "        ## attention_weight x encoder의 output들\n",
    "        ################################################\n",
    "\n",
    "        attention_value = torch.bmm(\n",
    "            attention_weights.unsqueeze(0), # (1, max_length)              => (1, 1, max_legnth)\n",
    "            encoder_outputs.unsqueeze(0)   # (max_length, hidden_size) =>(1, max_length, hidden_size)\n",
    "        )  # attention_value shape:   (1, 1, hidden_size)\n",
    "        # bmm() - batch_matrix_multiply (batch 행렬곱) => 1, 2 축 행렬끼리 행렬곱\n",
    "\n",
    "        ################################################################\n",
    "        # GRU에 입력할 값을 생성 -> GRU\n",
    "        ## attention_value 와 decoder 입력 embedding vector를 concat\n",
    "        ## concat 한 것을 GRU 입력 크기(hidden_size)  에 맞도록 Linear를 이용해 조절.\n",
    "        ## ReLU로 비선형 처리\n",
    "        ## GRU에 입력 (윗단계에서 처리한 값,  이전 timestep의 hidden state)\n",
    "        ################################################################\n",
    "        attn_combine_in = torch.concat(\n",
    "            [attention_value[0], embedding[0]],  # [1, hidden_size] + [1, embedding_dim] => [1, h_s + e_d]\n",
    "            dim=-1\n",
    "        )\n",
    "        gru_in = self.attn_combine(attn_combine_in) # 출력: [1, hidden_size]\n",
    "        gru_in = gru_in.unsqueeze(0)  # [1, 1, hidden_size]\n",
    "        gru_in = nn.ReLU()(gru_in)\n",
    "        output, hidden = self.gru(gru_in, hidden)  \n",
    "        ################################################################\n",
    "        # output을 classifier에 입력해서 단어 추론(분류)\n",
    "        ################################################################\n",
    "        last_output = self.classifier(output[0])   #output: [1, 1, hidden] -> [1, hidden] -Linear-> [1, num_vocabs]\n",
    "\n",
    "        # return: last_output[0] [1, num_vocab] => [num_vocab]\n",
    "        #          hidden: gru의 hidden state (1, 1, hidden_size)\n",
    "        #          attention_weight:  [1, max_length]\n",
    "        return last_output[0], hidden, attention_weights\n",
    "\n",
    "    def initHidden(self, device):\n",
    "        # Decoder 첫번 timestep에 넣어줄 이전 timestep의 hidden state 값. (value: 0)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "74978379-d6e3-4edd-bcf0-bc495f8be00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "hidden_size=32\n",
    "embedding_dim = 100\n",
    "dropout_p = 0.1\n",
    "max_length = 25\n",
    "\n",
    "decoder = AttentionDecoder(\n",
    "    num_vocabs=vocab_size, hidden_size=hidden_size, embedding_dim=embedding_dim,\n",
    "    dropout_p=dropout_p, max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8be86aa0-6edd-47e0-a9f6-0e19c7c7c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4998]) torch.Size([25, 32]) torch.Size([1, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "x = b[0]\n",
    "encoder_outputs = torch.randn(max_length, hidden_size)\n",
    "hidden = decoder.initHidden(device)\n",
    "print(x, encoder_outputs.shape, hidden.shape)\n",
    "\n",
    "o, h, aw = decoder(x, hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cbf998c2-df47-40cd-9aef-ffcd4ea278f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 32]), torch.Size([1, 25]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape\n",
    "o.max(dim=0)\n",
    "tokenizer.id_to_token(23597)\n",
    "h.shape, aw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f005b-c60f-4631-a235-f974fd63ea1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d869b-2956-4219-aa0a-91da04a00b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab7c43d-3691-4723-8051-7bc0a8a4a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5eb486a3-09bb-4d05-ad2d-149b355f6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8761"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "75080eb6-da0e-4c86-998e-2dd8405e5a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_TOKEN = dataset.tokenizer.token_to_id(\"[SOS]\")\n",
    "EOS_TOKEN = dataset.tokenizer.token_to_id(\"[EOS]\")\n",
    "SOS_TOKEN, EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "166baaa8-adcc-49a3-b0c7-10417584d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개 question-answer set 학습\n",
    "def train_fn(input_tensor, target_tensor, encoder, decoder, \n",
    "              encoder_optimizer, decoder_optimizer, \n",
    "              loss_fn, device, max_length=MAX_LENGTH, teacher_forcing_ratio=0.5):\n",
    "    # input_tensor, target_tensor : 1개 질문-답 쌍.\n",
    "    \n",
    "    # 옵티마이저 초기화(gradient값 초기화.)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #################\n",
    "    # Encoder 학습\n",
    "    #################\n",
    "    encoder_hidden = encoder.init_hidden(device)  # 첫 timestep에 입력할 hidden state값 조회.\n",
    "    # 입/출력 문장(질문-답변) 의 토큰(단어) 수 조회\n",
    "    input_length = input_tensor.shape[0]\n",
    "    output_length = target_tensor.shape[0]   \n",
    "\n",
    "    # encoder의 hidden state들을 저장할 변수.  hidden_size가 단어 개수만 나옴.\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0.0\n",
    "    ########################\n",
    "    # 질문 문장을 구성하는 단어(토큰)들을  하나씩 Encoder에 입력해서 hidden_state(특성)를 출력\n",
    "    #########################\n",
    "    for ei in range(input_length):  #[1, 30, 20, 780, 60...]\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], # 질문 문장의 ei 번째 토큰_id\n",
    "                                                                  encoder_hidden\n",
    "                                                                  )\n",
    "        # encoder_output을 encoder_outputs에 추가\n",
    "        encoder_outputs[ei] = encoder_output\n",
    "\n",
    "    #########################\n",
    "    # Decoder 처리\n",
    "    ############################\n",
    "\n",
    "    # Decoder 초기 입력 토큰: [SOS]\n",
    "    decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "    # Decoder 첫 timestep에 입력할 hidden state \n",
    "    decoder_hidden = encoder_hidden # Encoder의 마지막 timestep의 hidden state\n",
    "\n",
    "    is_teacher_forcing  = True if random.random() < teacher_forcing_ratio else False  # Teacher Forcing을 적용할 지 여부.\n",
    "    # True:  decoder 입력으로 정답 토큰.  False:  이전 timestep에서 decoder 생성한 토큰을 입력\n",
    "    ### 한 토큰씩 처리\n",
    "    for di in range(output_length):\n",
    "        # decoder_output: 전체 단어의 다음단어일 확률값 : shape - [총단어수]\n",
    "        # decoder_hidden: gru가 출력한 hidden_state: shape - [1, 1, hidden_size]\n",
    "        # attention_weight:  [1, max_length]\n",
    "        decoder_output, decoder_hidden, attention_weight = decoder(decoder_input,   # x_t\n",
    "                                                                                         decoder_hidden, # h_{t-1}\n",
    "                                                                                         encoder_outputs)# encoder의 모든 hidden\n",
    "        ## Loss 계산\n",
    "        loss = loss + loss_fn(\n",
    "            decoder_output.unsqueeze(0), # 모델이 예측한 값-단어별 확률 [총단어수] -> [1, 총단어수]\n",
    "            target_tensor[di]\n",
    "        )\n",
    "        ## decoder _input 만들기 (다음 timestep의 입력) is_teacher_forcing: True-정답, False: 추론\n",
    "        if is_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            # 모델추론 토큰   \n",
    "            output_token = decoder_output.argmax(dim=-1).unsqueeze(0)\n",
    "            decoder_input = output_token.detach() # detach(): 역전파할때 grad 계산 대상에 빼겠다.\n",
    "        \n",
    "        if decoder_input == EOS_TOKEN: # 다음에 입력할 값이 문장의 끝 이라면 멈춤.\n",
    "            break\n",
    "\n",
    "    ######### Encoder/Decoder 순전파 처리 #############\n",
    "    # 학습\n",
    "    \n",
    "    # loss 기반으로 gradient계산\n",
    "    loss.backward()\n",
    "    # 파라미터 update\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    ## loss 평균 리턴. (loss = decoder 생성 문자별 loss가 누적)\n",
    "    return loss.item()/output_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6824a9fb-1592-44f4-8a74-6c5098bd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 학습을 처리하는 함수.\n",
    "def train_iterations(encoder, decoder, n_iters, dataset, device, log_interval=1000, learning_rate=0.001):\n",
    "    # n_iters:  몇 step 학습할지 int. (1step당 1 문장쌍.)\n",
    "    # encoder/decoder를 train 모드로 변경\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    print_loss = 0.0   # log_interval step 당 한번씩 출력할 loss. 출력후 초기화(0)\n",
    "    # 옵티마이저\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 학습시킬 데이터 조회 -> random하게 n_iters 개수만큼 추출.\n",
    "    total_data = len(dataset)\n",
    "    train_pairs = [dataset[random.randint(0, total_data-1)]  for i in range(n_iters)]\n",
    "    \n",
    "    ## loss 함수 정의\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    s = time.time()  # n_iters 만큼 반복하면서 train\n",
    "    for idx in range(n_iters):\n",
    "        train_pair = train_pairs[idx]  # 질문-답변 한쌍을 조회.\n",
    "        \n",
    "        input_tensor, target_tensor = train_pair # 튜플대입        \n",
    "        input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # train_fn() 호출\n",
    "        loss = train_fn(input_tensor, target_tensor, encoder, decoder, \n",
    "                           encoder_optimizer, decoder_optimizer, loss_fn, device)\n",
    "        print_loss += loss\n",
    "\n",
    "        ##### log_interval에 한번씩 train 로그를 출력\n",
    "        if (idx+1) % log_interval == 0:\n",
    "            print_loss_avg = print_loss / log_interval\n",
    "            print_loss = 0.0\n",
    "            print(f\"Step: {idx+1} - 진행: {idx / n_iters * 100}%, Loss: {print_loss_avg:.4f}\")\n",
    "        \n",
    "    e = time.time()\n",
    "    print(f\"총걸린시간: {e-s}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "892bb6f0-f9cd-433a-84b2-c8130f7d1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOCABS = dataset.vocab_size\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDING_DIM = 256\n",
    "DROPOUT_P = 0.1\n",
    "N_ITERATION = 100000\n",
    "# N_ITERATION = 10\n",
    "\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  num_layers=1)\n",
    "\n",
    "decoder = AttentionDecoder(num_vocabs=NUM_VOCABS, \n",
    "                           hidden_size=HIDDEN_SIZE, \n",
    "                           embedding_dim=EMBEDDING_DIM, \n",
    "                           dropout_p=DROPOUT_P, \n",
    "                           max_length=MAX_LENGTH)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6fde0e0f-ae81-45ff-b685-866d94abe63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3 - 진행: 20.0%, Loss: 3.3927\n",
      "Step: 6 - 진행: 50.0%, Loss: 4.5857\n",
      "Step: 9 - 진행: 80.0%, Loss: 3.5898\n",
      "총걸린시간: 1.6738159656524658초\n"
     ]
    }
   ],
   "source": [
    "train_iterations(encoder, decoder, N_ITERATION, dataset, device, log_interval=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f416fa1-0a38-4a60-9c02-41261fab6cb0",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f2e04-2603-4e3a-a4cb-2b6a4afd5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('models/chatbot_attn_tokenizer', exist_ok=True)\n",
    "# encoder_save_path = 'models/chatbot_attn_tokenizer/encoder.pt'\n",
    "# decoder_save_path = 'models/chatbot_attn_tokenizer/decoder.pt'\n",
    "# torch.save(encoder, encoder_save_path)\n",
    "# torch.save(decoder, decoder_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d34-7308-493d-95a4-eef87c361d33",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd253493-e67b-4623-afbc-1cf002e08cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_TOKEN = tokenizer.token_to_id(\"[SOS]\")\n",
    "SOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed2ff49-9ceb-488f-9547-b687684187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, dataset, device, max_length=MAX_LENGTH):\n",
    "    ##  한개의 질문에 대해    질문, 정답 답변, 모델이 추정한 답변을 출력\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        # Encoder 처음 timestep에 입력할 hidden state\n",
    "        encoder_hidden = encoder.init_hidden(device=device)\n",
    "        \n",
    "        # Encoder 가 출력한 모든 hidden state값들을 저장할 tensor\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        ## decoder\n",
    "        decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []  # decoder가 추론한 단어(토큰)들을 저장할 리스트.\n",
    "        decoder_attentions = torch.zeros(max_length, max_length) # attention weight를 저장할 텐서\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, \n",
    "                                                                                    decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_attentions[di] = decoder_attention.data  \n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)  \n",
    "            # 단어별 확률 -> 가장 큰값. topv: 확률, topi: index (단어 토큰값)\n",
    "\n",
    "            if topi.item() == dataset.EOS:  # 리스트에 추가하고 다음 단어 예측을 중단.\n",
    "                decoded_words.append('[EOS]')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset.tokenizer.id_to_token(topi.item()))\n",
    "                # token_id -> token(단어:str), 리스트에 추가.\n",
    "            decoder_input = topi.detach()  # 다음 timestep의 input  \n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, dataset, device, n=10):\n",
    "    \n",
    "    for i in range(n):\n",
    "    \n",
    "        x, y = random.choice(dataset)\n",
    "        \n",
    "        print('질문(정답):', dataset.tokenizer.decode(x.flatten().tolist()))\n",
    "        print('답변(정답):', dataset.tokenizer.decode(y.flatten().tolist()))\n",
    "    \n",
    "        output_words, attentions = evaluate(encoder, decoder, x.to(device), dataset, device)\n",
    "        output_sentence = ' '.join(output_words[:-1])   # [EOS] 빼기\n",
    "        \n",
    "        print('답변(예측):', output_sentence)\n",
    "        print('----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d8dc3a82-94c7-4537-81f9-82e673d9d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a-b-안녕'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-\".join([\"a\", \"b\", \"안녕\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98932528-1986-4a77-aba3-c91b905a0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_save_path = \"models/chatbot_attn_model/encoder.pt\"\n",
    "decoder_save_path = \"models/chatbot_attn_model/decoder.pt\"\n",
    "encoder = torch.load(encoder_save_path, map_location=device)\n",
    "decoder = torch.load(decoder_save_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6081348f-d9cd-463c-ac48-e7cd06ecfb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문(정답): 내일은 친구들랑 놀까 ?\n",
      "답변(정답): 시간 있냐고 물어보세요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 밀당 어떻게 해 ?\n",
      "답변(정답): 바쁘면 자연스럽게 됩니다 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 정말 잊은걸까 ?\n",
      "답변(정답): 다른 곳으로 관심을 돌려보세요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 연상 어때 ?\n",
      "답변(정답): 사랑에 나이는 중요하지 않다고 생각해요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 아 괜히 헤어지자고 했나봐\n",
      "답변(정답): 어떤 결정이든 후회가 남을 수 밖에 없어요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 점심시간일 텐데 카톡이 안 와 .\n",
      "답변(정답): 먼저 연락을 해보는 건 어떨까요 ?\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 이제 정리하려고 합니다 .\n",
      "답변(정답): 맘 고생 많았어요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 치고 올라오는 신입\n",
      "답변(정답): 더 열심히 하세요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 머리카락 많이 빠져\n",
      "답변(정답): 스트레스를 받으시나봐요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n",
      "질문(정답): 남친 투정이 심해\n",
      "답변(정답): 대화를 통해 적절한 합의점을 찾아보세요 .\n",
      "답변(예측): 더 . . . . . . . . . . . . . . . . . . . . . . .\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder, decoder, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "15063fb2-6915-430a-9481-2e39dd64e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Attention Weight시각화를 위한 함수\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    " # plt.switch_backend('agg')\n",
    "\n",
    "plt.rcParams['font.family'] = 'malgun gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap=\"gray\")\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(range(len(input_sentence)))\n",
    "    ax.set_xticklabels(input_sentence, rotation=90)\n",
    "\n",
    "    ax.set_yticks(range(len(output_words)))\n",
    "    ax.set_yticklabels(output_words)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_show_attention(encoder, decoder, input_sentence, dataset, device):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence.to(device), dataset, device)\n",
    "    # 질문 문장 구성\n",
    "    input_sentence = ' '.join([tokenizer.id_to_token(t.item()) for t in dataset[idx][0].flatten()])\n",
    "    # 모델이 추정해서 전달한 토큰들을 문장으로 구성.\n",
    "    output_words = ' '.join(output_words)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output_words)\n",
    "    show_attention(input_sentence.split(), output_words.split(), attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6414c754-56a0-4b15-bf90-6b0e8ddd43a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3721],\n",
       "        [ 1905],\n",
       "        [12719],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f0cbbe07-77d4-44d1-bc61-c8fe823768bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 여름 빨리 지나갔으면 [EOS]\n",
      "output = . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 2100\n",
    "evaluate_and_show_attention(encoder, decoder, dataset[idx][0], dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290c137-825c-4725-8525-3cf7d8e9e677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
